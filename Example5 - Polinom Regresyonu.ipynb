{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_hesapla(thetas,X,Y):\n",
    "    return np.sum(((X.dot(thetas.T)-Y)**2)/(2*X.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_hesapla_reg(thetas,X,Y,reg_katsayi=0.1):\n",
    "    return (np.sum(((X.dot(thetas.T)-Y)**2))+reg_katsayi*np.sum(thetas[:,1:]**2))/(2*X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_reg(thetas,X,Y,reg_katsayi=0.01,lr=0.1):\n",
    "    thetas -= (lr*((X.dot(thetas.T)-Y).T.dot(X)))/X.shape[0]\n",
    "    thetas[:,1:] -= (reg_katsayi/X.shape[0])*(thetas[:,1:])\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polinom_feature(X,derece):\n",
    "    temp = np.empty((X.shape[0],derece-1))\n",
    "    for i in range(2,derece+1):\n",
    "        temp[:,i-2:i-1] = X**i\n",
    "    return np.concatenate([X,temp],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('ex5data1.mat')\n",
    "x_train = data['X']\n",
    "y_train = data['y']\n",
    "x_val = data['Xval']\n",
    "y_val = data['yval']\n",
    "x_test = data['Xtest']\n",
    "y_test = data['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rang = x_train.max() - x_train.min()\n",
    "x_train = (x_train-x_train.mean())/rang\n",
    "\n",
    "rang = x_val.max() - x_val.min()\n",
    "x_val = (x_val-x_val.mean())/rang\n",
    "\n",
    "rang = x_test.max() - x_test.min()\n",
    "x_test = (x_test-x_test.mean())/rang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "derece = 10\n",
    "x_train = polinom_feature(x_train,derece)\n",
    "x_val = polinom_feature(x_val,derece)\n",
    "x_test = polinom_feature(x_test,derece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([np.ones((x_train.shape[0],1)),x_train],axis=1)\n",
    "x_val = np.concatenate([np.ones((x_val.shape[0],1)),x_val],axis=1)\n",
    "x_test = np.concatenate([np.ones((x_test.shape[0],1)),x_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regüralizasyon ile Lineer Regresyon için ideal reg_degerinin bulunması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterasyon = 10000\n",
    "loss_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_values = [0,0.001,0.01,0.1,1,10,100,1000]\n",
    "reg_values = list(np.arange(0,0.1,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "son_losslar_train = list()\n",
    "son_losslar_val = list()\n",
    "for reg_value in reg_values:\n",
    "    #np.random.seed(1234)\n",
    "    #thetas = np.random.randn(1,x_train.shape[1])\n",
    "    thetas = np.ones((1,x_train.shape[1]))\n",
    "    for i in range(iterasyon):\n",
    "        for j in range(x_train.shape[0]):\n",
    "            thetas = gradient_descent_reg(thetas,x_train[j:j+1],y_train[j:j+1],lr=0.3,reg_katsayi=reg_value)\n",
    "    son_losslar_train.append(loss_hesapla(thetas,x_train,y_train))\n",
    "    son_losslar_val.append(loss_hesapla(thetas,x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(reg_values,son_losslar_train)\n",
    "plt.plot(reg_values,son_losslar_val)\n",
    "plt.xlabel(\"Reguralizasyon Degeri\")\n",
    "plt.ylabel(\"Loss(MSE)\")\n",
    "plt.legend([\"Train Loss\",\"Val Loss\"])\n",
    "plt.title(\"Reguralizasyon Değerine Göre Loss Değerleri\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterasyon = 300000\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "thetas = np.ones((1,x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.iterasyon, Train Loss = 38.914506131899394, Val Loss = 47.21958182720523\n",
      "100.iterasyon, Train Loss = 0.9651791251408737, Val Loss = 9.561566576701356\n",
      "200.iterasyon, Train Loss = 0.9202193887327333, Val Loss = 9.272552318648732\n",
      "300.iterasyon, Train Loss = 0.8966956883724316, Val Loss = 9.155318030634145\n",
      "400.iterasyon, Train Loss = 0.877079673695457, Val Loss = 9.071837249335882\n",
      "500.iterasyon, Train Loss = 0.8598468656708512, Val Loss = 9.01196808753167\n",
      "600.iterasyon, Train Loss = 0.84413377197007, Val Loss = 8.970124094079313\n",
      "700.iterasyon, Train Loss = 0.829431450337751, Val Loss = 8.94213016072681\n",
      "800.iterasyon, Train Loss = 0.8154415936985161, Val Loss = 8.92481078445422\n",
      "900.iterasyon, Train Loss = 0.8019904514758772, Val Loss = 8.915736447808142\n",
      "1000.iterasyon, Train Loss = 0.7889773376043022, Val Loss = 8.913039850471346\n",
      "1100.iterasyon, Train Loss = 0.7763439739272383, Val Loss = 8.915280452835539\n",
      "1200.iterasyon, Train Loss = 0.7640563499769408, Val Loss = 8.921343565913489\n",
      "1300.iterasyon, Train Loss = 0.7520940755105666, Val Loss = 8.93036450089403\n",
      "1400.iterasyon, Train Loss = 0.7404441983920215, Val Loss = 8.941671124003836\n",
      "1500.iterasyon, Train Loss = 0.7290976680811788, Val Loss = 8.954740077996696\n",
      "1600.iterasyon, Train Loss = 0.7180473545085454, Val Loss = 8.969163247817365\n",
      "1700.iterasyon, Train Loss = 0.707286971949749, Val Loss = 8.98462196661059\n",
      "1800.iterasyon, Train Loss = 0.6968105220517988, Val Loss = 9.000867109238715\n",
      "1900.iterasyon, Train Loss = 0.6866120287708681, Val Loss = 9.01770368852319\n",
      "2000.iterasyon, Train Loss = 0.6766854326940813, Val Loss = 9.034978910454715\n",
      "2100.iterasyon, Train Loss = 0.6670245684790547, Val Loss = 9.05257289607429\n",
      "2200.iterasyon, Train Loss = 0.6576231823352979, Val Loss = 9.070391465086079\n",
      "2300.iterasyon, Train Loss = 0.6484749658721264, Val Loss = 9.088360517111669\n",
      "2400.iterasyon, Train Loss = 0.6395735938367983, Val Loss = 9.106421653182496\n",
      "2500.iterasyon, Train Loss = 0.630912759619641, Val Loss = 9.124528761379818\n",
      "2600.iterasyon, Train Loss = 0.6224862059171972, Val Loss = 9.142645352824179\n",
      "2700.iterasyon, Train Loss = 0.6142877498151946, Val Loss = 9.160742482133431\n",
      "2800.iterasyon, Train Loss = 0.6063113024877601, Val Loss = 9.178797123450247\n",
      "2900.iterasyon, Train Loss = 0.5985508841230132, Val Loss = 9.196790901757547\n",
      "3000.iterasyon, Train Loss = 0.5910006348188023, Val Loss = 9.214709101391584\n",
      "3100.iterasyon, Train Loss = 0.5836548221840263, Val Loss = 9.23253989089858\n",
      "3200.iterasyon, Train Loss = 0.5765078463075506, Val Loss = 9.250273716786175\n",
      "3300.iterasyon, Train Loss = 0.5695542426601927, Val Loss = 9.26790282915647\n",
      "3400.iterasyon, Train Loss = 0.5627886833970749, Val Loss = 9.28542091033879\n",
      "3500.iterasyon, Train Loss = 0.5562059774378446, Val Loss = 9.302822783978645\n",
      "3600.iterasyon, Train Loss = 0.5498010696249181, Val Loss = 9.320104186983789\n",
      "3700.iterasyon, Train Loss = 0.5435690391955695, Val Loss = 9.337261590585465\n",
      "3800.iterasyon, Train Loss = 0.5375050977512993, Val Loss = 9.354292059783994\n",
      "3900.iterasyon, Train Loss = 0.5316045868662423, Val Loss = 9.371193142797908\n",
      "4000.iterasyon, Train Loss = 0.5258629754433505, Val Loss = 9.387962783970863\n",
      "4100.iterasyon, Train Loss = 0.5202758569013443, Val Loss = 9.404599255023287\n",
      "4200.iterasyon, Train Loss = 0.5148389462553437, Val Loss = 9.421101100655244\n",
      "4300.iterasyon, Train Loss = 0.5095480771387494, Val Loss = 9.437467095380008\n",
      "4400.iterasyon, Train Loss = 0.5043991988019672, Val Loss = 9.453696209151042\n",
      "4500.iterasyon, Train Loss = 0.499388373114493, Val Loss = 9.469787579878068\n",
      "4600.iterasyon, Train Loss = 0.49451177158998627, Val Loss = 9.485740491343904\n",
      "4700.iterasyon, Train Loss = 0.48976567244859887, Val Loss = 9.501554355359517\n",
      "4800.iterasyon, Train Loss = 0.48514645772687165, Val Loss = 9.517228697248358\n",
      "4900.iterasyon, Train Loss = 0.48065061044244467, Val Loss = 9.53276314394956\n",
      "5000.iterasyon, Train Loss = 0.4762747118185502, Val Loss = 9.548157414184962\n",
      "5100.iterasyon, Train Loss = 0.47201543857151745, Val Loss = 9.563411310255267\n",
      "5200.iterasyon, Train Loss = 0.4678695602632468, Val Loss = 9.578524711125985\n",
      "5300.iterasyon, Train Loss = 0.4638339367196254, Val Loss = 9.593497566537224\n",
      "5400.iterasyon, Train Loss = 0.45990551551514186, Val Loss = 9.60832989192911\n",
      "5500.iterasyon, Train Loss = 0.4560813295234453, Val Loss = 9.62302176402019\n",
      "5600.iterasyon, Train Loss = 0.45235849453318844, Val Loss = 9.6375733169106\n",
      "5700.iterasyon, Train Loss = 0.4487342069282209, Val Loss = 9.65198473861021\n",
      "5800.iterasyon, Train Loss = 0.4452057414310038, Val Loss = 9.666256267912726\n",
      "5900.iterasyon, Train Loss = 0.4417704489079886, Val Loss = 9.68038819155381\n",
      "6000.iterasyon, Train Loss = 0.438425754235571, Val Loss = 9.694380841604849\n",
      "6100.iterasyon, Train Loss = 0.43516915422524677, Val Loss = 9.708234593063247\n",
      "6200.iterasyon, Train Loss = 0.4319982156064572, Val Loss = 9.721949861609515\n",
      "6300.iterasyon, Train Loss = 0.4289105730657399, Val Loss = 9.735527101506248\n",
      "6400.iterasyon, Train Loss = 0.4259039273406395, Val Loss = 9.748966803620332\n",
      "6500.iterasyon, Train Loss = 0.42297604336699574, Val Loss = 9.762269493552672\n",
      "6600.iterasyon, Train Loss = 0.42012474847812126, Val Loss = 9.775435729863169\n",
      "6700.iterasyon, Train Loss = 0.41734793065449066, Val Loss = 9.788466102380887\n",
      "6800.iterasyon, Train Loss = 0.4146435368225461, Val Loss = 9.801361230591295\n",
      "6900.iterasyon, Train Loss = 0.4120095712012644, Val Loss = 9.814121762093922\n",
      "7000.iterasyon, Train Loss = 0.40944409369515095, Val Loss = 9.826748371124857\n",
      "7100.iterasyon, Train Loss = 0.406945218332398, Val Loss = 9.839241757139398\n",
      "7200.iterasyon, Train Loss = 0.4045111117469038, Val Loss = 9.851602643451145\n",
      "7300.iterasyon, Train Loss = 0.40213999170298115, Val Loss = 9.863831775923906\n",
      "7400.iterasyon, Train Loss = 0.39983012566149895, Val Loss = 9.875929921713759\n",
      "7500.iterasyon, Train Loss = 0.3975798293863395, Val Loss = 9.887897868058811\n",
      "7600.iterasyon, Train Loss = 0.3953874655900372, Val Loss = 9.899736421113902\n",
      "7700.iterasyon, Train Loss = 0.3932514426174602, Val Loss = 9.91144640482904\n",
      "7800.iterasyon, Train Loss = 0.39117021316651096, Val Loss = 9.923028659869058\n",
      "7900.iterasyon, Train Loss = 0.3891422730447847, Val Loss = 9.93448404257334\n",
      "8000.iterasyon, Train Loss = 0.38716615996117015, Val Loss = 9.94581342395362\n",
      "8100.iterasyon, Train Loss = 0.3852404523514228, Val Loss = 9.9570176887289\n",
      "8200.iterasyon, Train Loss = 0.3833637682367385, Val Loss = 9.968097734395903\n",
      "8300.iterasyon, Train Loss = 0.3815347641144231, Val Loss = 9.979054470333804\n",
      "8400.iterasyon, Train Loss = 0.3797521338797252, Val Loss = 9.989888816942118\n",
      "8500.iterasyon, Train Loss = 0.37801460777797585, Val Loss = 10.000601704810745\n",
      "8600.iterasyon, Train Loss = 0.37632095138618743, Val Loss = 10.011194073920741\n",
      "8700.iterasyon, Train Loss = 0.37466996462325236, Val Loss = 10.021666872875246\n",
      "8800.iterasyon, Train Loss = 0.37306048078797543, Val Loss = 10.032021058159195\n",
      "8900.iterasyon, Train Loss = 0.37149136562412327, Val Loss = 10.042257593427081\n",
      "9000.iterasyon, Train Loss = 0.36996151641175357, Val Loss = 10.05237744881787\n",
      "9100.iterasyon, Train Loss = 0.3684698610840773, Val Loss = 10.062381600295955\n",
      "9200.iterasyon, Train Loss = 0.3670153573691242, Val Loss = 10.072271029017601\n",
      "9300.iterasyon, Train Loss = 0.3655969919555413, Val Loss = 10.082046720721877\n",
      "9400.iterasyon, Train Loss = 0.36421377968180124, Val Loss = 10.091709665145373\n",
      "9500.iterasyon, Train Loss = 0.36286476274821866, Val Loss = 10.101260855459776\n",
      "9600.iterasyon, Train Loss = 0.36154900995108097, Val Loss = 10.110701287731747\n",
      "9700.iterasyon, Train Loss = 0.3602656159382928, Val Loss = 10.120031960404376\n",
      "9800.iterasyon, Train Loss = 0.359013700485957, Val Loss = 10.129253873799223\n",
      "9900.iterasyon, Train Loss = 0.3577924077952215, Val Loss = 10.138368029638766\n",
      "10000.iterasyon, Train Loss = 0.356600905808937, Val Loss = 10.147375430588124\n",
      "10100.iterasyon, Train Loss = 0.3554383855474563, Val Loss = 10.156277079815686\n",
      "10200.iterasyon, Train Loss = 0.3543040604631173, Val Loss = 10.165073980571966\n",
      "10300.iterasyon, Train Loss = 0.3531971658128341, Val Loss = 10.173767135786099\n",
      "10400.iterasyon, Train Loss = 0.3521169580483256, Val Loss = 10.182357547679377\n",
      "10500.iterasyon, Train Loss = 0.3510627142234468, Val Loss = 10.190846217395215\n",
      "10600.iterasyon, Train Loss = 0.35003373141817046, Val Loss = 10.199234144645198\n",
      "10700.iterasyon, Train Loss = 0.3490293261787463, Val Loss = 10.207522327370352\n",
      "10800.iterasyon, Train Loss = 0.34804883397356096, Val Loss = 10.215711761417515\n",
      "10900.iterasyon, Train Loss = 0.3470916086642933, Val Loss = 10.223803440229961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000.iterasyon, Train Loss = 0.34615702199190257, Val Loss = 10.23179835455204\n",
      "11100.iterasyon, Train Loss = 0.34524446307704343, Val Loss = 10.239697492147299\n",
      "11200.iterasyon, Train Loss = 0.344353337934516, Val Loss = 10.247501837529445\n",
      "11300.iterasyon, Train Loss = 0.34348306900132713, Val Loss = 10.25521237170615\n",
      "11400.iterasyon, Train Loss = 0.34263309467801384, Val Loss = 10.262830071934712\n",
      "11500.iterasyon, Train Loss = 0.34180286888281924, Val Loss = 10.270355911489728\n",
      "11600.iterasyon, Train Loss = 0.34099186061839926, Val Loss = 10.277790859441827\n",
      "11700.iterasyon, Train Loss = 0.34019955355066833, Val Loss = 10.285135880447566\n",
      "11800.iterasyon, Train Loss = 0.339425445599479, Val Loss = 10.2923919345498\n",
      "11900.iterasyon, Train Loss = 0.33866904854077334, Val Loss = 10.299559976988295\n",
      "12000.iterasyon, Train Loss = 0.33792988761989234, Val Loss = 10.306640958020168\n",
      "12100.iterasyon, Train Loss = 0.3372075011757503, Val Loss = 10.313635822749857\n",
      "12200.iterasyon, Train Loss = 0.33650144027553813, Val Loss = 10.320545510968246\n",
      "12300.iterasyon, Train Loss = 0.33581126835967057, Val Loss = 10.327370957000708\n",
      "12400.iterasyon, Train Loss = 0.33513656089669364, Val Loss = 10.334113089563715\n",
      "12500.iterasyon, Train Loss = 0.3344769050478844, Val Loss = 10.340772831629522\n",
      "12600.iterasyon, Train Loss = 0.33383189934122964, Val Loss = 10.34735110029907\n",
      "12700.iterasyon, Train Loss = 0.33320115335456363, Val Loss = 10.3538488066824\n",
      "12800.iterasyon, Train Loss = 0.3325842874075937, Val Loss = 10.360266855786449\n",
      "12900.iterasyon, Train Loss = 0.33198093226254316, Val Loss = 10.366606146410083\n",
      "13000.iterasyon, Train Loss = 0.3313907288332019, Val Loss = 10.372867571045965\n",
      "13100.iterasyon, Train Loss = 0.33081332790212153, Val Loss = 10.379052015789048\n",
      "13200.iterasyon, Train Loss = 0.33024838984574817, Val Loss = 10.385160360251403\n",
      "13300.iterasyon, Train Loss = 0.32969558436725205, Val Loss = 10.391193477483302\n",
      "13400.iterasyon, Train Loss = 0.329154590236834, Val Loss = 10.397152233900202\n",
      "13500.iterasyon, Train Loss = 0.3286250950393403, Val Loss = 10.403037489215349\n",
      "13600.iterasyon, Train Loss = 0.32810679492891287, Val Loss = 10.40885009637795\n",
      "13700.iterasyon, Train Loss = 0.3275993943905389, Val Loss = 10.414590901516698\n",
      "13800.iterasyon, Train Loss = 0.32710260600826674, Val Loss = 10.420260743888253\n",
      "13900.iterasyon, Train Loss = 0.32661615023992235, Val Loss = 10.425860455830655\n",
      "14000.iterasyon, Train Loss = 0.3261397551981323, Val Loss = 10.431390862721571\n",
      "14100.iterasyon, Train Loss = 0.3256731564374795, Val Loss = 10.436852782940893\n",
      "14200.iterasyon, Train Loss = 0.32521609674762136, Val Loss = 10.442247027837805\n",
      "14300.iterasyon, Train Loss = 0.32476832595219957, Val Loss = 10.447574401702022\n",
      "14400.iterasyon, Train Loss = 0.32432960071338246, Val Loss = 10.452835701739007\n",
      "14500.iterasyon, Train Loss = 0.32389968434187133, Val Loss = 10.458031718049082\n",
      "14600.iterasyon, Train Loss = 0.32347834661224356, Val Loss = 10.463163233610254\n",
      "14700.iterasyon, Train Loss = 0.3230653635834358, Val Loss = 10.46823102426459\n",
      "14800.iterasyon, Train Loss = 0.32266051742428536, Val Loss = 10.473235858707998\n",
      "14900.iterasyon, Train Loss = 0.3222635962439251, Val Loss = 10.478178498483281\n",
      "15000.iterasyon, Train Loss = 0.32187439392693756, Val Loss = 10.483059697976428\n",
      "15100.iterasyon, Train Loss = 0.32149270997312357, Val Loss = 10.487880204415726\n",
      "15200.iterasyon, Train Loss = 0.3211183493417425, Val Loss = 10.492640757873955\n",
      "15300.iterasyon, Train Loss = 0.3207511223001222, Val Loss = 10.497342091273234\n",
      "15400.iterasyon, Train Loss = 0.32039084427649395, Val Loss = 10.50198493039254\n",
      "15500.iterasyon, Train Loss = 0.32003733571694776, Val Loss = 10.506569993877783\n",
      "15600.iterasyon, Train Loss = 0.31969042194638403, Val Loss = 10.511097993254259\n",
      "15700.iterasyon, Train Loss = 0.3193499330333582, Val Loss = 10.515569632941462\n",
      "15800.iterasyon, Train Loss = 0.31901570365869625, Val Loss = 10.519985610270083\n",
      "15900.iterasyon, Train Loss = 0.3186875729877909, Val Loss = 10.524346615501166\n",
      "16000.iterasyon, Train Loss = 0.31836538454646174, Val Loss = 10.528653331847158\n",
      "16100.iterasyon, Train Loss = 0.31804898610028515, Val Loss = 10.5329064354951\n",
      "16200.iterasyon, Train Loss = 0.3177382295373028, Val Loss = 10.537106595631386\n",
      "16300.iterasyon, Train Loss = 0.3174329707539903, Val Loss = 10.541254474468523\n",
      "16400.iterasyon, Train Loss = 0.3171330695444328, Val Loss = 10.545350727273382\n",
      "16500.iterasyon, Train Loss = 0.3168383894925798, Val Loss = 10.549396002397062\n",
      "16600.iterasyon, Train Loss = 0.31654879786751683, Val Loss = 10.55339094130638\n",
      "16700.iterasyon, Train Loss = 0.3162641655216581, Val Loss = 10.557336178616595\n",
      "16800.iterasyon, Train Loss = 0.31598436679178066, Val Loss = 10.561232342125683\n",
      "16900.iterasyon, Train Loss = 0.31570927940283416, Val Loss = 10.565080052849662\n",
      "17000.iterasyon, Train Loss = 0.31543878437440503, Val Loss = 10.56887992505945\n",
      "17100.iterasyon, Train Loss = 0.3151727659298347, Val Loss = 10.572632566318553\n",
      "17200.iterasyon, Train Loss = 0.31491111140782635, Val Loss = 10.57633857752209\n",
      "17300.iterasyon, Train Loss = 0.31465371117656005, Val Loss = 10.579998552936692\n",
      "17400.iterasyon, Train Loss = 0.3144004585501689, Val Loss = 10.583613080241378\n",
      "17500.iterasyon, Train Loss = 0.3141512497075754, Val Loss = 10.5871827405695\n",
      "17600.iterasyon, Train Loss = 0.313905983613563, Val Loss = 10.590708108551315\n",
      "17700.iterasyon, Train Loss = 0.3136645619420703, Val Loss = 10.594189752357533\n",
      "17800.iterasyon, Train Loss = 0.31342688900160803, Val Loss = 10.597628233743581\n",
      "17900.iterasyon, Train Loss = 0.3131928716627694, Val Loss = 10.601024108094455\n",
      "18000.iterasyon, Train Loss = 0.3129624192877419, Val Loss = 10.60437792447042\n",
      "18100.iterasyon, Train Loss = 0.31273544366180916, Val Loss = 10.607690225653132\n",
      "18200.iterasyon, Train Loss = 0.31251185892672867, Val Loss = 10.610961548192508\n",
      "18300.iterasyon, Train Loss = 0.31229158151599096, Val Loss = 10.61419242245396\n",
      "18400.iterasyon, Train Loss = 0.31207453009187375, Val Loss = 10.617383372666207\n",
      "18500.iterasyon, Train Loss = 0.31186062548424726, Val Loss = 10.620534916969454\n",
      "18600.iterasyon, Train Loss = 0.3116497906310786, Val Loss = 10.623647567464126\n",
      "18700.iterasyon, Train Loss = 0.3114419505206025, Val Loss = 10.62672183025976\n",
      "18800.iterasyon, Train Loss = 0.311237032135094, Val Loss = 10.629758205524391\n",
      "18900.iterasyon, Train Loss = 0.31103496439620626, Val Loss = 10.632757187534196\n",
      "19000.iterasyon, Train Loss = 0.31083567811183316, Val Loss = 10.635719264723397\n",
      "19100.iterasyon, Train Loss = 0.31063910592444416, Val Loss = 10.638644919734363\n",
      "19200.iterasyon, Train Loss = 0.31044518226085377, Val Loss = 10.641534629468016\n",
      "19300.iterasyon, Train Loss = 0.3102538432833993, Val Loss = 10.644388865134374\n",
      "19400.iterasyon, Train Loss = 0.3100650268424659, Val Loss = 10.647208092303103\n",
      "19500.iterasyon, Train Loss = 0.30987867243032646, Val Loss = 10.64999277095449\n",
      "19600.iterasyon, Train Loss = 0.3096947211362804, Val Loss = 10.652743355530234\n",
      "19700.iterasyon, Train Loss = 0.309513115603023, Val Loss = 10.655460294984426\n",
      "19800.iterasyon, Train Loss = 0.30933379998422605, Val Loss = 10.658144032834658\n",
      "19900.iterasyon, Train Loss = 0.30915671990331534, Val Loss = 10.660795007212986\n",
      "20000.iterasyon, Train Loss = 0.30898182241336625, Val Loss = 10.663413650917013\n",
      "20100.iterasyon, Train Loss = 0.30880905595813046, Val Loss = 10.66600039146101\n",
      "20200.iterasyon, Train Loss = 0.30863837033414027, Val Loss = 10.668555651126836\n",
      "20300.iterasyon, Train Loss = 0.30846971665386014, Val Loss = 10.671079847014951\n",
      "20400.iterasyon, Train Loss = 0.3083030473098589, Val Loss = 10.673573391095324\n",
      "20500.iterasyon, Train Loss = 0.30813831593998126, Val Loss = 10.676036690258163\n",
      "20600.iterasyon, Train Loss = 0.3079754773934881, Val Loss = 10.67847014636471\n",
      "20700.iterasyon, Train Loss = 0.3078144876981137, Val Loss = 10.680874156297792\n",
      "20800.iterasyon, Train Loss = 0.3076553040280639, Val Loss = 10.683249112012167\n",
      "20900.iterasyon, Train Loss = 0.30749788467288736, Val Loss = 10.685595400584978\n",
      "21000.iterasyon, Train Loss = 0.3073421890071932, Val Loss = 10.687913404265776\n",
      "21100.iterasyon, Train Loss = 0.3071881774612448, Val Loss = 10.6902035005265\n",
      "21200.iterasyon, Train Loss = 0.3070358114923268, Val Loss = 10.692466062111233\n",
      "21300.iterasyon, Train Loss = 0.30688505355692935, Val Loss = 10.694701457085843\n",
      "21400.iterasyon, Train Loss = 0.30673586708369777, Val Loss = 10.696910048887196\n",
      "21500.iterasyon, Train Loss = 0.30658821644712114, Val Loss = 10.699092196372424\n",
      "21600.iterasyon, Train Loss = 0.3064420669419563, Val Loss = 10.70124825386767\n",
      "21700.iterasyon, Train Loss = 0.30629738475835655, Val Loss = 10.70337857121693\n",
      "21800.iterasyon, Train Loss = 0.30615413695769006, Val Loss = 10.705483493830236\n",
      "21900.iterasyon, Train Loss = 0.3060122914490176, Val Loss = 10.70756336273194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000.iterasyon, Train Loss = 0.30587181696623195, Val Loss = 10.709618514608593\n",
      "22100.iterasyon, Train Loss = 0.30573268304581547, Val Loss = 10.711649281856388\n",
      "22200.iterasyon, Train Loss = 0.30559486000522146, Val Loss = 10.713655992628668\n",
      "22300.iterasyon, Train Loss = 0.30545831892184616, Val Loss = 10.71563897088275\n",
      "22400.iterasyon, Train Loss = 0.3053230316125863, Val Loss = 10.717598536426824\n",
      "22500.iterasyon, Train Loss = 0.30518897061395184, Val Loss = 10.719535004966158\n",
      "22600.iterasyon, Train Loss = 0.305056109162731, Val Loss = 10.72144868814941\n",
      "22700.iterasyon, Train Loss = 0.30492442117719243, Val Loss = 10.723339893614252\n",
      "22800.iterasyon, Train Loss = 0.30479388123879675, Val Loss = 10.725208925032934\n",
      "22900.iterasyon, Train Loss = 0.3046644645744138, Val Loss = 10.727056082157437\n",
      "23000.iterasyon, Train Loss = 0.3045361470390372, Val Loss = 10.728881660864173\n",
      "23100.iterasyon, Train Loss = 0.3044089050989621, Val Loss = 10.730685953198583\n",
      "23200.iterasyon, Train Loss = 0.3042827158154321, Val Loss = 10.732469247419232\n",
      "23300.iterasyon, Train Loss = 0.30415755682873863, Val Loss = 10.7342318280416\n",
      "23400.iterasyon, Train Loss = 0.3040334063427509, Val Loss = 10.735973975881539\n",
      "23500.iterasyon, Train Loss = 0.3039102431098671, Val Loss = 10.737695968098464\n",
      "23600.iterasyon, Train Loss = 0.3037880464164024, Val Loss = 10.739398078237956\n",
      "23700.iterasyon, Train Loss = 0.30366679606833946, Val Loss = 10.741080576274364\n",
      "23800.iterasyon, Train Loss = 0.3035464723775027, Val Loss = 10.742743728652682\n",
      "23900.iterasyon, Train Loss = 0.30342705614810106, Val Loss = 10.744387798330381\n",
      "24000.iterasyon, Train Loss = 0.30330852866363256, Val Loss = 10.746013044818659\n",
      "24100.iterasyon, Train Loss = 0.30319087167415854, Val Loss = 10.747619724223506\n",
      "24200.iterasyon, Train Loss = 0.3030740673839196, Val Loss = 10.74920808928625\n",
      "24300.iterasyon, Train Loss = 0.3029580984392875, Val Loss = 10.750778389423873\n",
      "24400.iterasyon, Train Loss = 0.30284294791706234, Val Loss = 10.752330870768908\n",
      "24500.iterasyon, Train Loss = 0.3027285993130712, Val Loss = 10.753865776208919\n",
      "24600.iterasyon, Train Loss = 0.3026150365310901, Val Loss = 10.755383345425743\n",
      "24700.iterasyon, Train Loss = 0.3025022438720597, Val Loss = 10.75688381493426\n",
      "24800.iterasyon, Train Loss = 0.30239020602361005, Val Loss = 10.758367418120867\n",
      "24900.iterasyon, Train Loss = 0.3022789080498514, Val Loss = 10.759834385281527\n",
      "25000.iterasyon, Train Loss = 0.3021683353814669, Val Loss = 10.761284943659478\n",
      "25100.iterasyon, Train Loss = 0.30205847380605316, Val Loss = 10.762719317482604\n",
      "25200.iterasyon, Train Loss = 0.301949309458736, Val Loss = 10.764137728000422\n",
      "25300.iterasyon, Train Loss = 0.3018408288130442, Val Loss = 10.765540393520737\n",
      "25400.iterasyon, Train Loss = 0.3017330186720255, Val Loss = 10.766927529445843\n",
      "25500.iterasyon, Train Loss = 0.30162586615959863, Val Loss = 10.768299348308501\n",
      "25600.iterasyon, Train Loss = 0.3015193587121673, Val Loss = 10.769656059807353\n",
      "25700.iterasyon, Train Loss = 0.3014134840704246, Val Loss = 10.770997870842283\n",
      "25800.iterasyon, Train Loss = 0.3013082302714076, Val Loss = 10.772324985549055\n",
      "25900.iterasyon, Train Loss = 0.3012035856407593, Val Loss = 10.773637605333878\n",
      "26000.iterasyon, Train Loss = 0.3010995387852002, Val Loss = 10.774935928907517\n",
      "26100.iterasyon, Train Loss = 0.30099607858520033, Val Loss = 10.776220152319034\n",
      "26200.iterasyon, Train Loss = 0.30089319418786215, Val Loss = 10.777490468989134\n",
      "26300.iterasyon, Train Loss = 0.30079087499998053, Val Loss = 10.778747069743302\n",
      "26400.iterasyon, Train Loss = 0.3006891106813039, Val Loss = 10.779990142844387\n",
      "26500.iterasyon, Train Loss = 0.3005878911379713, Val Loss = 10.781219874025073\n",
      "26600.iterasyon, Train Loss = 0.3004872065161264, Val Loss = 10.782436446519814\n",
      "26700.iterasyon, Train Loss = 0.3003870471957079, Val Loss = 10.78364004109643\n",
      "26800.iterasyon, Train Loss = 0.3002874037844064, Val Loss = 10.784830836087538\n",
      "26900.iterasyon, Train Loss = 0.3001882671117803, Val Loss = 10.786009007421438\n",
      "27000.iterasyon, Train Loss = 0.300089628223539, Val Loss = 10.787174728652783\n",
      "27100.iterasyon, Train Loss = 0.29999147837597356, Val Loss = 10.788328170992852\n",
      "27200.iterasyon, Train Loss = 0.29989380903053847, Val Loss = 10.78946950333948\n",
      "27300.iterasyon, Train Loss = 0.2997966118485815, Val Loss = 10.79059889230675\n",
      "27400.iterasyon, Train Loss = 0.29969987868621384, Val Loss = 10.791716502254213\n",
      "27500.iterasyon, Train Loss = 0.2996036015893167, Val Loss = 10.792822495315885\n",
      "27600.iterasyon, Train Loss = 0.2995077727886886, Val Loss = 10.793917031428922\n",
      "27700.iterasyon, Train Loss = 0.29941238469531567, Val Loss = 10.795000268361823\n",
      "27800.iterasyon, Train Loss = 0.29931742989576987, Val Loss = 10.796072361742553\n",
      "27900.iterasyon, Train Loss = 0.29922290114773736, Val Loss = 10.797133465086098\n",
      "28000.iterasyon, Train Loss = 0.2991287913756597, Val Loss = 10.798183729821842\n",
      "28100.iterasyon, Train Loss = 0.2990350936664893, Val Loss = 10.799223305320714\n",
      "28200.iterasyon, Train Loss = 0.2989418012655746, Val Loss = 10.800252338921727\n",
      "28300.iterasyon, Train Loss = 0.29884890757263094, Val Loss = 10.801270975958575\n",
      "28400.iterasyon, Train Loss = 0.2987564061378411, Val Loss = 10.802279359785576\n",
      "28500.iterasyon, Train Loss = 0.29866429065804945, Val Loss = 10.803277631803606\n",
      "28600.iterasyon, Train Loss = 0.29857255497305807, Val Loss = 10.804265931485475\n",
      "28700.iterasyon, Train Loss = 0.2984811930620195, Val Loss = 10.805244396401282\n",
      "28800.iterasyon, Train Loss = 0.29839019903993813, Val Loss = 10.806213162243177\n",
      "28900.iterasyon, Train Loss = 0.29829956715424333, Val Loss = 10.80717236284999\n",
      "29000.iterasyon, Train Loss = 0.29820929178148164, Val Loss = 10.808122130231592\n",
      "29100.iterasyon, Train Loss = 0.2981193674240626, Val Loss = 10.80906259459293\n",
      "29200.iterasyon, Train Loss = 0.29802978870712354, Val Loss = 10.809993884357656\n",
      "29300.iterasyon, Train Loss = 0.29794055037546224, Val Loss = 10.810916126191701\n",
      "29400.iterasyon, Train Loss = 0.2978516472905454, Val Loss = 10.811829445026357\n",
      "29500.iterasyon, Train Loss = 0.2977630744276088, Val Loss = 10.812733964081227\n",
      "29600.iterasyon, Train Loss = 0.29767482687282626, Val Loss = 10.813629804886807\n",
      "29700.iterasyon, Train Loss = 0.29758689982056014, Val Loss = 10.814517087306811\n",
      "29800.iterasyon, Train Loss = 0.2974992885706732, Val Loss = 10.815395929560221\n",
      "29900.iterasyon, Train Loss = 0.2974119885259237, Val Loss = 10.81626644824321\n",
      "30000.iterasyon, Train Loss = 0.29732499518942473, Val Loss = 10.817128758350467\n",
      "30100.iterasyon, Train Loss = 0.29723830416216424, Val Loss = 10.817982973296658\n",
      "30200.iterasyon, Train Loss = 0.2971519111405995, Val Loss = 10.818829204937279\n",
      "30300.iterasyon, Train Loss = 0.2970658119143134, Val Loss = 10.819667563589508\n",
      "30400.iterasyon, Train Loss = 0.29698000236372396, Val Loss = 10.820498158052626\n",
      "30500.iterasyon, Train Loss = 0.2968944784578647, Val Loss = 10.82132109562829\n",
      "30600.iterasyon, Train Loss = 0.29680923625221783, Val Loss = 10.822136482140513\n",
      "30700.iterasyon, Train Loss = 0.29672427188660055, Val Loss = 10.822944421955382\n",
      "30800.iterasyon, Train Loss = 0.2966395815831184, Val Loss = 10.823745018000555\n",
      "30900.iterasyon, Train Loss = 0.2965551616441617, Val Loss = 10.82453837178452\n",
      "31000.iterasyon, Train Loss = 0.29647100845045277, Val Loss = 10.825324583415583\n",
      "31100.iterasyon, Train Loss = 0.29638711845915283, Val Loss = 10.826103751620618\n",
      "31200.iterasyon, Train Loss = 0.29630348820201197, Val Loss = 10.826875973763707\n",
      "31300.iterasyon, Train Loss = 0.2962201142835762, Val Loss = 10.827641345864263\n",
      "31400.iterasyon, Train Loss = 0.2961369933794201, Val Loss = 10.828399962615201\n",
      "31500.iterasyon, Train Loss = 0.29605412223445604, Val Loss = 10.829151917400807\n",
      "31600.iterasyon, Train Loss = 0.2959714976612565, Val Loss = 10.82989730231426\n",
      "31700.iterasyon, Train Loss = 0.29588911653844185, Val Loss = 10.830636208175077\n",
      "31800.iterasyon, Train Loss = 0.2958069758091013, Val Loss = 10.831368724546278\n",
      "31900.iterasyon, Train Loss = 0.2957250724792492, Val Loss = 10.83209493975134\n",
      "32000.iterasyon, Train Loss = 0.29564340361633934, Val Loss = 10.832814940890863\n",
      "32100.iterasyon, Train Loss = 0.2955619663477906, Val Loss = 10.833528813859179\n",
      "32200.iterasyon, Train Loss = 0.295480757859579, Val Loss = 10.834236643360638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32300.iterasyon, Train Loss = 0.29539977539484724, Val Loss = 10.834938512925628\n",
      "32400.iterasyon, Train Loss = 0.29531901625254814, Val Loss = 10.835634504926615\n",
      "32500.iterasyon, Train Loss = 0.29523847778614104, Val Loss = 10.836324700593652\n",
      "32600.iterasyon, Train Loss = 0.2951581574023065, Val Loss = 10.837009180029991\n",
      "32700.iterasyon, Train Loss = 0.29507805255969627, Val Loss = 10.837688022227347\n",
      "32800.iterasyon, Train Loss = 0.2949981607677171, Val Loss = 10.83836130508097\n",
      "32900.iterasyon, Train Loss = 0.2949184795853457, Val Loss = 10.83902910540454\n",
      "33000.iterasyon, Train Loss = 0.2948390066199784, Val Loss = 10.839691498944807\n",
      "33100.iterasyon, Train Loss = 0.29475973952629414, Val Loss = 10.84034856039622\n",
      "33200.iterasyon, Train Loss = 0.2946806760051713, Val Loss = 10.841000363415139\n",
      "33300.iterasyon, Train Loss = 0.2946018138026027, Val Loss = 10.841646980634055\n",
      "33400.iterasyon, Train Loss = 0.2945231507086686, Val Loss = 10.842288483675471\n",
      "33500.iterasyon, Train Loss = 0.2944446845565131, Val Loss = 10.842924943165675\n",
      "33600.iterasyon, Train Loss = 0.29436641322135165, Val Loss = 10.84355642874838\n",
      "33700.iterasyon, Train Loss = 0.2942883346195116, Val Loss = 10.844183009098089\n",
      "33800.iterasyon, Train Loss = 0.29421044670748925, Val Loss = 10.844804751933326\n",
      "33900.iterasyon, Train Loss = 0.29413274748102797, Val Loss = 10.845421724029745\n",
      "34000.iterasyon, Train Loss = 0.29405523497423797, Val Loss = 10.846033991232941\n",
      "34100.iterasyon, Train Loss = 0.2939779072587047, Val Loss = 10.846641618471233\n",
      "34200.iterasyon, Train Loss = 0.2939007624426564, Val Loss = 10.847244669768175\n",
      "34300.iterasyon, Train Loss = 0.2938237986701227, Val Loss = 10.847843208255027\n",
      "34400.iterasyon, Train Loss = 0.29374701412014115, Val Loss = 10.848437296182821\n",
      "34500.iterasyon, Train Loss = 0.2936704070059532, Val Loss = 10.849026994934546\n",
      "34600.iterasyon, Train Loss = 0.29359397557424444, Val Loss = 10.849612365037054\n",
      "34700.iterasyon, Train Loss = 0.29351771810440136, Val Loss = 10.85019346617274\n",
      "34800.iterasyon, Train Loss = 0.29344163290776903, Val Loss = 10.850770357191168\n",
      "34900.iterasyon, Train Loss = 0.29336571832694347, Val Loss = 10.851343096120504\n",
      "35000.iterasyon, Train Loss = 0.293289972735082, Val Loss = 10.851911740178762\n",
      "35100.iterasyon, Train Loss = 0.2932143945352202, Val Loss = 10.852476345785036\n",
      "35200.iterasyon, Train Loss = 0.29313898215961015, Val Loss = 10.853036968570311\n",
      "35300.iterasyon, Train Loss = 0.29306373406908093, Val Loss = 10.853593663388526\n",
      "35400.iterasyon, Train Loss = 0.2929886487524043, Val Loss = 10.854146484327062\n",
      "35500.iterasyon, Train Loss = 0.2929137247256891, Val Loss = 10.85469548471743\n",
      "35600.iterasyon, Train Loss = 0.29283896053176867, Val Loss = 10.85524071714562\n",
      "35700.iterasyon, Train Loss = 0.29276435473963885, Val Loss = 10.855782233462424\n",
      "35800.iterasyon, Train Loss = 0.2926899059438633, Val Loss = 10.856320084793484\n",
      "35900.iterasyon, Train Loss = 0.29261561276404185, Val Loss = 10.856854321549323\n",
      "36000.iterasyon, Train Loss = 0.2925414738442526, Val Loss = 10.857384993435307\n",
      "36100.iterasyon, Train Loss = 0.2924674878525256, Val Loss = 10.857912149461246\n",
      "36200.iterasyon, Train Loss = 0.2923936534803326, Val Loss = 10.858435837951056\n",
      "36300.iterasyon, Train Loss = 0.29231996944208033, Val Loss = 10.858956106552233\n",
      "36400.iterasyon, Train Loss = 0.29224643447461446, Val Loss = 10.859473002245164\n",
      "36500.iterasyon, Train Loss = 0.29217304733674937, Val Loss = 10.859986571352389\n",
      "36600.iterasyon, Train Loss = 0.29209980680878966, Val Loss = 10.860496859547695\n",
      "36700.iterasyon, Train Loss = 0.29202671169208405, Val Loss = 10.861003911865039\n",
      "36800.iterasyon, Train Loss = 0.29195376080856866, Val Loss = 10.861507772707464\n",
      "36900.iterasyon, Train Loss = 0.29188095300034056, Val Loss = 10.862008485855773\n",
      "37000.iterasyon, Train Loss = 0.2918082871292263, Val Loss = 10.8625060944772\n",
      "37100.iterasyon, Train Loss = 0.2917357620763737, Val Loss = 10.863000641133858\n",
      "37200.iterasyon, Train Loss = 0.29166337674183934, Val Loss = 10.863492167791158\n",
      "37300.iterasyon, Train Loss = 0.29159113004420106, Val Loss = 10.863980715826042\n",
      "37400.iterasyon, Train Loss = 0.29151902092016707, Val Loss = 10.864466326035197\n",
      "37500.iterasyon, Train Loss = 0.2914470483241993, Val Loss = 10.864949038643054\n",
      "37600.iterasyon, Train Loss = 0.2913752112281455, Val Loss = 10.865428893309705\n",
      "37700.iterasyon, Train Loss = 0.29130350862088544, Val Loss = 10.865905929138812\n",
      "37800.iterasyon, Train Loss = 0.29123193950797216, Val Loss = 10.866380184685244\n",
      "37900.iterasyon, Train Loss = 0.2911605029112906, Val Loss = 10.86685169796281\n",
      "38000.iterasyon, Train Loss = 0.2910891978687305, Val Loss = 10.867320506451629\n",
      "38100.iterasyon, Train Loss = 0.2910180234338473, Val Loss = 10.867786647105678\n",
      "38200.iterasyon, Train Loss = 0.29094697867555497, Val Loss = 10.868250156359954\n",
      "38300.iterasyon, Train Loss = 0.29087606267780913, Val Loss = 10.868711070137897\n",
      "38400.iterasyon, Train Loss = 0.29080527453929744, Val Loss = 10.86916942385824\n",
      "38500.iterasyon, Train Loss = 0.29073461337315076, Val Loss = 10.86962525244231\n",
      "38600.iterasyon, Train Loss = 0.2906640783066459, Val Loss = 10.87007859032075\n",
      "38700.iterasyon, Train Loss = 0.29059366848092594, Val Loss = 10.87052947144036\n",
      "38800.iterasyon, Train Loss = 0.2905233830507199, Val Loss = 10.870977929270909\n",
      "38900.iterasyon, Train Loss = 0.2904532211840708, Val Loss = 10.871423996811782\n",
      "39000.iterasyon, Train Loss = 0.2903831820620783, Val Loss = 10.871867706598422\n",
      "39100.iterasyon, Train Loss = 0.2903132648786269, Val Loss = 10.872309090708931\n",
      "39200.iterasyon, Train Loss = 0.2902434688401417, Val Loss = 10.8727481807703\n",
      "39300.iterasyon, Train Loss = 0.2901737931653444, Val Loss = 10.873185007964842\n",
      "39400.iterasyon, Train Loss = 0.2901042370849993, Val Loss = 10.873619603036254\n",
      "39500.iterasyon, Train Loss = 0.2900347998416899, Val Loss = 10.874051996295814\n",
      "39600.iterasyon, Train Loss = 0.28996548068957817, Val Loss = 10.874482217628398\n",
      "39700.iterasyon, Train Loss = 0.2898962788941876, Val Loss = 10.87491029649843\n",
      "39800.iterasyon, Train Loss = 0.28982719373217203, Val Loss = 10.875336261955697\n",
      "39900.iterasyon, Train Loss = 0.28975822449111005, Val Loss = 10.875760142641248\n",
      "40000.iterasyon, Train Loss = 0.2896893704692913, Val Loss = 10.876181966792975\n",
      "40100.iterasyon, Train Loss = 0.28962063097550417, Val Loss = 10.876601762251275\n",
      "40200.iterasyon, Train Loss = 0.2895520053288399, Val Loss = 10.877019556464674\n",
      "40300.iterasyon, Train Loss = 0.28948349285849767, Val Loss = 10.877435376495255\n",
      "40400.iterasyon, Train Loss = 0.2894150929035876, Val Loss = 10.877849249023964\n",
      "40500.iterasyon, Train Loss = 0.2893468048129377, Val Loss = 10.878261200356132\n",
      "40600.iterasyon, Train Loss = 0.2892786279449207, Val Loss = 10.878671256426493\n",
      "40700.iterasyon, Train Loss = 0.2892105616672651, Val Loss = 10.879079442804585\n",
      "40800.iterasyon, Train Loss = 0.289142605356882, Val Loss = 10.879485784699655\n",
      "40900.iterasyon, Train Loss = 0.2890747583996882, Val Loss = 10.879890306965935\n",
      "41000.iterasyon, Train Loss = 0.28900702019044816, Val Loss = 10.880293034107309\n",
      "41100.iterasyon, Train Loss = 0.28893939013259456, Val Loss = 10.880693990282502\n",
      "41200.iterasyon, Train Loss = 0.2888718676380779, Val Loss = 10.881093199309698\n",
      "41300.iterasyon, Train Loss = 0.28880445212720224, Val Loss = 10.881490684671412\n",
      "41400.iterasyon, Train Loss = 0.2887371430284729, Val Loss = 10.881886469519149\n",
      "41500.iterasyon, Train Loss = 0.2886699397784443, Val Loss = 10.882280576678085\n",
      "41600.iterasyon, Train Loss = 0.2886028418215723, Val Loss = 10.882673028651599\n",
      "41700.iterasyon, Train Loss = 0.28853584861007003, Val Loss = 10.883063847625744\n",
      "41800.iterasyon, Train Loss = 0.28846895960376034, Val Loss = 10.883453055473757\n",
      "41900.iterasyon, Train Loss = 0.2884021742699476, Val Loss = 10.883840673760412\n",
      "42000.iterasyon, Train Loss = 0.2883354920832714, Val Loss = 10.884226723746353\n",
      "42100.iterasyon, Train Loss = 0.28826891252558035, Val Loss = 10.884611226392316\n",
      "42200.iterasyon, Train Loss = 0.2882024350857972, Val Loss = 10.884994202363405\n",
      "42300.iterasyon, Train Loss = 0.2881360592597943, Val Loss = 10.88537567203315\n",
      "42400.iterasyon, Train Loss = 0.2880697845502647, Val Loss = 10.885755655487674\n",
      "42500.iterasyon, Train Loss = 0.2880036104666082, Val Loss = 10.88613417252968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42600.iterasyon, Train Loss = 0.2879375365248005, Val Loss = 10.886511242682419\n",
      "42700.iterasyon, Train Loss = 0.28787156224728744, Val Loss = 10.886886885193615\n",
      "42800.iterasyon, Train Loss = 0.2878056871628593, Val Loss = 10.88726111903935\n",
      "42900.iterasyon, Train Loss = 0.28773991080654787, Val Loss = 10.887633962927875\n",
      "43000.iterasyon, Train Loss = 0.28767423271950715, Val Loss = 10.88800543530335\n",
      "43100.iterasyon, Train Loss = 0.28760865244891487, Val Loss = 10.88837555434953\n",
      "43200.iterasyon, Train Loss = 0.2875431695478623, Val Loss = 10.888744337993497\n",
      "43300.iterasyon, Train Loss = 0.28747778357524584, Val Loss = 10.889111803909154\n",
      "43400.iterasyon, Train Loss = 0.28741249409567554, Val Loss = 10.889477969520868\n",
      "43500.iterasyon, Train Loss = 0.2873473006793702, Val Loss = 10.889842852006973\n",
      "43600.iterasyon, Train Loss = 0.2872822029020587, Val Loss = 10.890206468303129\n",
      "43700.iterasyon, Train Loss = 0.28721720034488807, Val Loss = 10.890568835105846\n",
      "43800.iterasyon, Train Loss = 0.28715229259432956, Val Loss = 10.890929968875756\n",
      "43900.iterasyon, Train Loss = 0.2870874792420841, Val Loss = 10.891289885840996\n",
      "44000.iterasyon, Train Loss = 0.28702275988499687, Val Loss = 10.891648602000444\n",
      "44100.iterasyon, Train Loss = 0.286958134124967, Val Loss = 10.892006133126939\n",
      "44200.iterasyon, Train Loss = 0.2868936015688586, Val Loss = 10.892362494770413\n",
      "44300.iterasyon, Train Loss = 0.2868291618284216, Val Loss = 10.892717702261146\n",
      "44400.iterasyon, Train Loss = 0.28676481452021174, Val Loss = 10.893071770712718\n",
      "44500.iterasyon, Train Loss = 0.2867005592654961, Val Loss = 10.893424715025116\n",
      "44600.iterasyon, Train Loss = 0.28663639569018823, Val Loss = 10.893776549887768\n",
      "44700.iterasyon, Train Loss = 0.2865723234247559, Val Loss = 10.894127289782473\n",
      "44800.iterasyon, Train Loss = 0.28650834210416365, Val Loss = 10.894476948986286\n",
      "44900.iterasyon, Train Loss = 0.2864444513677771, Val Loss = 10.89482554157445\n",
      "45000.iterasyon, Train Loss = 0.28638065085930664, Val Loss = 10.895173081423215\n",
      "45100.iterasyon, Train Loss = 0.2863169402267233, Val Loss = 10.895519582212629\n",
      "45200.iterasyon, Train Loss = 0.2862533191221923, Val Loss = 10.895865057429306\n",
      "45300.iterasyon, Train Loss = 0.2861897872020045, Val Loss = 10.896209520369196\n",
      "45400.iterasyon, Train Loss = 0.2861263441265115, Val Loss = 10.896552984140145\n",
      "45500.iterasyon, Train Loss = 0.2860629895600533, Val Loss = 10.896895461664696\n",
      "45600.iterasyon, Train Loss = 0.2859997231708894, Val Loss = 10.897236965682541\n",
      "45700.iterasyon, Train Loss = 0.285936544631149, Val Loss = 10.897577508753281\n",
      "45800.iterasyon, Train Loss = 0.28587345361675526, Val Loss = 10.897917103258749\n",
      "45900.iterasyon, Train Loss = 0.28581044980736725, Val Loss = 10.898255761405668\n",
      "46000.iterasyon, Train Loss = 0.2857475328863219, Val Loss = 10.898593495228061\n",
      "46100.iterasyon, Train Loss = 0.2856847025405718, Val Loss = 10.898930316589668\n",
      "46200.iterasyon, Train Loss = 0.28562195846061983, Val Loss = 10.89926623718641\n",
      "46300.iterasyon, Train Loss = 0.28555930034048554, Val Loss = 10.899601268548677\n",
      "46400.iterasyon, Train Loss = 0.28549672787761726, Val Loss = 10.8999354220437\n",
      "46500.iterasyon, Train Loss = 0.2854342407728623, Val Loss = 10.90026870887782\n",
      "46600.iterasyon, Train Loss = 0.2853718387304015, Val Loss = 10.900601140098805\n",
      "46700.iterasyon, Train Loss = 0.285309521457694, Val Loss = 10.900932726598043\n",
      "46800.iterasyon, Train Loss = 0.2852472886654349, Val Loss = 10.901263479112774\n",
      "46900.iterasyon, Train Loss = 0.2851851400674954, Val Loss = 10.901593408228187\n",
      "47000.iterasyon, Train Loss = 0.2851230753808753, Val Loss = 10.9019225243797\n",
      "47100.iterasyon, Train Loss = 0.285061094325655, Val Loss = 10.902250837854904\n",
      "47200.iterasyon, Train Loss = 0.2849991966249448, Val Loss = 10.90257835879584\n",
      "47300.iterasyon, Train Loss = 0.28493738200484203, Val Loss = 10.902905097200843\n",
      "47400.iterasyon, Train Loss = 0.2848756501943747, Val Loss = 10.903231062926768\n",
      "47500.iterasyon, Train Loss = 0.28481400092547215, Val Loss = 10.903556265690792\n",
      "47600.iterasyon, Train Loss = 0.28475243393290034, Val Loss = 10.903880715072537\n",
      "47700.iterasyon, Train Loss = 0.28469094895423425, Val Loss = 10.90420442051593\n",
      "47800.iterasyon, Train Loss = 0.28462954572980237, Val Loss = 10.904527391331165\n",
      "47900.iterasyon, Train Loss = 0.2845682240026543, Val Loss = 10.904849636696529\n",
      "48000.iterasyon, Train Loss = 0.2845069835185097, Val Loss = 10.905171165660317\n",
      "48100.iterasyon, Train Loss = 0.2844458240257285, Val Loss = 10.905491987142618\n",
      "48200.iterasyon, Train Loss = 0.2843847452752542, Val Loss = 10.905812109937216\n",
      "48300.iterasyon, Train Loss = 0.28432374702059304, Val Loss = 10.90613154271321\n",
      "48400.iterasyon, Train Loss = 0.2842628290177586, Val Loss = 10.906450294016967\n",
      "48500.iterasyon, Train Loss = 0.284201991025245, Val Loss = 10.906768372273682\n",
      "48600.iterasyon, Train Loss = 0.28414123280398645, Val Loss = 10.90708578578919\n",
      "48700.iterasyon, Train Loss = 0.2840805541173137, Val Loss = 10.907402542751637\n",
      "48800.iterasyon, Train Loss = 0.28401995473092684, Val Loss = 10.907718651233099\n",
      "48900.iterasyon, Train Loss = 0.28395943441285604, Val Loss = 10.908034119191276\n",
      "49000.iterasyon, Train Loss = 0.28389899293342546, Val Loss = 10.90834895447105\n",
      "49100.iterasyon, Train Loss = 0.28383863006522, Val Loss = 10.908663164806097\n",
      "49200.iterasyon, Train Loss = 0.2837783455830499, Val Loss = 10.908976757820536\n",
      "49300.iterasyon, Train Loss = 0.2837181392639208, Val Loss = 10.90928974103033\n",
      "49400.iterasyon, Train Loss = 0.283658010887002, Val Loss = 10.909602121844898\n",
      "49500.iterasyon, Train Loss = 0.28359796023358597, Val Loss = 10.909913907568624\n",
      "49600.iterasyon, Train Loss = 0.28353798708706585, Val Loss = 10.910225105402343\n",
      "49700.iterasyon, Train Loss = 0.28347809123289963, Val Loss = 10.91053572244479\n",
      "49800.iterasyon, Train Loss = 0.28341827245858386, Val Loss = 10.91084576569398\n",
      "49900.iterasyon, Train Loss = 0.2833585305536167, Val Loss = 10.911155242048768\n",
      "50000.iterasyon, Train Loss = 0.2832988653094751, Val Loss = 10.911464158310107\n",
      "50100.iterasyon, Train Loss = 0.28323927651958714, Val Loss = 10.911772521182499\n",
      "50200.iterasyon, Train Loss = 0.2831797639792922, Val Loss = 10.91208033727539\n",
      "50300.iterasyon, Train Loss = 0.28312032748582733, Val Loss = 10.912387613104466\n",
      "50400.iterasyon, Train Loss = 0.2830609668382888, Val Loss = 10.91269435509295\n",
      "50500.iterasyon, Train Loss = 0.28300168183761193, Val Loss = 10.913000569572981\n",
      "50600.iterasyon, Train Loss = 0.28294247228654285, Val Loss = 10.91330626278684\n",
      "50700.iterasyon, Train Loss = 0.282883337989608, Val Loss = 10.913611440888292\n",
      "50800.iterasyon, Train Loss = 0.2828242787530942, Val Loss = 10.913916109943726\n",
      "50900.iterasyon, Train Loss = 0.2827652943850224, Val Loss = 10.914220275933507\n",
      "51000.iterasyon, Train Loss = 0.2827063846951175, Val Loss = 10.914523944753094\n",
      "51100.iterasyon, Train Loss = 0.28264754949478954, Val Loss = 10.914827122214332\n",
      "51200.iterasyon, Train Loss = 0.28258878859711106, Val Loss = 10.91512981404654\n",
      "51300.iterasyon, Train Loss = 0.2825301018167865, Val Loss = 10.915432025897745\n",
      "51400.iterasyon, Train Loss = 0.28247148897013147, Val Loss = 10.915733763335803\n",
      "51500.iterasyon, Train Loss = 0.28241294987505755, Val Loss = 10.916035031849553\n",
      "51600.iterasyon, Train Loss = 0.2823544843510377, Val Loss = 10.916335836849916\n",
      "51700.iterasyon, Train Loss = 0.28229609221909113, Val Loss = 10.916636183670994\n",
      "51800.iterasyon, Train Loss = 0.2822377733017592, Val Loss = 10.916936077571163\n",
      "51900.iterasyon, Train Loss = 0.282179527423086, Val Loss = 10.917235523734155\n",
      "52000.iterasyon, Train Loss = 0.2821213544085983, Val Loss = 10.917534527270098\n",
      "52100.iterasyon, Train Loss = 0.28206325408527805, Val Loss = 10.917833093216545\n",
      "52200.iterasyon, Train Loss = 0.2820052262815507, Val Loss = 10.918131226539506\n",
      "52300.iterasyon, Train Loss = 0.2819472708272576, Val Loss = 10.918428932134534\n",
      "52400.iterasyon, Train Loss = 0.28188938755364323, Val Loss = 10.91872621482759\n",
      "52500.iterasyon, Train Loss = 0.28183157629332767, Val Loss = 10.919023079376132\n",
      "52600.iterasyon, Train Loss = 0.2817738368802957, Val Loss = 10.91931953047003\n",
      "52700.iterasyon, Train Loss = 0.28171616914987135, Val Loss = 10.919615572732544\n",
      "52800.iterasyon, Train Loss = 0.2816585729387043, Val Loss = 10.919911210721247\n",
      "52900.iterasyon, Train Loss = 0.2816010480847476, Val Loss = 10.920206448929036\n",
      "53000.iterasyon, Train Loss = 0.2815435944272446, Val Loss = 10.920501291784891\n",
      "53100.iterasyon, Train Loss = 0.281486211806703, Val Loss = 10.920795743654903\n",
      "53200.iterasyon, Train Loss = 0.28142890006488686, Val Loss = 10.921089808843124\n",
      "53300.iterasyon, Train Loss = 0.2813716590447964, Val Loss = 10.921383491592497\n",
      "53400.iterasyon, Train Loss = 0.2813144885906464, Val Loss = 10.921676796085583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53500.iterasyon, Train Loss = 0.28125738854785365, Val Loss = 10.921969726445576\n",
      "53600.iterasyon, Train Loss = 0.2812003587630235, Val Loss = 10.922262286737052\n",
      "53700.iterasyon, Train Loss = 0.2811433990839248, Val Loss = 10.922554480966838\n",
      "53800.iterasyon, Train Loss = 0.28108650935948765, Val Loss = 10.922846313084776\n",
      "53900.iterasyon, Train Loss = 0.28102968943977413, Val Loss = 10.923137786984586\n",
      "54000.iterasyon, Train Loss = 0.28097293917596877, Val Loss = 10.923428906504652\n",
      "54100.iterasyon, Train Loss = 0.2809162584203651, Val Loss = 10.923719675428774\n",
      "54200.iterasyon, Train Loss = 0.2808596470263507, Val Loss = 10.924010097486939\n",
      "54300.iterasyon, Train Loss = 0.28080310484838894, Val Loss = 10.924300176356143\n",
      "54400.iterasyon, Train Loss = 0.28074663174200554, Val Loss = 10.924589915661082\n",
      "54500.iterasyon, Train Loss = 0.2806902275637777, Val Loss = 10.924879318974899\n",
      "54600.iterasyon, Train Loss = 0.28063389217131707, Val Loss = 10.925168389819948\n",
      "54700.iterasyon, Train Loss = 0.28057762542325737, Val Loss = 10.9254571316685\n",
      "54800.iterasyon, Train Loss = 0.28052142717923795, Val Loss = 10.925745547943439\n",
      "54900.iterasyon, Train Loss = 0.28046529729989594, Val Loss = 10.926033642018913\n",
      "55000.iterasyon, Train Loss = 0.28040923564684805, Val Loss = 10.92632141722116\n",
      "55100.iterasyon, Train Loss = 0.28035324208267876, Val Loss = 10.926608876829045\n",
      "55200.iterasyon, Train Loss = 0.2802973164709302, Val Loss = 10.926896024074814\n",
      "55300.iterasyon, Train Loss = 0.28024145867608696, Val Loss = 10.927182862144747\n",
      "55400.iterasyon, Train Loss = 0.28018566856356397, Val Loss = 10.927469394179749\n",
      "55500.iterasyon, Train Loss = 0.28012994599969526, Val Loss = 10.927755623276058\n",
      "55600.iterasyon, Train Loss = 0.2800742908517228, Val Loss = 10.928041552485853\n",
      "55700.iterasyon, Train Loss = 0.28001870298777926, Val Loss = 10.928327184817908\n",
      "55800.iterasyon, Train Loss = 0.27996318227688544, Val Loss = 10.92861252323812\n",
      "55900.iterasyon, Train Loss = 0.27990772858893054, Val Loss = 10.928897570670248\n",
      "56000.iterasyon, Train Loss = 0.2798523417946672, Val Loss = 10.929182329996376\n",
      "56100.iterasyon, Train Loss = 0.2797970217656921, Val Loss = 10.929466804057615\n",
      "56200.iterasyon, Train Loss = 0.2797417683744481, Val Loss = 10.929750995654615\n",
      "56300.iterasyon, Train Loss = 0.279686581494198, Val Loss = 10.930034907548173\n",
      "56400.iterasyon, Train Loss = 0.2796314609990266, Val Loss = 10.930318542459732\n",
      "56500.iterasyon, Train Loss = 0.27957640676382395, Val Loss = 10.930601903072045\n",
      "56600.iterasyon, Train Loss = 0.2795214186642729, Val Loss = 10.930884992029645\n",
      "56700.iterasyon, Train Loss = 0.27946649657684747, Val Loss = 10.931167811939407\n",
      "56800.iterasyon, Train Loss = 0.279411640378795, Val Loss = 10.931450365371086\n",
      "56900.iterasyon, Train Loss = 0.27935684994812743, Val Loss = 10.931732654857859\n",
      "57000.iterasyon, Train Loss = 0.279302125163616, Val Loss = 10.932014682896783\n",
      "57100.iterasyon, Train Loss = 0.2792474659047785, Val Loss = 10.932296451949384\n",
      "57200.iterasyon, Train Loss = 0.2791928720518677, Val Loss = 10.932577964442148\n",
      "57300.iterasyon, Train Loss = 0.2791383434858676, Val Loss = 10.932859222766941\n",
      "57400.iterasyon, Train Loss = 0.27908388008847645, Val Loss = 10.933140229281609\n",
      "57500.iterasyon, Train Loss = 0.2790294817421053, Val Loss = 10.93342098631037\n",
      "57600.iterasyon, Train Loss = 0.2789751483298677, Val Loss = 10.933701496144323\n",
      "57700.iterasyon, Train Loss = 0.27892087973556834, Val Loss = 10.933981761041949\n",
      "57800.iterasyon, Train Loss = 0.2788666758436922, Val Loss = 10.934261783229537\n",
      "57900.iterasyon, Train Loss = 0.27881253653940413, Val Loss = 10.93454156490168\n",
      "58000.iterasyon, Train Loss = 0.27875846170853097, Val Loss = 10.934821108221625\n",
      "58100.iterasyon, Train Loss = 0.27870445123756293, Val Loss = 10.935100415321841\n",
      "58200.iterasyon, Train Loss = 0.2786505050136346, Val Loss = 10.935379488304363\n",
      "58300.iterasyon, Train Loss = 0.27859662292452636, Val Loss = 10.935658329241344\n",
      "58400.iterasyon, Train Loss = 0.2785428048586545, Val Loss = 10.935936940175282\n",
      "58500.iterasyon, Train Loss = 0.2784890507050603, Val Loss = 10.936215323119635\n",
      "58600.iterasyon, Train Loss = 0.27843536035339506, Val Loss = 10.936493480059179\n",
      "58700.iterasyon, Train Loss = 0.27838173369393987, Val Loss = 10.936771412950266\n",
      "58800.iterasyon, Train Loss = 0.2783281706175616, Val Loss = 10.93704912372149\n",
      "58900.iterasyon, Train Loss = 0.27827467101573183, Val Loss = 10.937326614273884\n",
      "59000.iterasyon, Train Loss = 0.2782212347805121, Val Loss = 10.937603886481327\n",
      "59100.iterasyon, Train Loss = 0.27816786180454184, Val Loss = 10.937880942191034\n",
      "59200.iterasyon, Train Loss = 0.2781145519810381, Val Loss = 10.938157783223872\n",
      "59300.iterasyon, Train Loss = 0.27806130520378597, Val Loss = 10.938434411374676\n",
      "59400.iterasyon, Train Loss = 0.2780081213671275, Val Loss = 10.93871082841275\n",
      "59500.iterasyon, Train Loss = 0.2779550003659648, Val Loss = 10.938987036082107\n",
      "59600.iterasyon, Train Loss = 0.27790194209574476, Val Loss = 10.939263036101865\n",
      "59700.iterasyon, Train Loss = 0.27784894645245595, Val Loss = 10.939538830166693\n",
      "59800.iterasyon, Train Loss = 0.27779601333261855, Val Loss = 10.939814419946995\n",
      "59900.iterasyon, Train Loss = 0.27774314263328465, Val Loss = 10.940089807089343\n",
      "60000.iterasyon, Train Loss = 0.2776903342520299, Val Loss = 10.940364993216834\n",
      "60100.iterasyon, Train Loss = 0.27763758808693784, Val Loss = 10.940639979929427\n",
      "60200.iterasyon, Train Loss = 0.277584904036612, Val Loss = 10.940914768804195\n",
      "60300.iterasyon, Train Loss = 0.2775322820001486, Val Loss = 10.941189361395711\n",
      "60400.iterasyon, Train Loss = 0.27747972187715225, Val Loss = 10.941463759236356\n",
      "60500.iterasyon, Train Loss = 0.27742722356771116, Val Loss = 10.941737963836617\n",
      "60600.iterasyon, Train Loss = 0.27737478697240553, Val Loss = 10.942011976685459\n",
      "60700.iterasyon, Train Loss = 0.2773224119922913, Val Loss = 10.942285799250556\n",
      "60800.iterasyon, Train Loss = 0.2772700985288994, Val Loss = 10.9425594329786\n",
      "60900.iterasyon, Train Loss = 0.2772178464842338, Val Loss = 10.94283287929566\n",
      "61000.iterasyon, Train Loss = 0.27716565576075886, Val Loss = 10.943106139607432\n",
      "61100.iterasyon, Train Loss = 0.2771135262614, Val Loss = 10.94337921529948\n",
      "61200.iterasyon, Train Loss = 0.2770614578895298, Val Loss = 10.94365210773768\n",
      "61300.iterasyon, Train Loss = 0.27700945054897486, Val Loss = 10.943924818268309\n",
      "61400.iterasyon, Train Loss = 0.27695750414400216, Val Loss = 10.944197348218436\n",
      "61500.iterasyon, Train Loss = 0.2769056185793193, Val Loss = 10.944469698896153\n",
      "61600.iterasyon, Train Loss = 0.27685379376005953, Val Loss = 10.944741871590884\n",
      "61700.iterasyon, Train Loss = 0.2768020295917873, Val Loss = 10.945013867573644\n",
      "61800.iterasyon, Train Loss = 0.27675032598049415, Val Loss = 10.945285688097231\n",
      "61900.iterasyon, Train Loss = 0.27669868283258264, Val Loss = 10.945557334396545\n",
      "62000.iterasyon, Train Loss = 0.2766471000548734, Val Loss = 10.945828807688843\n",
      "62100.iterasyon, Train Loss = 0.2765955775545907, Val Loss = 10.94610010917405\n",
      "62200.iterasyon, Train Loss = 0.27654411523936734, Val Loss = 10.946371240034818\n",
      "62300.iterasyon, Train Loss = 0.27649271301723327, Val Loss = 10.946642201437001\n",
      "62400.iterasyon, Train Loss = 0.2764413707966146, Val Loss = 10.946912994529669\n",
      "62500.iterasyon, Train Loss = 0.2763900884863257, Val Loss = 10.947183620445564\n",
      "62600.iterasyon, Train Loss = 0.2763388659955697, Val Loss = 10.947454080301174\n",
      "62700.iterasyon, Train Loss = 0.2762877032339292, Val Loss = 10.947724375197048\n",
      "62800.iterasyon, Train Loss = 0.2762366001113666, Val Loss = 10.947994506217938\n",
      "62900.iterasyon, Train Loss = 0.27618555653821575, Val Loss = 10.94826447443317\n",
      "63000.iterasyon, Train Loss = 0.2761345724251765, Val Loss = 10.948534280896718\n",
      "63100.iterasyon, Train Loss = 0.2760836476833219, Val Loss = 10.94880392664744\n",
      "63200.iterasyon, Train Loss = 0.2760327822240806, Val Loss = 10.949073412709426\n",
      "63300.iterasyon, Train Loss = 0.27598197595924095, Val Loss = 10.949342740091996\n",
      "63400.iterasyon, Train Loss = 0.275931228800941, Val Loss = 10.949611909790145\n",
      "63500.iterasyon, Train Loss = 0.27588054066167017, Val Loss = 10.949880922784551\n",
      "63600.iterasyon, Train Loss = 0.2758299114542648, Val Loss = 10.95014978004187\n",
      "63700.iterasyon, Train Loss = 0.27577934109190366, Val Loss = 10.950418482514934\n",
      "63800.iterasyon, Train Loss = 0.2757288294881021, Val Loss = 10.95068703114294\n",
      "63900.iterasyon, Train Loss = 0.2756783765567061, Val Loss = 10.950955426851637\n",
      "64000.iterasyon, Train Loss = 0.27562798221189927, Val Loss = 10.951223670553519\n",
      "64100.iterasyon, Train Loss = 0.27557764636819143, Val Loss = 10.951491763147988\n",
      "64200.iterasyon, Train Loss = 0.27552736894041047, Val Loss = 10.951759705521615\n",
      "64300.iterasyon, Train Loss = 0.2754771498437138, Val Loss = 10.952027498548194\n",
      "64400.iterasyon, Train Loss = 0.2754269889935669, Val Loss = 10.952295143089136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64500.iterasyon, Train Loss = 0.275376886305756, Val Loss = 10.95256263999339\n",
      "64600.iterasyon, Train Loss = 0.27532684169637217, Val Loss = 10.952829990097822\n",
      "64700.iterasyon, Train Loss = 0.27527685508181876, Val Loss = 10.953097194227245\n",
      "64800.iterasyon, Train Loss = 0.2752269263788008, Val Loss = 10.95336425319472\n",
      "64900.iterasyon, Train Loss = 0.27517705550431987, Val Loss = 10.953631167801644\n",
      "65000.iterasyon, Train Loss = 0.2751272423756834, Val Loss = 10.953897938837887\n",
      "65100.iterasyon, Train Loss = 0.2750774869104862, Val Loss = 10.95416456708205\n",
      "65200.iterasyon, Train Loss = 0.2750277890266145, Val Loss = 10.954431053301555\n",
      "65300.iterasyon, Train Loss = 0.2749781486422525, Val Loss = 10.954697398252803\n",
      "65400.iterasyon, Train Loss = 0.2749285656758566, Val Loss = 10.954963602681415\n",
      "65500.iterasyon, Train Loss = 0.27487904004617586, Val Loss = 10.955229667322266\n",
      "65600.iterasyon, Train Loss = 0.2748295716722352, Val Loss = 10.955495592899727\n",
      "65700.iterasyon, Train Loss = 0.27478016047333165, Val Loss = 10.955761380127765\n",
      "65800.iterasyon, Train Loss = 0.27473080636904745, Val Loss = 10.956027029710114\n",
      "65900.iterasyon, Train Loss = 0.2746815092792229, Val Loss = 10.956292542340453\n",
      "66000.iterasyon, Train Loss = 0.2746322691239768, Val Loss = 10.956557918702458\n",
      "66100.iterasyon, Train Loss = 0.27458308582369084, Val Loss = 10.956823159470021\n",
      "66200.iterasyon, Train Loss = 0.2745339592990076, Val Loss = 10.957088265307403\n",
      "66300.iterasyon, Train Loss = 0.27448488947083133, Val Loss = 10.95735323686926\n",
      "66400.iterasyon, Train Loss = 0.274435876260325, Val Loss = 10.957618074800948\n",
      "66500.iterasyon, Train Loss = 0.27438691958890676, Val Loss = 10.957882779738489\n",
      "66600.iterasyon, Train Loss = 0.2743380193782446, Val Loss = 10.958147352308877\n",
      "66700.iterasyon, Train Loss = 0.2742891755502635, Val Loss = 10.958411793129972\n",
      "66800.iterasyon, Train Loss = 0.27424038802712997, Val Loss = 10.958676102810875\n",
      "66900.iterasyon, Train Loss = 0.2741916567312554, Val Loss = 10.958940281951909\n",
      "67000.iterasyon, Train Loss = 0.2741429815853037, Val Loss = 10.95920433114479\n",
      "67100.iterasyon, Train Loss = 0.27409436251216307, Val Loss = 10.959468250972748\n",
      "67200.iterasyon, Train Loss = 0.2740457994349747, Val Loss = 10.959732042010613\n",
      "67300.iterasyon, Train Loss = 0.2739972922771136, Val Loss = 10.959995704824973\n",
      "67400.iterasyon, Train Loss = 0.2739488409621796, Val Loss = 10.96025923997433\n",
      "67500.iterasyon, Train Loss = 0.2739004454140126, Val Loss = 10.960522648009114\n",
      "67600.iterasyon, Train Loss = 0.2738521055566803, Val Loss = 10.960785929471866\n",
      "67700.iterasyon, Train Loss = 0.2738038213144732, Val Loss = 10.961049084897327\n",
      "67800.iterasyon, Train Loss = 0.2737555926119139, Val Loss = 10.961312114812605\n",
      "67900.iterasyon, Train Loss = 0.27370741937374055, Val Loss = 10.961575019737175\n",
      "68000.iterasyon, Train Loss = 0.27365930152491624, Val Loss = 10.961837800183089\n",
      "68100.iterasyon, Train Loss = 0.2736112389906224, Val Loss = 10.962100456655055\n",
      "68200.iterasyon, Train Loss = 0.2735632316962583, Val Loss = 10.962362989650483\n",
      "68300.iterasyon, Train Loss = 0.27351527956743343, Val Loss = 10.962625399659702\n",
      "68400.iterasyon, Train Loss = 0.27346738252997294, Val Loss = 10.962887687165969\n",
      "68500.iterasyon, Train Loss = 0.27341954050991474, Val Loss = 10.96314985264556\n",
      "68600.iterasyon, Train Loss = 0.2733717534335005, Val Loss = 10.963411896567965\n",
      "68700.iterasyon, Train Loss = 0.27332402122718463, Val Loss = 10.96367381939592\n",
      "68800.iterasyon, Train Loss = 0.2732763438176194, Val Loss = 10.963935621585474\n",
      "68900.iterasyon, Train Loss = 0.2732287211316679, Val Loss = 10.964197303586174\n",
      "69000.iterasyon, Train Loss = 0.2731811530963885, Val Loss = 10.964458865841047\n",
      "69100.iterasyon, Train Loss = 0.27313363963904214, Val Loss = 10.964720308786813\n",
      "69200.iterasyon, Train Loss = 0.2730861806870849, Val Loss = 10.964981632853888\n",
      "69300.iterasyon, Train Loss = 0.2730387761681736, Val Loss = 10.965242838466487\n",
      "69400.iterasyon, Train Loss = 0.27299142601015375, Val Loss = 10.965503926042746\n",
      "69500.iterasyon, Train Loss = 0.2729441301410657, Val Loss = 10.965764895994802\n",
      "69600.iterasyon, Train Loss = 0.27289688848914256, Val Loss = 10.966025748728816\n",
      "69700.iterasyon, Train Loss = 0.2728497009828041, Val Loss = 10.96628648464518\n",
      "69800.iterasyon, Train Loss = 0.2728025675506549, Val Loss = 10.96654710413853\n",
      "69900.iterasyon, Train Loss = 0.2727554881214945, Val Loss = 10.96680760759774\n",
      "70000.iterasyon, Train Loss = 0.2727084626242953, Val Loss = 10.967067995406161\n",
      "70100.iterasyon, Train Loss = 0.27266149098821796, Val Loss = 10.967328267941644\n",
      "70200.iterasyon, Train Loss = 0.2726145731426072, Val Loss = 10.967588425576544\n",
      "70300.iterasyon, Train Loss = 0.27256770901698196, Val Loss = 10.967848468677948\n",
      "70400.iterasyon, Train Loss = 0.27252089854104145, Val Loss = 10.968108397607585\n",
      "70500.iterasyon, Train Loss = 0.27247414164466005, Val Loss = 10.968368212722025\n",
      "70600.iterasyon, Train Loss = 0.2724274382578899, Val Loss = 10.96862791437268\n",
      "70700.iterasyon, Train Loss = 0.27238078831095436, Val Loss = 10.968887502905943\n",
      "70800.iterasyon, Train Loss = 0.27233419173425016, Val Loss = 10.969146978663188\n",
      "70900.iterasyon, Train Loss = 0.2722876484583427, Val Loss = 10.96940634198091\n",
      "71000.iterasyon, Train Loss = 0.2722411584139673, Val Loss = 10.969665593190786\n",
      "71100.iterasyon, Train Loss = 0.272194721532032, Val Loss = 10.969924732619658\n",
      "71200.iterasyon, Train Loss = 0.27214833774360225, Val Loss = 10.970183760589718\n",
      "71300.iterasyon, Train Loss = 0.27210200697991554, Val Loss = 10.9704426774185\n",
      "71400.iterasyon, Train Loss = 0.27205572917237086, Val Loss = 10.970701483418972\n",
      "71500.iterasyon, Train Loss = 0.2720095042525315, Val Loss = 10.970960178899615\n",
      "71600.iterasyon, Train Loss = 0.27196333215211715, Val Loss = 10.971218764164446\n",
      "71700.iterasyon, Train Loss = 0.271917212803011, Val Loss = 10.971477239513154\n",
      "71800.iterasyon, Train Loss = 0.27187114613725455, Val Loss = 10.971735605241088\n",
      "71900.iterasyon, Train Loss = 0.2718251320870493, Val Loss = 10.971993861639287\n",
      "72000.iterasyon, Train Loss = 0.2717791705847468, Val Loss = 10.97225200899475\n",
      "72100.iterasyon, Train Loss = 0.2717332615628599, Val Loss = 10.97251004759019\n",
      "72200.iterasyon, Train Loss = 0.27168740495404636, Val Loss = 10.972767977704413\n",
      "72300.iterasyon, Train Loss = 0.271641600691126, Val Loss = 10.973025799612069\n",
      "72400.iterasyon, Train Loss = 0.27159584870706505, Val Loss = 10.973283513583914\n",
      "72500.iterasyon, Train Loss = 0.27155014893498053, Val Loss = 10.973541119886844\n",
      "72600.iterasyon, Train Loss = 0.2715045013081379, Val Loss = 10.973798618783885\n",
      "72700.iterasyon, Train Loss = 0.27145890575995096, Val Loss = 10.974056010534245\n",
      "72800.iterasyon, Train Loss = 0.2714133622239821, Val Loss = 10.974313295393468\n",
      "72900.iterasyon, Train Loss = 0.27136787063393675, Val Loss = 10.974570473613387\n",
      "73000.iterasyon, Train Loss = 0.27132243092366604, Val Loss = 10.974827545442235\n",
      "73100.iterasyon, Train Loss = 0.27127704302716354, Val Loss = 10.975084511124614\n",
      "73200.iterasyon, Train Loss = 0.27123170687856507, Val Loss = 10.975341370901708\n",
      "73300.iterasyon, Train Loss = 0.27118642241215163, Val Loss = 10.97559812501115\n",
      "73400.iterasyon, Train Loss = 0.2711411895623399, Val Loss = 10.975854773687216\n",
      "73500.iterasyon, Train Loss = 0.2710960082636876, Val Loss = 10.976111317160758\n",
      "73600.iterasyon, Train Loss = 0.27105087845088893, Val Loss = 10.976367755659343\n",
      "73700.iterasyon, Train Loss = 0.2710058000587802, Val Loss = 10.976624089407261\n",
      "73800.iterasyon, Train Loss = 0.27096077302232857, Val Loss = 10.976880318625568\n",
      "73900.iterasyon, Train Loss = 0.2709157972766408, Val Loss = 10.977136443532142\n",
      "74000.iterasyon, Train Loss = 0.2708708727569559, Val Loss = 10.977392464341754\n",
      "74100.iterasyon, Train Loss = 0.2708259993986439, Val Loss = 10.977648381266095\n",
      "74200.iterasyon, Train Loss = 0.2707811771372132, Val Loss = 10.977904194513744\n",
      "74300.iterasyon, Train Loss = 0.27073640590829784, Val Loss = 10.97815990429033\n",
      "74400.iterasyon, Train Loss = 0.27069168564766694, Val Loss = 10.978415510798552\n",
      "74500.iterasyon, Train Loss = 0.2706470162912174, Val Loss = 10.978671014238156\n",
      "74600.iterasyon, Train Loss = 0.2706023977749745, Val Loss = 10.978926414806049\n",
      "74700.iterasyon, Train Loss = 0.2705578300350901, Val Loss = 10.979181712696281\n",
      "74800.iterasyon, Train Loss = 0.2705133130078488, Val Loss = 10.979436908100137\n",
      "74900.iterasyon, Train Loss = 0.2704688466296558, Val Loss = 10.979692001206146\n",
      "75000.iterasyon, Train Loss = 0.2704244308370441, Val Loss = 10.979946992200128\n",
      "75100.iterasyon, Train Loss = 0.27038006556667077, Val Loss = 10.980201881265241\n",
      "75200.iterasyon, Train Loss = 0.27033575075531674, Val Loss = 10.980456668582017\n",
      "75300.iterasyon, Train Loss = 0.27029148633988453, Val Loss = 10.980711354328406\n",
      "75400.iterasyon, Train Loss = 0.270247272257403, Val Loss = 10.980965938679763\n",
      "75500.iterasyon, Train Loss = 0.27020310844501655, Val Loss = 10.981220421808942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75600.iterasyon, Train Loss = 0.2701589948399926, Val Loss = 10.981474803886394\n",
      "75700.iterasyon, Train Loss = 0.2701149313797197, Val Loss = 10.981729085080035\n",
      "75800.iterasyon, Train Loss = 0.2700709180017085, Val Loss = 10.98198326555536\n",
      "75900.iterasyon, Train Loss = 0.27002695464357535, Val Loss = 10.982237345475582\n",
      "76000.iterasyon, Train Loss = 0.26998304124306854, Val Loss = 10.98249132500149\n",
      "76100.iterasyon, Train Loss = 0.2699391777380458, Val Loss = 10.982745204291634\n",
      "76200.iterasyon, Train Loss = 0.2698953640664805, Val Loss = 10.982998983502249\n",
      "76300.iterasyon, Train Loss = 0.2698516001664621, Val Loss = 10.983252662787352\n",
      "76400.iterasyon, Train Loss = 0.2698078859761987, Val Loss = 10.98350624229874\n",
      "76500.iterasyon, Train Loss = 0.26976422143400697, Val Loss = 10.983759722186036\n",
      "76600.iterasyon, Train Loss = 0.2697206064783188, Val Loss = 10.984013102596759\n",
      "76700.iterasyon, Train Loss = 0.2696770410476796, Val Loss = 10.984266383676264\n",
      "76800.iterasyon, Train Loss = 0.2696335250807424, Val Loss = 10.984519565567872\n",
      "76900.iterasyon, Train Loss = 0.2695900585162794, Val Loss = 10.984772648412829\n",
      "77000.iterasyon, Train Loss = 0.26954664129316236, Val Loss = 10.985025632350355\n",
      "77100.iterasyon, Train Loss = 0.2695032733503853, Val Loss = 10.985278517517699\n",
      "77200.iterasyon, Train Loss = 0.2694599546270394, Val Loss = 10.985531304050134\n",
      "77300.iterasyon, Train Loss = 0.26941668506233435, Val Loss = 10.985783992081034\n",
      "77400.iterasyon, Train Loss = 0.26937346459557926, Val Loss = 10.98603658174183\n",
      "77500.iterasyon, Train Loss = 0.26933029316620016, Val Loss = 10.986289073162075\n",
      "77600.iterasyon, Train Loss = 0.2692871707137175, Val Loss = 10.986541466469511\n",
      "77700.iterasyon, Train Loss = 0.26924409717776815, Val Loss = 10.986793761790027\n",
      "77800.iterasyon, Train Loss = 0.26920107249809216, Val Loss = 10.987045959247698\n",
      "77900.iterasyon, Train Loss = 0.26915809661453005, Val Loss = 10.987298058964921\n",
      "78000.iterasyon, Train Loss = 0.26911516946703157, Val Loss = 10.987550061062251\n",
      "78100.iterasyon, Train Loss = 0.26907229099564933, Val Loss = 10.987801965658573\n",
      "78200.iterasyon, Train Loss = 0.26902946114053394, Val Loss = 10.988053772871044\n",
      "78300.iterasyon, Train Loss = 0.26898667984194646, Val Loss = 10.988305482815212\n",
      "78400.iterasyon, Train Loss = 0.26894394704024505, Val Loss = 10.988557095604946\n",
      "78500.iterasyon, Train Loss = 0.26890126267589054, Val Loss = 10.988808611352482\n",
      "78600.iterasyon, Train Loss = 0.2688586266894441, Val Loss = 10.989060030168472\n",
      "78700.iterasyon, Train Loss = 0.26881603902156814, Val Loss = 10.98931135216205\n",
      "78800.iterasyon, Train Loss = 0.2687734996130244, Val Loss = 10.989562577440708\n",
      "78900.iterasyon, Train Loss = 0.2687310084046755, Val Loss = 10.989813706110466\n",
      "79000.iterasyon, Train Loss = 0.268688565337479, Val Loss = 10.99006473827587\n",
      "79100.iterasyon, Train Loss = 0.26864617035249355, Val Loss = 10.990315674039921\n",
      "79200.iterasyon, Train Loss = 0.26860382339087757, Val Loss = 10.990566513504204\n",
      "79300.iterasyon, Train Loss = 0.2685615243938835, Val Loss = 10.990817256768821\n",
      "79400.iterasyon, Train Loss = 0.2685192733028576, Val Loss = 10.991067903932516\n",
      "79500.iterasyon, Train Loss = 0.26847707005925125, Val Loss = 10.991318455092584\n",
      "79600.iterasyon, Train Loss = 0.2684349146046025, Val Loss = 10.99156891034504\n",
      "79700.iterasyon, Train Loss = 0.2683928068805541, Val Loss = 10.99181926978438\n",
      "79800.iterasyon, Train Loss = 0.26835074682883214, Val Loss = 10.992069533503939\n",
      "79900.iterasyon, Train Loss = 0.2683087343912676, Val Loss = 10.992319701595594\n",
      "80000.iterasyon, Train Loss = 0.2682667695097783, Val Loss = 10.992569774150041\n",
      "80100.iterasyon, Train Loss = 0.26822485212638053, Val Loss = 10.992819751256638\n",
      "80200.iterasyon, Train Loss = 0.2681829821831792, Val Loss = 10.993069633003497\n",
      "80300.iterasyon, Train Loss = 0.2681411596223759, Val Loss = 10.993319419477498\n",
      "80400.iterasyon, Train Loss = 0.2680993843862602, Val Loss = 10.993569110764293\n",
      "80500.iterasyon, Train Loss = 0.2680576564172187, Val Loss = 10.993818706948302\n",
      "80600.iterasyon, Train Loss = 0.26801597565772234, Val Loss = 10.994068208112848\n",
      "80700.iterasyon, Train Loss = 0.2679743420503385, Val Loss = 10.99431761433999\n",
      "80800.iterasyon, Train Loss = 0.2679327555377214, Val Loss = 10.994566925710705\n",
      "80900.iterasyon, Train Loss = 0.2678912160626193, Val Loss = 10.994816142304806\n",
      "81000.iterasyon, Train Loss = 0.2678497235678651, Val Loss = 10.995065264200981\n",
      "81100.iterasyon, Train Loss = 0.26780827799638435, Val Loss = 10.995314291476834\n",
      "81200.iterasyon, Train Loss = 0.26776687929119014, Val Loss = 10.99556322420894\n",
      "81300.iterasyon, Train Loss = 0.26772552739538386, Val Loss = 10.995812062472686\n",
      "81400.iterasyon, Train Loss = 0.2676842222521549, Val Loss = 10.996060806342525\n",
      "81500.iterasyon, Train Loss = 0.2676429638047824, Val Loss = 10.996309455891799\n",
      "81600.iterasyon, Train Loss = 0.2676017519966261, Val Loss = 10.996558011192871\n",
      "81700.iterasyon, Train Loss = 0.2675605867711423, Val Loss = 10.996806472317092\n",
      "81800.iterasyon, Train Loss = 0.2675194680718637, Val Loss = 10.997054839334828\n",
      "81900.iterasyon, Train Loss = 0.2674783958424167, Val Loss = 10.997303112315468\n",
      "82000.iterasyon, Train Loss = 0.2674373700265091, Val Loss = 10.997551291327424\n",
      "82100.iterasyon, Train Loss = 0.2673963905679373, Val Loss = 10.997799376438152\n",
      "82200.iterasyon, Train Loss = 0.2673554574105785, Val Loss = 10.998047367714245\n",
      "82300.iterasyon, Train Loss = 0.2673145704983976, Val Loss = 10.99829526522129\n",
      "82400.iterasyon, Train Loss = 0.26727372977544384, Val Loss = 10.998543069024082\n",
      "82500.iterasyon, Train Loss = 0.2672329351858493, Val Loss = 10.998790779186413\n",
      "82600.iterasyon, Train Loss = 0.2671921866738259, Val Loss = 10.99903839577127\n",
      "82700.iterasyon, Train Loss = 0.26715148418367773, Val Loss = 10.999285918840727\n",
      "82800.iterasyon, Train Loss = 0.26711082765978633, Val Loss = 10.999533348456033\n",
      "82900.iterasyon, Train Loss = 0.2670702170466101, Val Loss = 10.999780684677654\n",
      "83000.iterasyon, Train Loss = 0.26702965228870157, Val Loss = 11.000027927565135\n",
      "83100.iterasyon, Train Loss = 0.26698913333068663, Val Loss = 11.000275077177257\n",
      "83200.iterasyon, Train Loss = 0.2669486601172765, Val Loss = 11.000522133572014\n",
      "83300.iterasyon, Train Loss = 0.2669082325932611, Val Loss = 11.000769096806586\n",
      "83400.iterasyon, Train Loss = 0.26686785070351327, Val Loss = 11.001015966937366\n",
      "83500.iterasyon, Train Loss = 0.26682751439298485, Val Loss = 11.001262744020035\n",
      "83600.iterasyon, Train Loss = 0.2667872236067108, Val Loss = 11.001509428109454\n",
      "83700.iterasyon, Train Loss = 0.2667469782898017, Val Loss = 11.001756019259835\n",
      "83800.iterasyon, Train Loss = 0.26670677838745166, Val Loss = 11.002002517524573\n",
      "83900.iterasyon, Train Loss = 0.26666662384493217, Val Loss = 11.002248922956378\n",
      "84000.iterasyon, Train Loss = 0.2666265146075951, Val Loss = 11.002495235607247\n",
      "84100.iterasyon, Train Loss = 0.26658645062086855, Val Loss = 11.002741455528513\n",
      "84200.iterasyon, Train Loss = 0.2665464318302632, Val Loss = 11.00298758277077\n",
      "84300.iterasyon, Train Loss = 0.2665064581813649, Val Loss = 11.003233617383952\n",
      "84400.iterasyon, Train Loss = 0.2664665296198388, Val Loss = 11.003479559417352\n",
      "84500.iterasyon, Train Loss = 0.2664266460914259, Val Loss = 11.003725408919578\n",
      "84600.iterasyon, Train Loss = 0.2663868075419468, Val Loss = 11.003971165938644\n",
      "84700.iterasyon, Train Loss = 0.26634701391729587, Val Loss = 11.00421683052186\n",
      "84800.iterasyon, Train Loss = 0.2663072651634505, Val Loss = 11.004462402715962\n",
      "84900.iterasyon, Train Loss = 0.2662675612264589, Val Loss = 11.004707882567017\n",
      "85000.iterasyon, Train Loss = 0.26622790205244445, Val Loss = 11.004953270120584\n",
      "85100.iterasyon, Train Loss = 0.26618828758761653, Val Loss = 11.005198565421496\n",
      "85200.iterasyon, Train Loss = 0.26614871777824417, Val Loss = 11.005443768514118\n",
      "85300.iterasyon, Train Loss = 0.2661091925706887, Val Loss = 11.005688879442136\n",
      "85400.iterasyon, Train Loss = 0.26606971191137746, Val Loss = 11.00593389824874\n",
      "85500.iterasyon, Train Loss = 0.2660302757468113, Val Loss = 11.006178824976548\n",
      "85600.iterasyon, Train Loss = 0.2659908840235695, Val Loss = 11.006423659667579\n",
      "85700.iterasyon, Train Loss = 0.26595153668830723, Val Loss = 11.006668402363342\n",
      "85800.iterasyon, Train Loss = 0.2659122336877503, Val Loss = 11.006913053104816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85900.iterasyon, Train Loss = 0.2658729749687, Val Loss = 11.007157611932456\n",
      "86000.iterasyon, Train Loss = 0.26583376047803314, Val Loss = 11.007402078886164\n",
      "86100.iterasyon, Train Loss = 0.26579459016269696, Val Loss = 11.007646454005377\n",
      "86200.iterasyon, Train Loss = 0.26575546396971056, Val Loss = 11.007890737329015\n",
      "86300.iterasyon, Train Loss = 0.2657163818461739, Val Loss = 11.008134928895467\n",
      "86400.iterasyon, Train Loss = 0.2656773437392503, Val Loss = 11.008379028742679\n",
      "86500.iterasyon, Train Loss = 0.2656383495961836, Val Loss = 11.0086230369081\n",
      "86600.iterasyon, Train Loss = 0.2655993993642833, Val Loss = 11.008866953428736\n",
      "86700.iterasyon, Train Loss = 0.26556049299093676, Val Loss = 11.009110778341077\n",
      "86800.iterasyon, Train Loss = 0.2655216304235981, Val Loss = 11.009354511681186\n",
      "86900.iterasyon, Train Loss = 0.2654828116097973, Val Loss = 11.009598153484696\n",
      "87000.iterasyon, Train Loss = 0.26544403649713555, Val Loss = 11.009841703786732\n",
      "87100.iterasyon, Train Loss = 0.26540530503328047, Val Loss = 11.010085162622067\n",
      "87200.iterasyon, Train Loss = 0.26536661716597537, Val Loss = 11.010328530024973\n",
      "87300.iterasyon, Train Loss = 0.26532797284303417, Val Loss = 11.010571806029334\n",
      "87400.iterasyon, Train Loss = 0.2652893720123402, Val Loss = 11.01081499066863\n",
      "87500.iterasyon, Train Loss = 0.26525081462184746, Val Loss = 11.011058083975936\n",
      "87600.iterasyon, Train Loss = 0.26521230061957896, Val Loss = 11.011301085983861\n",
      "87700.iterasyon, Train Loss = 0.26517382995362787, Val Loss = 11.011543996724713\n",
      "87800.iterasyon, Train Loss = 0.26513540257216023, Val Loss = 11.011786816230321\n",
      "87900.iterasyon, Train Loss = 0.2650970184234081, Val Loss = 11.012029544532213\n",
      "88000.iterasyon, Train Loss = 0.2650586774556763, Val Loss = 11.012272181661459\n",
      "88100.iterasyon, Train Loss = 0.2650203796173328, Val Loss = 11.01251472764884\n",
      "88200.iterasyon, Train Loss = 0.26498212485682243, Val Loss = 11.012757182524709\n",
      "88300.iterasyon, Train Loss = 0.2649439131226538, Val Loss = 11.012999546319062\n",
      "88400.iterasyon, Train Loss = 0.2649057443634047, Val Loss = 11.013241819061589\n",
      "88500.iterasyon, Train Loss = 0.2648676185277233, Val Loss = 11.013484000781597\n",
      "88600.iterasyon, Train Loss = 0.26482953556432093, Val Loss = 11.013726091508056\n",
      "88700.iterasyon, Train Loss = 0.26479149542198693, Val Loss = 11.01396809126961\n",
      "88800.iterasyon, Train Loss = 0.2647534980495658, Val Loss = 11.014210000094574\n",
      "88900.iterasyon, Train Loss = 0.2647155433959787, Val Loss = 11.0144518180109\n",
      "89000.iterasyon, Train Loss = 0.26467763141021367, Val Loss = 11.014693545046265\n",
      "89100.iterasyon, Train Loss = 0.26463976204132, Val Loss = 11.014935181228026\n",
      "89200.iterasyon, Train Loss = 0.2646019352384205, Val Loss = 11.015176726583205\n",
      "89300.iterasyon, Train Loss = 0.2645641509507031, Val Loss = 11.015418181138543\n",
      "89400.iterasyon, Train Loss = 0.2645264091274225, Val Loss = 11.015659544920473\n",
      "89500.iterasyon, Train Loss = 0.2644887097178947, Val Loss = 11.015900817955131\n",
      "89600.iterasyon, Train Loss = 0.26445105267151403, Val Loss = 11.016142000268328\n",
      "89700.iterasyon, Train Loss = 0.2644134379377286, Val Loss = 11.016383091885679\n",
      "89800.iterasyon, Train Loss = 0.26437586546606034, Val Loss = 11.016624092832439\n",
      "89900.iterasyon, Train Loss = 0.2643383352060959, Val Loss = 11.016865003133619\n",
      "90000.iterasyon, Train Loss = 0.264300847107481, Val Loss = 11.017105822813969\n",
      "90100.iterasyon, Train Loss = 0.2642634011199383, Val Loss = 11.017346551897937\n",
      "90200.iterasyon, Train Loss = 0.26422599719324785, Val Loss = 11.017587190409763\n",
      "90300.iterasyon, Train Loss = 0.2641886352772557, Val Loss = 11.017827738373372\n",
      "90400.iterasyon, Train Loss = 0.26415131532187724, Val Loss = 11.018068195812464\n",
      "90500.iterasyon, Train Loss = 0.26411403727708765, Val Loss = 11.018308562750493\n",
      "90600.iterasyon, Train Loss = 0.26407680109293025, Val Loss = 11.018548839210661\n",
      "90700.iterasyon, Train Loss = 0.26403960671951426, Val Loss = 11.018789025215915\n",
      "90800.iterasyon, Train Loss = 0.2640024541070047, Val Loss = 11.019029120788996\n",
      "90900.iterasyon, Train Loss = 0.2639653432056443, Val Loss = 11.019269125952412\n",
      "91000.iterasyon, Train Loss = 0.2639282739657284, Val Loss = 11.019509040728416\n",
      "91100.iterasyon, Train Loss = 0.26389124633762245, Val Loss = 11.019748865139045\n",
      "91200.iterasyon, Train Loss = 0.26385426027175435, Val Loss = 11.01998859920612\n",
      "91300.iterasyon, Train Loss = 0.2638173157186136, Val Loss = 11.020228242951259\n",
      "91400.iterasyon, Train Loss = 0.26378041262875773, Val Loss = 11.020467796395852\n",
      "91500.iterasyon, Train Loss = 0.2637435509528052, Val Loss = 11.02070725956106\n",
      "91600.iterasyon, Train Loss = 0.2637067306414352, Val Loss = 11.020946632467878\n",
      "91700.iterasyon, Train Loss = 0.26366995164539425, Val Loss = 11.021185915137055\n",
      "91800.iterasyon, Train Loss = 0.2636332139154881, Val Loss = 11.021425107589188\n",
      "91900.iterasyon, Train Loss = 0.26359651740258916, Val Loss = 11.021664209844664\n",
      "92000.iterasyon, Train Loss = 0.26355986205763116, Val Loss = 11.021903221923662\n",
      "92100.iterasyon, Train Loss = 0.26352324783160846, Val Loss = 11.022142143846176\n",
      "92200.iterasyon, Train Loss = 0.2634866746755799, Val Loss = 11.022380975632023\n",
      "92300.iterasyon, Train Loss = 0.26345014254066407, Val Loss = 11.022619717300865\n",
      "92400.iterasyon, Train Loss = 0.2634136513780468, Val Loss = 11.022858368872116\n",
      "92500.iterasyon, Train Loss = 0.2633772011389716, Val Loss = 11.023096930365071\n",
      "92600.iterasyon, Train Loss = 0.2633407917747436, Val Loss = 11.023335401798835\n",
      "92700.iterasyon, Train Loss = 0.26330442323673087, Val Loss = 11.023573783192337\n",
      "92800.iterasyon, Train Loss = 0.26326809547636715, Val Loss = 11.023812074564358\n",
      "92900.iterasyon, Train Loss = 0.26323180844513694, Val Loss = 11.024050275933538\n",
      "93000.iterasyon, Train Loss = 0.26319556209459855, Val Loss = 11.024288387318302\n",
      "93100.iterasyon, Train Loss = 0.2631593563763635, Val Loss = 11.024526408736952\n",
      "93200.iterasyon, Train Loss = 0.2631231912421049, Val Loss = 11.024764340207598\n",
      "93300.iterasyon, Train Loss = 0.26308706664356124, Val Loss = 11.025002181748281\n",
      "93400.iterasyon, Train Loss = 0.2630509825325284, Val Loss = 11.025239933376792\n",
      "93500.iterasyon, Train Loss = 0.2630149388608636, Val Loss = 11.025477595110836\n",
      "93600.iterasyon, Train Loss = 0.2629789355804842, Val Loss = 11.02571516696796\n",
      "93700.iterasyon, Train Loss = 0.26294297264336913, Val Loss = 11.025952648965587\n",
      "93800.iterasyon, Train Loss = 0.26290705000155634, Val Loss = 11.026190041120978\n",
      "93900.iterasyon, Train Loss = 0.2628711676071441, Val Loss = 11.026427343451296\n",
      "94000.iterasyon, Train Loss = 0.26283532541229254, Val Loss = 11.026664555973499\n",
      "94100.iterasyon, Train Loss = 0.26279952336921897, Val Loss = 11.026901678704524\n",
      "94200.iterasyon, Train Loss = 0.2627637614302035, Val Loss = 11.027138711661069\n",
      "94300.iterasyon, Train Loss = 0.2627280395475823, Val Loss = 11.027375654859773\n",
      "94400.iterasyon, Train Loss = 0.26269235767375654, Val Loss = 11.02761250831711\n",
      "94500.iterasyon, Train Loss = 0.26265671576118027, Val Loss = 11.027849272049515\n",
      "94600.iterasyon, Train Loss = 0.2626211137623726, Val Loss = 11.028085946073189\n",
      "94700.iterasyon, Train Loss = 0.2625855516299069, Val Loss = 11.028322530404333\n",
      "94800.iterasyon, Train Loss = 0.2625500293164222, Val Loss = 11.028559025058938\n",
      "94900.iterasyon, Train Loss = 0.26251454677460906, Val Loss = 11.02879543005297\n",
      "95000.iterasyon, Train Loss = 0.2624791039572196, Val Loss = 11.029031745402218\n",
      "95100.iterasyon, Train Loss = 0.2624437008170693, Val Loss = 11.029267971122348\n",
      "95200.iterasyon, Train Loss = 0.2624083373070251, Val Loss = 11.02950410722899\n",
      "95300.iterasyon, Train Loss = 0.262373013380019, Val Loss = 11.029740153737642\n",
      "95400.iterasyon, Train Loss = 0.26233772898903734, Val Loss = 11.029976110663734\n",
      "95500.iterasyon, Train Loss = 0.2623024840871239, Val Loss = 11.030211978022532\n",
      "95600.iterasyon, Train Loss = 0.2622672786273851, Val Loss = 11.030447755829247\n",
      "95700.iterasyon, Train Loss = 0.2622321125629794, Val Loss = 11.030683444099008\n",
      "95800.iterasyon, Train Loss = 0.2621969858471302, Val Loss = 11.030919042846799\n",
      "95900.iterasyon, Train Loss = 0.2621618984331169, Val Loss = 11.03115455208758\n",
      "96000.iterasyon, Train Loss = 0.26212685027426996, Val Loss = 11.031389971836159\n",
      "96100.iterasyon, Train Loss = 0.2620918413239851, Val Loss = 11.031625302107322\n",
      "96200.iterasyon, Train Loss = 0.26205687153571483, Val Loss = 11.031860542915695\n",
      "96300.iterasyon, Train Loss = 0.2620219408629684, Val Loss = 11.032095694275878\n",
      "96400.iterasyon, Train Loss = 0.261987049259307, Val Loss = 11.032330756202379\n",
      "96500.iterasyon, Train Loss = 0.26195219667835734, Val Loss = 11.032565728709626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96600.iterasyon, Train Loss = 0.2619173830737977, Val Loss = 11.032800611811965\n",
      "96700.iterasyon, Train Loss = 0.26188260839936733, Val Loss = 11.033035405523657\n",
      "96800.iterasyon, Train Loss = 0.2618478726088597, Val Loss = 11.033270109858906\n",
      "96900.iterasyon, Train Loss = 0.26181317565612594, Val Loss = 11.033504724831829\n",
      "97000.iterasyon, Train Loss = 0.26177851749507164, Val Loss = 11.033739250456513\n",
      "97100.iterasyon, Train Loss = 0.26174389807966536, Val Loss = 11.033973686746867\n",
      "97200.iterasyon, Train Loss = 0.2617093173639239, Val Loss = 11.034208033716878\n",
      "97300.iterasyon, Train Loss = 0.26167477530192684, Val Loss = 11.034442291380342\n",
      "97400.iterasyon, Train Loss = 0.2616402718478086, Val Loss = 11.034676459751063\n",
      "97500.iterasyon, Train Loss = 0.26160580695575714, Val Loss = 11.03491053884278\n",
      "97600.iterasyon, Train Loss = 0.2615713805800209, Val Loss = 11.035144528669148\n",
      "97700.iterasyon, Train Loss = 0.26153699267490244, Val Loss = 11.035378429243744\n",
      "97800.iterasyon, Train Loss = 0.26150264319475763, Val Loss = 11.035612240580141\n",
      "97900.iterasyon, Train Loss = 0.26146833209400283, Val Loss = 11.0358459626918\n",
      "98000.iterasyon, Train Loss = 0.2614340593271064, Val Loss = 11.03607959559213\n",
      "98100.iterasyon, Train Loss = 0.26139982484859353, Val Loss = 11.036313139294538\n",
      "98200.iterasyon, Train Loss = 0.2613656286130478, Val Loss = 11.036546593812364\n",
      "98300.iterasyon, Train Loss = 0.2613314705751031, Val Loss = 11.03677995915886\n",
      "98400.iterasyon, Train Loss = 0.2612973506894529, Val Loss = 11.037013235347187\n",
      "98500.iterasyon, Train Loss = 0.2612632689108454, Val Loss = 11.037246422390602\n",
      "98600.iterasyon, Train Loss = 0.26122922519408137, Val Loss = 11.037479520302178\n",
      "98700.iterasyon, Train Loss = 0.2611952194940188, Val Loss = 11.037712529094994\n",
      "98800.iterasyon, Train Loss = 0.26116125176557226, Val Loss = 11.037945448782088\n",
      "98900.iterasyon, Train Loss = 0.2611273219637064, Val Loss = 11.03817827937646\n",
      "99000.iterasyon, Train Loss = 0.2610934300434458, Val Loss = 11.038411020891033\n",
      "99100.iterasyon, Train Loss = 0.2610595759598667, Val Loss = 11.038643673338713\n",
      "99200.iterasyon, Train Loss = 0.2610257596681012, Val Loss = 11.038876236732346\n",
      "99300.iterasyon, Train Loss = 0.260991981123337, Val Loss = 11.039108711084767\n",
      "99400.iterasyon, Train Loss = 0.26095824028081355, Val Loss = 11.039341096408815\n",
      "99500.iterasyon, Train Loss = 0.26092453709582847, Val Loss = 11.03957339271715\n",
      "99600.iterasyon, Train Loss = 0.2608908715237291, Val Loss = 11.039805600022543\n",
      "99700.iterasyon, Train Loss = 0.26085724351991874, Val Loss = 11.040037718337617\n",
      "99800.iterasyon, Train Loss = 0.2608236530398595, Val Loss = 11.040269747675058\n",
      "99900.iterasyon, Train Loss = 0.2607901000390569, Val Loss = 11.04050168804749\n",
      "100000.iterasyon, Train Loss = 0.2607565844730831, Val Loss = 11.040733539467409\n",
      "100100.iterasyon, Train Loss = 0.2607231062975565, Val Loss = 11.040965301947452\n",
      "100200.iterasyon, Train Loss = 0.2606896654681493, Val Loss = 11.041196975500092\n",
      "100300.iterasyon, Train Loss = 0.26065626194059005, Val Loss = 11.04142856013787\n",
      "100400.iterasyon, Train Loss = 0.26062289567065916, Val Loss = 11.0416600558732\n",
      "100500.iterasyon, Train Loss = 0.26058956661419225, Val Loss = 11.041891462718516\n",
      "100600.iterasyon, Train Loss = 0.2605562747270757, Val Loss = 11.04212278068624\n",
      "100700.iterasyon, Train Loss = 0.2605230199652554, Val Loss = 11.042354009788768\n",
      "100800.iterasyon, Train Loss = 0.26048980228472146, Val Loss = 11.042585150038438\n",
      "100900.iterasyon, Train Loss = 0.26045662164152333, Val Loss = 11.042816201447604\n",
      "101000.iterasyon, Train Loss = 0.2604234779917618, Val Loss = 11.043047164028582\n",
      "101100.iterasyon, Train Loss = 0.2603903712915926, Val Loss = 11.043278037793693\n",
      "101200.iterasyon, Train Loss = 0.2603573014972212, Val Loss = 11.043508822755182\n",
      "101300.iterasyon, Train Loss = 0.26032426856490753, Val Loss = 11.043739518925296\n",
      "101400.iterasyon, Train Loss = 0.26029127245096545, Val Loss = 11.04397012631625\n",
      "101500.iterasyon, Train Loss = 0.2602583131117598, Val Loss = 11.044200644940318\n",
      "101600.iterasyon, Train Loss = 0.260225390503711, Val Loss = 11.04443107480968\n",
      "101700.iterasyon, Train Loss = 0.26019250458328486, Val Loss = 11.04466141593655\n",
      "101800.iterasyon, Train Loss = 0.2601596553070088, Val Loss = 11.044891668333067\n",
      "101900.iterasyon, Train Loss = 0.26012684263145786, Val Loss = 11.045121832011372\n",
      "102000.iterasyon, Train Loss = 0.2600940665132594, Val Loss = 11.04535190698362\n",
      "102100.iterasyon, Train Loss = 0.26006132690909384, Val Loss = 11.045581893261971\n",
      "102200.iterasyon, Train Loss = 0.2600286237756924, Val Loss = 11.045811790858506\n",
      "102300.iterasyon, Train Loss = 0.25999595706984113, Val Loss = 11.046041599785358\n",
      "102400.iterasyon, Train Loss = 0.2599633267483772, Val Loss = 11.046271320054593\n",
      "102500.iterasyon, Train Loss = 0.25993073276818757, Val Loss = 11.046500951678304\n",
      "102600.iterasyon, Train Loss = 0.25989817508621205, Val Loss = 11.04673049466858\n",
      "102700.iterasyon, Train Loss = 0.25986565365944525, Val Loss = 11.04695994903746\n",
      "102800.iterasyon, Train Loss = 0.25983316844492726, Val Loss = 11.047189314797055\n",
      "102900.iterasyon, Train Loss = 0.2598007193997555, Val Loss = 11.04741859195938\n",
      "103000.iterasyon, Train Loss = 0.25976830648107957, Val Loss = 11.047647780536446\n",
      "103100.iterasyon, Train Loss = 0.259735929646093, Val Loss = 11.047876880540366\n",
      "103200.iterasyon, Train Loss = 0.2597035888520507, Val Loss = 11.048105891983088\n",
      "103300.iterasyon, Train Loss = 0.25967128405624756, Val Loss = 11.048334814876732\n",
      "103400.iterasyon, Train Loss = 0.25963901521604144, Val Loss = 11.04856364923321\n",
      "103500.iterasyon, Train Loss = 0.2596067822888356, Val Loss = 11.048792395064627\n",
      "103600.iterasyon, Train Loss = 0.25957458523208227, Val Loss = 11.049021052382964\n",
      "103700.iterasyon, Train Loss = 0.25954242400328986, Val Loss = 11.049249621200238\n",
      "103800.iterasyon, Train Loss = 0.25951029856001323, Val Loss = 11.049478101528493\n",
      "103900.iterasyon, Train Loss = 0.25947820885985895, Val Loss = 11.049706493379723\n",
      "104000.iterasyon, Train Loss = 0.2594461548604903, Val Loss = 11.049934796765912\n",
      "104100.iterasyon, Train Loss = 0.2594141365196126, Val Loss = 11.0501630116991\n",
      "104200.iterasyon, Train Loss = 0.2593821537949868, Val Loss = 11.05039113819127\n",
      "104300.iterasyon, Train Loss = 0.25935020664442515, Val Loss = 11.050619176254438\n",
      "104400.iterasyon, Train Loss = 0.2593182950257838, Val Loss = 11.050847125900601\n",
      "104500.iterasyon, Train Loss = 0.2592864188969779, Val Loss = 11.051074987141805\n",
      "104600.iterasyon, Train Loss = 0.25925457821597037, Val Loss = 11.051302759990083\n",
      "104700.iterasyon, Train Loss = 0.2592227729407705, Val Loss = 11.051530444457415\n",
      "104800.iterasyon, Train Loss = 0.2591910030294444, Val Loss = 11.051758040555809\n",
      "104900.iterasyon, Train Loss = 0.2591592684401004, Val Loss = 11.051985548297337\n",
      "105000.iterasyon, Train Loss = 0.25912756913090385, Val Loss = 11.052212967693997\n",
      "105100.iterasyon, Train Loss = 0.2590959050600685, Val Loss = 11.05244029875783\n",
      "105200.iterasyon, Train Loss = 0.25906427618585515, Val Loss = 11.05266754150085\n",
      "105300.iterasyon, Train Loss = 0.25903268246657646, Val Loss = 11.052894695935167\n",
      "105400.iterasyon, Train Loss = 0.25900112386059676, Val Loss = 11.053121762072792\n",
      "105500.iterasyon, Train Loss = 0.258969600326325, Val Loss = 11.053348739925774\n",
      "105600.iterasyon, Train Loss = 0.25893811182222654, Val Loss = 11.053575629506174\n",
      "105700.iterasyon, Train Loss = 0.25890665830681014, Val Loss = 11.053802430826083\n",
      "105800.iterasyon, Train Loss = 0.2588752397386409, Val Loss = 11.054029143897605\n",
      "105900.iterasyon, Train Loss = 0.2588438560763261, Val Loss = 11.054255768732789\n",
      "106000.iterasyon, Train Loss = 0.2588125072785269, Val Loss = 11.054482305343726\n",
      "106100.iterasyon, Train Loss = 0.2587811933039514, Val Loss = 11.054708753742577\n",
      "106200.iterasyon, Train Loss = 0.25874991411136, Val Loss = 11.054935113941406\n",
      "106300.iterasyon, Train Loss = 0.2587186696595607, Val Loss = 11.055161385952347\n",
      "106400.iterasyon, Train Loss = 0.2586874599074056, Val Loss = 11.055387569787548\n",
      "106500.iterasyon, Train Loss = 0.2586562848138086, Val Loss = 11.055613665459148\n",
      "106600.iterasyon, Train Loss = 0.2586251443377198, Val Loss = 11.055839672979285\n",
      "106700.iterasyon, Train Loss = 0.2585940384381433, Val Loss = 11.056065592360161\n",
      "106800.iterasyon, Train Loss = 0.2585629670741359, Val Loss = 11.056291423613933\n",
      "106900.iterasyon, Train Loss = 0.2585319302047952, Val Loss = 11.056517166752787\n",
      "107000.iterasyon, Train Loss = 0.2585009277892745, Val Loss = 11.056742821788912\n",
      "107100.iterasyon, Train Loss = 0.2584699597867708, Val Loss = 11.056968388734587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107200.iterasyon, Train Loss = 0.2584390261565319, Val Loss = 11.057193867602\n",
      "107300.iterasyon, Train Loss = 0.25840812685785536, Val Loss = 11.057419258403419\n",
      "107400.iterasyon, Train Loss = 0.2583772618500876, Val Loss = 11.057644561151038\n",
      "107500.iterasyon, Train Loss = 0.2583464310926173, Val Loss = 11.057869775857188\n",
      "107600.iterasyon, Train Loss = 0.2583156345448909, Val Loss = 11.058094902534089\n",
      "107700.iterasyon, Train Loss = 0.2582848721663964, Val Loss = 11.058319941194094\n",
      "107800.iterasyon, Train Loss = 0.2582541439166717, Val Loss = 11.058544891849484\n",
      "107900.iterasyon, Train Loss = 0.2582234497553038, Val Loss = 11.058769754512612\n",
      "108000.iterasyon, Train Loss = 0.25819278964192494, Val Loss = 11.058994529195832\n",
      "108100.iterasyon, Train Loss = 0.2581621635362186, Val Loss = 11.059219215911487\n",
      "108200.iterasyon, Train Loss = 0.258131571397919, Val Loss = 11.05944381467192\n",
      "108300.iterasyon, Train Loss = 0.25810101318679896, Val Loss = 11.05966832548955\n",
      "108400.iterasyon, Train Loss = 0.2580704888626893, Val Loss = 11.059892748376775\n",
      "108500.iterasyon, Train Loss = 0.25803999838546, Val Loss = 11.06011708334598\n",
      "108600.iterasyon, Train Loss = 0.25800954171503654, Val Loss = 11.060341330409708\n",
      "108700.iterasyon, Train Loss = 0.2579791188113855, Val Loss = 11.060565489580355\n",
      "108800.iterasyon, Train Loss = 0.25794872963452553, Val Loss = 11.060789560870376\n",
      "108900.iterasyon, Train Loss = 0.25791837414451885, Val Loss = 11.061013544292287\n",
      "109000.iterasyon, Train Loss = 0.25788805230148143, Val Loss = 11.061237439858584\n",
      "109100.iterasyon, Train Loss = 0.2578577640655683, Val Loss = 11.061461247581818\n",
      "109200.iterasyon, Train Loss = 0.25782750939698884, Val Loss = 11.06168496747454\n",
      "109300.iterasyon, Train Loss = 0.2577972882559952, Val Loss = 11.06190859954929\n",
      "109400.iterasyon, Train Loss = 0.2577671006028914, Val Loss = 11.062132143818653\n",
      "109500.iterasyon, Train Loss = 0.2577369463980232, Val Loss = 11.062355600295255\n",
      "109600.iterasyon, Train Loss = 0.2577068256017886, Val Loss = 11.062578968991666\n",
      "109700.iterasyon, Train Loss = 0.2576767381746263, Val Loss = 11.0628022499206\n",
      "109800.iterasyon, Train Loss = 0.25764668407703134, Val Loss = 11.063025443094665\n",
      "109900.iterasyon, Train Loss = 0.2576166632695348, Val Loss = 11.063248548526555\n",
      "110000.iterasyon, Train Loss = 0.2575866757127246, Val Loss = 11.063471566228978\n",
      "110100.iterasyon, Train Loss = 0.25755672136722424, Val Loss = 11.063694496214671\n",
      "110200.iterasyon, Train Loss = 0.2575268001937186, Val Loss = 11.06391733849634\n",
      "110300.iterasyon, Train Loss = 0.25749691215292564, Val Loss = 11.064140093086754\n",
      "110400.iterasyon, Train Loss = 0.2574670572056181, Val Loss = 11.064362759998723\n",
      "110500.iterasyon, Train Loss = 0.25743723531261237, Val Loss = 11.064585339245014\n",
      "110600.iterasyon, Train Loss = 0.2574074464347689, Val Loss = 11.064807830838491\n",
      "110700.iterasyon, Train Loss = 0.25737769053300175, Val Loss = 11.065030234791962\n",
      "110800.iterasyon, Train Loss = 0.25734796756826456, Val Loss = 11.065252551118297\n",
      "110900.iterasyon, Train Loss = 0.25731827750155895, Val Loss = 11.065474779830451\n",
      "111000.iterasyon, Train Loss = 0.2572886202939349, Val Loss = 11.065696920941267\n",
      "111100.iterasyon, Train Loss = 0.25725899590648765, Val Loss = 11.065918974463655\n",
      "111200.iterasyon, Train Loss = 0.25722940430035596, Val Loss = 11.066140940410616\n",
      "111300.iterasyon, Train Loss = 0.2571998454367307, Val Loss = 11.066362818795106\n",
      "111400.iterasyon, Train Loss = 0.2571703192768411, Val Loss = 11.066584609630173\n",
      "111500.iterasyon, Train Loss = 0.2571408257819649, Val Loss = 11.066806312928795\n",
      "111600.iterasyon, Train Loss = 0.2571113649134316, Val Loss = 11.067027928704011\n",
      "111700.iterasyon, Train Loss = 0.2570819366326097, Val Loss = 11.067249456968877\n",
      "111800.iterasyon, Train Loss = 0.25705254090091556, Val Loss = 11.067470897736527\n",
      "111900.iterasyon, Train Loss = 0.25702317767981103, Val Loss = 11.067692251020091\n",
      "112000.iterasyon, Train Loss = 0.2569938469308048, Val Loss = 11.067913516832636\n",
      "112100.iterasyon, Train Loss = 0.2569645486154521, Val Loss = 11.068134695187368\n",
      "112200.iterasyon, Train Loss = 0.25693528269535015, Val Loss = 11.068355786097445\n",
      "112300.iterasyon, Train Loss = 0.25690604913214365, Val Loss = 11.068576789576053\n",
      "112400.iterasyon, Train Loss = 0.25687684788752235, Val Loss = 11.068797705636463\n",
      "112500.iterasyon, Train Loss = 0.2568476789232233, Val Loss = 11.06901853429191\n",
      "112600.iterasyon, Train Loss = 0.256818542201024, Val Loss = 11.06923927555569\n",
      "112700.iterasyon, Train Loss = 0.25678943768275364, Val Loss = 11.069459929441111\n",
      "112800.iterasyon, Train Loss = 0.2567603653302836, Val Loss = 11.069680495961432\n",
      "112900.iterasyon, Train Loss = 0.2567313251055287, Val Loss = 11.06990097513011\n",
      "113000.iterasyon, Train Loss = 0.2567023169704506, Val Loss = 11.070121366960455\n",
      "113100.iterasyon, Train Loss = 0.2566733408870579, Val Loss = 11.070341671465876\n",
      "113200.iterasyon, Train Loss = 0.25664439681739776, Val Loss = 11.07056188865979\n",
      "113300.iterasyon, Train Loss = 0.2566154847235713, Val Loss = 11.070782018555688\n",
      "113400.iterasyon, Train Loss = 0.25658660456771726, Val Loss = 11.071002061166979\n",
      "113500.iterasyon, Train Loss = 0.2565577563120233, Val Loss = 11.071222016507164\n",
      "113600.iterasyon, Train Loss = 0.25652893991871967, Val Loss = 11.071441884589818\n",
      "113700.iterasyon, Train Loss = 0.25650015535008286, Val Loss = 11.071661665428433\n",
      "113800.iterasyon, Train Loss = 0.2564714025684305, Val Loss = 11.071881359036663\n",
      "113900.iterasyon, Train Loss = 0.2564426815361301, Val Loss = 11.072100965428003\n",
      "114000.iterasyon, Train Loss = 0.2564139922155937, Val Loss = 11.0723204846161\n",
      "114100.iterasyon, Train Loss = 0.25638533456926743, Val Loss = 11.072539916614675\n",
      "114200.iterasyon, Train Loss = 0.2563567085596565, Val Loss = 11.072759261437351\n",
      "114300.iterasyon, Train Loss = 0.2563281141493009, Val Loss = 11.072978519097822\n",
      "114400.iterasyon, Train Loss = 0.25629955130078885, Val Loss = 11.073197689609827\n",
      "114500.iterasyon, Train Loss = 0.2562710199767502, Val Loss = 11.073416772987104\n",
      "114600.iterasyon, Train Loss = 0.25624252013986093, Val Loss = 11.073635769243417\n",
      "114700.iterasyon, Train Loss = 0.2562140517528421, Val Loss = 11.073854678392602\n",
      "114800.iterasyon, Train Loss = 0.2561856147784575, Val Loss = 11.074073500448469\n",
      "114900.iterasyon, Train Loss = 0.25615720917951235, Val Loss = 11.074292235424902\n",
      "115000.iterasyon, Train Loss = 0.25612883491885974, Val Loss = 11.074510883335746\n",
      "115100.iterasyon, Train Loss = 0.25610049195939805, Val Loss = 11.074729444194912\n",
      "115200.iterasyon, Train Loss = 0.25607218026406264, Val Loss = 11.074947918016331\n",
      "115300.iterasyon, Train Loss = 0.2560438997958401, Val Loss = 11.075166304813928\n",
      "115400.iterasyon, Train Loss = 0.256015650517756, Val Loss = 11.075384604601771\n",
      "115500.iterasyon, Train Loss = 0.2559874323928824, Val Loss = 11.075602817393785\n",
      "115600.iterasyon, Train Loss = 0.25595924538433346, Val Loss = 11.075820943204056\n",
      "115700.iterasyon, Train Loss = 0.25593108945526694, Val Loss = 11.076038982046647\n",
      "115800.iterasyon, Train Loss = 0.2559029645688873, Val Loss = 11.076256933935616\n",
      "115900.iterasyon, Train Loss = 0.2558748706884357, Val Loss = 11.076474798885107\n",
      "116000.iterasyon, Train Loss = 0.2558468077772032, Val Loss = 11.076692576909283\n",
      "116100.iterasyon, Train Loss = 0.2558187757985224, Val Loss = 11.076910268022255\n",
      "116200.iterasyon, Train Loss = 0.25579077471576694, Val Loss = 11.077127872238277\n",
      "116300.iterasyon, Train Loss = 0.25576280449235894, Val Loss = 11.077345389571503\n",
      "116400.iterasyon, Train Loss = 0.2557348650917568, Val Loss = 11.077562820036253\n",
      "116500.iterasyon, Train Loss = 0.2557069564774683, Val Loss = 11.077780163646757\n",
      "116600.iterasyon, Train Loss = 0.25567907861303885, Val Loss = 11.077997420417324\n",
      "116700.iterasyon, Train Loss = 0.2556512314620653, Val Loss = 11.078214590362274\n",
      "116800.iterasyon, Train Loss = 0.25562341498817764, Val Loss = 11.078431673495993\n",
      "116900.iterasyon, Train Loss = 0.2555956291550513, Val Loss = 11.078648669832864\n",
      "117000.iterasyon, Train Loss = 0.25556787392641295, Val Loss = 11.078865579387275\n",
      "117100.iterasyon, Train Loss = 0.2555401492660219, Val Loss = 11.079082402173638\n",
      "117200.iterasyon, Train Loss = 0.25551245513768317, Val Loss = 11.079299138206457\n",
      "117300.iterasyon, Train Loss = 0.255484791505249, Val Loss = 11.079515787500231\n",
      "117400.iterasyon, Train Loss = 0.2554571583326089, Val Loss = 11.079732350069435\n",
      "117500.iterasyon, Train Loss = 0.2554295555836971, Val Loss = 11.079948825928685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117600.iterasyon, Train Loss = 0.25540198322249125, Val Loss = 11.080165215092507\n",
      "117700.iterasyon, Train Loss = 0.25537444121300956, Val Loss = 11.080381517575447\n",
      "117800.iterasyon, Train Loss = 0.2553469295193161, Val Loss = 11.08059773339219\n",
      "117900.iterasyon, Train Loss = 0.25531944810551216, Val Loss = 11.080813862557386\n",
      "118000.iterasyon, Train Loss = 0.25529199693574717, Val Loss = 11.081029905085698\n",
      "118100.iterasyon, Train Loss = 0.2552645759742077, Val Loss = 11.081245860991867\n",
      "118200.iterasyon, Train Loss = 0.25523718518512734, Val Loss = 11.08146173029059\n",
      "118300.iterasyon, Train Loss = 0.2552098245327784, Val Loss = 11.081677512996611\n",
      "118400.iterasyon, Train Loss = 0.25518249398147613, Val Loss = 11.081893209124763\n",
      "118500.iterasyon, Train Loss = 0.25515519349558036, Val Loss = 11.082108818689838\n",
      "118600.iterasyon, Train Loss = 0.25512792303948967, Val Loss = 11.082324341706725\n",
      "118700.iterasyon, Train Loss = 0.25510068257764734, Val Loss = 11.082539778190208\n",
      "118800.iterasyon, Train Loss = 0.25507347207453696, Val Loss = 11.082755128155236\n",
      "118900.iterasyon, Train Loss = 0.2550462914946833, Val Loss = 11.08297039161675\n",
      "119000.iterasyon, Train Loss = 0.255019140802656, Val Loss = 11.08318556858968\n",
      "119100.iterasyon, Train Loss = 0.2549920199630639, Val Loss = 11.083400659088982\n",
      "119200.iterasyon, Train Loss = 0.2549649289405584, Val Loss = 11.083615663129686\n",
      "119300.iterasyon, Train Loss = 0.25493786769983623, Val Loss = 11.083830580726817\n",
      "119400.iterasyon, Train Loss = 0.254910836205627, Val Loss = 11.084045411895469\n",
      "119500.iterasyon, Train Loss = 0.25488383442270807, Val Loss = 11.084260156650677\n",
      "119600.iterasyon, Train Loss = 0.2548568623159025, Val Loss = 11.084474815007605\n",
      "119700.iterasyon, Train Loss = 0.25482991985006426, Val Loss = 11.08468938698136\n",
      "119800.iterasyon, Train Loss = 0.25480300699009895, Val Loss = 11.084903872587121\n",
      "119900.iterasyon, Train Loss = 0.2547761237009452, Val Loss = 11.085118271840091\n",
      "120000.iterasyon, Train Loss = 0.2547492699475905, Val Loss = 11.085332584755495\n",
      "120100.iterasyon, Train Loss = 0.2547224456950601, Val Loss = 11.08554681134855\n",
      "120200.iterasyon, Train Loss = 0.25469565090841767, Val Loss = 11.085760951634585\n",
      "120300.iterasyon, Train Loss = 0.25466888555277145, Val Loss = 11.08597500562892\n",
      "120400.iterasyon, Train Loss = 0.2546421495932768, Val Loss = 11.08618897334681\n",
      "120500.iterasyon, Train Loss = 0.2546154429951161, Val Loss = 11.086402854803646\n",
      "120600.iterasyon, Train Loss = 0.25458876572352296, Val Loss = 11.086616650014884\n",
      "120700.iterasyon, Train Loss = 0.2545621177437723, Val Loss = 11.086830358995877\n",
      "120800.iterasyon, Train Loss = 0.25453549902117517, Val Loss = 11.087043981762054\n",
      "120900.iterasyon, Train Loss = 0.2545089095210864, Val Loss = 11.08725751832887\n",
      "121000.iterasyon, Train Loss = 0.25448234920890084, Val Loss = 11.087470968711886\n",
      "121100.iterasyon, Train Loss = 0.2544558180500541, Val Loss = 11.087684332926592\n",
      "121200.iterasyon, Train Loss = 0.2544293160100248, Val Loss = 11.08789761098857\n",
      "121300.iterasyon, Train Loss = 0.2544028430543277, Val Loss = 11.08811080291336\n",
      "121400.iterasyon, Train Loss = 0.25437639914852367, Val Loss = 11.08832390871656\n",
      "121500.iterasyon, Train Loss = 0.25434998425821037, Val Loss = 11.088536928413838\n",
      "121600.iterasyon, Train Loss = 0.2543235983490287, Val Loss = 11.088749862020848\n",
      "121700.iterasyon, Train Loss = 0.25429724138665544, Val Loss = 11.088962709553245\n",
      "121800.iterasyon, Train Loss = 0.2542709133368154, Val Loss = 11.089175471026762\n",
      "121900.iterasyon, Train Loss = 0.2542446141652655, Val Loss = 11.089388146457164\n",
      "122000.iterasyon, Train Loss = 0.2542183438378106, Val Loss = 11.089600735860168\n",
      "122100.iterasyon, Train Loss = 0.2541921023202893, Val Loss = 11.089813239251624\n",
      "122200.iterasyon, Train Loss = 0.2541658895785864, Val Loss = 11.09002565664732\n",
      "122300.iterasyon, Train Loss = 0.2541397055786221, Val Loss = 11.090237988063127\n",
      "122400.iterasyon, Train Loss = 0.25411355028635857, Val Loss = 11.090450233514934\n",
      "122500.iterasyon, Train Loss = 0.2540874236678018, Val Loss = 11.090662393018587\n",
      "122600.iterasyon, Train Loss = 0.2540613256889913, Val Loss = 11.090874466590076\n",
      "122700.iterasyon, Train Loss = 0.2540352563160117, Val Loss = 11.091086454245291\n",
      "122800.iterasyon, Train Loss = 0.25400921551498445, Val Loss = 11.091298356000284\n",
      "122900.iterasyon, Train Loss = 0.25398320325207296, Val Loss = 11.09151017187107\n",
      "123000.iterasyon, Train Loss = 0.25395721949348077, Val Loss = 11.09172190187363\n",
      "123100.iterasyon, Train Loss = 0.25393126420545137, Val Loss = 11.091933546024038\n",
      "123200.iterasyon, Train Loss = 0.25390533735426274, Val Loss = 11.092145104338435\n",
      "123300.iterasyon, Train Loss = 0.2538794389062413, Val Loss = 11.092356576832897\n",
      "123400.iterasyon, Train Loss = 0.25385356882774585, Val Loss = 11.092567963523589\n",
      "123500.iterasyon, Train Loss = 0.25382772708518214, Val Loss = 11.092779264426698\n",
      "123600.iterasyon, Train Loss = 0.25380191364498783, Val Loss = 11.092990479558399\n",
      "123700.iterasyon, Train Loss = 0.2537761284736471, Val Loss = 11.093201608934898\n",
      "123800.iterasyon, Train Loss = 0.25375037153767727, Val Loss = 11.093412652572502\n",
      "123900.iterasyon, Train Loss = 0.2537246428036402, Val Loss = 11.09362361048744\n",
      "124000.iterasyon, Train Loss = 0.2536989422381331, Val Loss = 11.093834482696085\n",
      "124100.iterasyon, Train Loss = 0.25367326980779586, Val Loss = 11.094045269214687\n",
      "124200.iterasyon, Train Loss = 0.2536476254793077, Val Loss = 11.094255970059667\n",
      "124300.iterasyon, Train Loss = 0.2536220092193872, Val Loss = 11.094466585247329\n",
      "124400.iterasyon, Train Loss = 0.2535964209947871, Val Loss = 11.094677114794191\n",
      "124500.iterasyon, Train Loss = 0.2535708607723039, Val Loss = 11.094887558716685\n",
      "124600.iterasyon, Train Loss = 0.25354532851877454, Val Loss = 11.095097917031195\n",
      "124700.iterasyon, Train Loss = 0.25351982420107155, Val Loss = 11.095308189754277\n",
      "124800.iterasyon, Train Loss = 0.2534943477861104, Val Loss = 11.095518376902444\n",
      "124900.iterasyon, Train Loss = 0.2534688992408417, Val Loss = 11.095728478492225\n",
      "125000.iterasyon, Train Loss = 0.2534434785322566, Val Loss = 11.095938494540176\n",
      "125100.iterasyon, Train Loss = 0.25341808562738694, Val Loss = 11.096148425062914\n",
      "125200.iterasyon, Train Loss = 0.25339272049329836, Val Loss = 11.09635827007706\n",
      "125300.iterasyon, Train Loss = 0.2533673830971021, Val Loss = 11.096568029599295\n",
      "125400.iterasyon, Train Loss = 0.253342073405942, Val Loss = 11.096777703646271\n",
      "125500.iterasyon, Train Loss = 0.25331679138700675, Val Loss = 11.096987292234664\n",
      "125600.iterasyon, Train Loss = 0.25329153700751805, Val Loss = 11.09719679538128\n",
      "125700.iterasyon, Train Loss = 0.2532663102347408, Val Loss = 11.097406213102818\n",
      "125800.iterasyon, Train Loss = 0.25324111103597424, Val Loss = 11.09761554541605\n",
      "125900.iterasyon, Train Loss = 0.2532159393785585, Val Loss = 11.097824792337786\n",
      "126000.iterasyon, Train Loss = 0.2531907952298722, Val Loss = 11.098033953884924\n",
      "126100.iterasyon, Train Loss = 0.25316567855733557, Val Loss = 11.098243030074235\n",
      "126200.iterasyon, Train Loss = 0.2531405893283976, Val Loss = 11.098452020922668\n",
      "126300.iterasyon, Train Loss = 0.25311552751055655, Val Loss = 11.098660926447108\n",
      "126400.iterasyon, Train Loss = 0.2530904930713445, Val Loss = 11.098869746664496\n",
      "126500.iterasyon, Train Loss = 0.25306548597832995, Val Loss = 11.099078481591851\n",
      "126600.iterasyon, Train Loss = 0.25304050619912366, Val Loss = 11.099287131246076\n",
      "126700.iterasyon, Train Loss = 0.2530155537013702, Val Loss = 11.099495695644222\n",
      "126800.iterasyon, Train Loss = 0.252990628452756, Val Loss = 11.099704174803346\n",
      "126900.iterasyon, Train Loss = 0.2529657304210023, Val Loss = 11.099912568740493\n",
      "127000.iterasyon, Train Loss = 0.25294085957387447, Val Loss = 11.100120877472769\n",
      "127100.iterasyon, Train Loss = 0.25291601587916757, Val Loss = 11.100329101017293\n",
      "127200.iterasyon, Train Loss = 0.252891199304718, Val Loss = 11.100537239391262\n",
      "127300.iterasyon, Train Loss = 0.2528664098184043, Val Loss = 11.10074529261175\n",
      "127400.iterasyon, Train Loss = 0.25284164738813936, Val Loss = 11.100953260695976\n",
      "127500.iterasyon, Train Loss = 0.25281691198187145, Val Loss = 11.101161143661203\n",
      "127600.iterasyon, Train Loss = 0.252792203567589, Val Loss = 11.101368941524667\n",
      "127700.iterasyon, Train Loss = 0.25276752211332043, Val Loss = 11.101576654303601\n",
      "127800.iterasyon, Train Loss = 0.25274286758713055, Val Loss = 11.101784282015364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127900.iterasyon, Train Loss = 0.25271823995711856, Val Loss = 11.101991824677238\n",
      "128000.iterasyon, Train Loss = 0.2526936391914236, Val Loss = 11.102199282306586\n",
      "128100.iterasyon, Train Loss = 0.2526690652582226, Val Loss = 11.102406654920799\n",
      "128200.iterasyon, Train Loss = 0.2526445181257338, Val Loss = 11.102613942537229\n",
      "128300.iterasyon, Train Loss = 0.25261999776220256, Val Loss = 11.102821145173358\n",
      "128400.iterasyon, Train Loss = 0.2525955041359242, Val Loss = 11.10302826284661\n",
      "128500.iterasyon, Train Loss = 0.2525710372152211, Val Loss = 11.103235295574468\n",
      "128600.iterasyon, Train Loss = 0.2525465969684609, Val Loss = 11.103442243374424\n",
      "128700.iterasyon, Train Loss = 0.25252218336404236, Val Loss = 11.103649106263996\n",
      "128800.iterasyon, Train Loss = 0.2524977963704062, Val Loss = 11.103855884260746\n",
      "128900.iterasyon, Train Loss = 0.2524734359560238, Val Loss = 11.104062577382262\n",
      "129000.iterasyon, Train Loss = 0.25244910208941407, Val Loss = 11.10426918564611\n",
      "129100.iterasyon, Train Loss = 0.2524247947391229, Val Loss = 11.104475709069952\n",
      "129200.iterasyon, Train Loss = 0.2524005138737407, Val Loss = 11.104682147671406\n",
      "129300.iterasyon, Train Loss = 0.2523762594618875, Val Loss = 11.104888501468194\n",
      "129400.iterasyon, Train Loss = 0.25235203147222746, Val Loss = 11.105094770477969\n",
      "129500.iterasyon, Train Loss = 0.25232782987345864, Val Loss = 11.105300954718501\n",
      "129600.iterasyon, Train Loss = 0.2523036546343157, Val Loss = 11.105507054207504\n",
      "129700.iterasyon, Train Loss = 0.252279505723572, Val Loss = 11.105713068962757\n",
      "129800.iterasyon, Train Loss = 0.2522553831100323, Val Loss = 11.105918999002062\n",
      "129900.iterasyon, Train Loss = 0.25223128676254797, Val Loss = 11.106124844343253\n",
      "130000.iterasyon, Train Loss = 0.2522072166499944, Val Loss = 11.106330605004173\n",
      "130100.iterasyon, Train Loss = 0.25218317274129626, Val Loss = 11.106536281002727\n",
      "130200.iterasyon, Train Loss = 0.25215915500540775, Val Loss = 11.106741872356748\n",
      "130300.iterasyon, Train Loss = 0.2521351634113196, Val Loss = 11.10694737908421\n",
      "130400.iterasyon, Train Loss = 0.2521111979280611, Val Loss = 11.107152801203055\n",
      "130500.iterasyon, Train Loss = 0.2520872585246962, Val Loss = 11.107358138731248\n",
      "130600.iterasyon, Train Loss = 0.2520633451703311, Val Loss = 11.107563391686769\n",
      "130700.iterasyon, Train Loss = 0.25203945783409976, Val Loss = 11.107768560087642\n",
      "130800.iterasyon, Train Loss = 0.2520155964851788, Val Loss = 11.107973643951896\n",
      "130900.iterasyon, Train Loss = 0.25199176109277927, Val Loss = 11.108178643297661\n",
      "131000.iterasyon, Train Loss = 0.25196795162614605, Val Loss = 11.10838355814299\n",
      "131100.iterasyon, Train Loss = 0.25194416805456393, Val Loss = 11.10858838850598\n",
      "131200.iterasyon, Train Loss = 0.2519204103473547, Val Loss = 11.108793134404761\n",
      "131300.iterasyon, Train Loss = 0.25189667847387315, Val Loss = 11.108997795857539\n",
      "131400.iterasyon, Train Loss = 0.25187297240351, Val Loss = 11.109202372882528\n",
      "131500.iterasyon, Train Loss = 0.2518492921056929, Val Loss = 11.109406865497906\n",
      "131600.iterasyon, Train Loss = 0.2518256375498874, Val Loss = 11.109611273721896\n",
      "131700.iterasyon, Train Loss = 0.2518020087055936, Val Loss = 11.109815597572783\n",
      "131800.iterasyon, Train Loss = 0.2517784055423475, Val Loss = 11.110019837068805\n",
      "131900.iterasyon, Train Loss = 0.25175482802971927, Val Loss = 11.110223992228356\n",
      "132000.iterasyon, Train Loss = 0.2517312761373211, Val Loss = 11.110428063069687\n",
      "132100.iterasyon, Train Loss = 0.25170774983479355, Val Loss = 11.1106320496112\n",
      "132200.iterasyon, Train Loss = 0.25168424909181225, Val Loss = 11.110835951871273\n",
      "132300.iterasyon, Train Loss = 0.25166077387810193, Val Loss = 11.111039769868283\n",
      "132400.iterasyon, Train Loss = 0.2516373241634054, Val Loss = 11.111243503620654\n",
      "132500.iterasyon, Train Loss = 0.2516138999175158, Val Loss = 11.11144715314685\n",
      "132600.iterasyon, Train Loss = 0.25159050111024933, Val Loss = 11.111650718465366\n",
      "132700.iterasyon, Train Loss = 0.25156712771146533, Val Loss = 11.111854199594664\n",
      "132800.iterasyon, Train Loss = 0.2515437796910585, Val Loss = 11.112057596553305\n",
      "132900.iterasyon, Train Loss = 0.2515204570189584, Val Loss = 11.11226090935979\n",
      "133000.iterasyon, Train Loss = 0.25149715966512937, Val Loss = 11.112464138032708\n",
      "133100.iterasyon, Train Loss = 0.2514738875995695, Val Loss = 11.11266728259062\n",
      "133200.iterasyon, Train Loss = 0.2514506407923136, Val Loss = 11.112870343052172\n",
      "133300.iterasyon, Train Loss = 0.25142741921343387, Val Loss = 11.113073319436005\n",
      "133400.iterasyon, Train Loss = 0.2514042228330344, Val Loss = 11.113276211760745\n",
      "133500.iterasyon, Train Loss = 0.2513810516212586, Val Loss = 11.113479020045096\n",
      "133600.iterasyon, Train Loss = 0.251357905548281, Val Loss = 11.113681744307783\n",
      "133700.iterasyon, Train Loss = 0.2513347845843105, Val Loss = 11.113884384567523\n",
      "133800.iterasyon, Train Loss = 0.25131168869959725, Val Loss = 11.114086940843059\n",
      "133900.iterasyon, Train Loss = 0.25128861786442247, Val Loss = 11.11428941315315\n",
      "134000.iterasyon, Train Loss = 0.25126557204909866, Val Loss = 11.114491801516628\n",
      "134100.iterasyon, Train Loss = 0.25124255122398315, Val Loss = 11.114694105952328\n",
      "134200.iterasyon, Train Loss = 0.2512195553594591, Val Loss = 11.114896326479043\n",
      "134300.iterasyon, Train Loss = 0.2511965844259475, Val Loss = 11.115098463115649\n",
      "134400.iterasyon, Train Loss = 0.2511736383939063, Val Loss = 11.11530051588106\n",
      "134500.iterasyon, Train Loss = 0.25115071723382515, Val Loss = 11.11550248479415\n",
      "134600.iterasyon, Train Loss = 0.2511278209162335, Val Loss = 11.115704369873917\n",
      "134700.iterasyon, Train Loss = 0.25110494941168715, Val Loss = 11.11590617113925\n",
      "134800.iterasyon, Train Loss = 0.2510821026907838, Val Loss = 11.11610788860915\n",
      "134900.iterasyon, Train Loss = 0.25105928072415307, Val Loss = 11.116309522302647\n",
      "135000.iterasyon, Train Loss = 0.2510364834824615, Val Loss = 11.11651107223872\n",
      "135100.iterasyon, Train Loss = 0.2510137109364067, Val Loss = 11.116712538436424\n",
      "135200.iterasyon, Train Loss = 0.25099096305672247, Val Loss = 11.116913920914861\n",
      "135300.iterasyon, Train Loss = 0.25096823981417676, Val Loss = 11.117115219693083\n",
      "135400.iterasyon, Train Loss = 0.25094554117957274, Val Loss = 11.117316434790268\n",
      "135500.iterasyon, Train Loss = 0.25092286712374745, Val Loss = 11.117517566225489\n",
      "135600.iterasyon, Train Loss = 0.2509002176175729, Val Loss = 11.117718614017901\n",
      "135700.iterasyon, Train Loss = 0.25087759263195447, Val Loss = 11.117919578186704\n",
      "135800.iterasyon, Train Loss = 0.2508549921378329, Val Loss = 11.11812045875113\n",
      "135900.iterasyon, Train Loss = 0.25083241610618245, Val Loss = 11.118321255730315\n",
      "136000.iterasyon, Train Loss = 0.2508098645080113, Val Loss = 11.118521969143595\n",
      "136100.iterasyon, Train Loss = 0.2507873373143647, Val Loss = 11.118722599010196\n",
      "136200.iterasyon, Train Loss = 0.250764834496317, Val Loss = 11.118923145349413\n",
      "136300.iterasyon, Train Loss = 0.2507423560249811, Val Loss = 11.11912360818056\n",
      "136400.iterasyon, Train Loss = 0.25071990187150034, Val Loss = 11.119323987522954\n",
      "136500.iterasyon, Train Loss = 0.2506974720070533, Val Loss = 11.119524283395974\n",
      "136600.iterasyon, Train Loss = 0.2506750664028576, Val Loss = 11.119724495818986\n",
      "136700.iterasyon, Train Loss = 0.2506526850301583, Val Loss = 11.119924624811345\n",
      "136800.iterasyon, Train Loss = 0.2506303278602342, Val Loss = 11.120124670392542\n",
      "136900.iterasyon, Train Loss = 0.2506079948644016, Val Loss = 11.120324632582006\n",
      "137000.iterasyon, Train Loss = 0.2505856860140096, Val Loss = 11.120524511399186\n",
      "137100.iterasyon, Train Loss = 0.25056340128044036, Val Loss = 11.120724306863563\n",
      "137200.iterasyon, Train Loss = 0.25054114063510863, Val Loss = 11.120924018994641\n",
      "137300.iterasyon, Train Loss = 0.25051890404946914, Val Loss = 11.121123647811935\n",
      "137400.iterasyon, Train Loss = 0.25049669149499965, Val Loss = 11.121323193335018\n",
      "137500.iterasyon, Train Loss = 0.2504745029432226, Val Loss = 11.121522655583464\n",
      "137600.iterasyon, Train Loss = 0.2504523383656824, Val Loss = 11.121722034576862\n",
      "137700.iterasyon, Train Loss = 0.2504301977339705, Val Loss = 11.12192133033477\n",
      "137800.iterasyon, Train Loss = 0.25040808101970097, Val Loss = 11.122120542876868\n",
      "137900.iterasyon, Train Loss = 0.2503859881945257, Val Loss = 11.122319672222808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138000.iterasyon, Train Loss = 0.2503639192301282, Val Loss = 11.122518718392259\n",
      "138100.iterasyon, Train Loss = 0.25034187409822956, Val Loss = 11.122717681404897\n",
      "138200.iterasyon, Train Loss = 0.25031985277057806, Val Loss = 11.122916561280482\n",
      "138300.iterasyon, Train Loss = 0.2502978552189614, Val Loss = 11.123115358038723\n",
      "138400.iterasyon, Train Loss = 0.2502758814151944, Val Loss = 11.123314071699388\n",
      "138500.iterasyon, Train Loss = 0.25025393133113066, Val Loss = 11.123512702282227\n",
      "138600.iterasyon, Train Loss = 0.2502320049386538, Val Loss = 11.123711249807089\n",
      "138700.iterasyon, Train Loss = 0.2502101022096806, Val Loss = 11.123909714293822\n",
      "138800.iterasyon, Train Loss = 0.2501882231161657, Val Loss = 11.124108095762166\n",
      "138900.iterasyon, Train Loss = 0.2501663676300886, Val Loss = 11.124306394232063\n",
      "139000.iterasyon, Train Loss = 0.2501445357234666, Val Loss = 11.124504609723353\n",
      "139100.iterasyon, Train Loss = 0.2501227273683518, Val Loss = 11.124702742255971\n",
      "139200.iterasyon, Train Loss = 0.2501009425368254, Val Loss = 11.124900791849822\n",
      "139300.iterasyon, Train Loss = 0.25007918120100486, Val Loss = 11.12509875852486\n",
      "139400.iterasyon, Train Loss = 0.2500574433330363, Val Loss = 11.125296642301036\n",
      "139500.iterasyon, Train Loss = 0.2500357289051031, Val Loss = 11.125494443198349\n",
      "139600.iterasyon, Train Loss = 0.2500140378894165, Val Loss = 11.125692161236817\n",
      "139700.iterasyon, Train Loss = 0.2499923702582267, Val Loss = 11.125889796436445\n",
      "139800.iterasyon, Train Loss = 0.24997072598381176, Val Loss = 11.126087348817254\n",
      "139900.iterasyon, Train Loss = 0.2499491050384872, Val Loss = 11.12628481839935\n",
      "140000.iterasyon, Train Loss = 0.24992750739459302, Val Loss = 11.126482205202825\n",
      "140100.iterasyon, Train Loss = 0.24990593302450725, Val Loss = 11.126679509247795\n",
      "140200.iterasyon, Train Loss = 0.24988438190064532, Val Loss = 11.126876730554333\n",
      "140300.iterasyon, Train Loss = 0.2498628539954477, Val Loss = 11.127073869142608\n",
      "140400.iterasyon, Train Loss = 0.24984134928138738, Val Loss = 11.127270925032812\n",
      "140500.iterasyon, Train Loss = 0.2498198677309733, Val Loss = 11.127467898245106\n",
      "140600.iterasyon, Train Loss = 0.24979840931674543, Val Loss = 11.127664788799702\n",
      "140700.iterasyon, Train Loss = 0.2497769740112773, Val Loss = 11.127861596716805\n",
      "140800.iterasyon, Train Loss = 0.24975556178717354, Val Loss = 11.128058322016711\n",
      "140900.iterasyon, Train Loss = 0.2497341726170705, Val Loss = 11.128254964719634\n",
      "141000.iterasyon, Train Loss = 0.2497128064736368, Val Loss = 11.12845152484593\n",
      "141100.iterasyon, Train Loss = 0.2496914633295744, Val Loss = 11.128648002415837\n",
      "141200.iterasyon, Train Loss = 0.24967014315762195, Val Loss = 11.128844397449692\n",
      "141300.iterasyon, Train Loss = 0.24964884593053827, Val Loss = 11.129040709967835\n",
      "141400.iterasyon, Train Loss = 0.2496275716211259, Val Loss = 11.12923693999065\n",
      "141500.iterasyon, Train Loss = 0.24960632020221163, Val Loss = 11.129433087538514\n",
      "141600.iterasyon, Train Loss = 0.24958509164666168, Val Loss = 11.129629152631832\n",
      "141700.iterasyon, Train Loss = 0.2495638859273677, Val Loss = 11.129825135290998\n",
      "141800.iterasyon, Train Loss = 0.2495427030172581, Val Loss = 11.13002103553649\n",
      "141900.iterasyon, Train Loss = 0.24952154288928632, Val Loss = 11.130216853388738\n",
      "142000.iterasyon, Train Loss = 0.2495004055164479, Val Loss = 11.130412588868268\n",
      "142100.iterasyon, Train Loss = 0.24947929087176124, Val Loss = 11.13060824199554\n",
      "142200.iterasyon, Train Loss = 0.24945819892828128, Val Loss = 11.13080381279109\n",
      "142300.iterasyon, Train Loss = 0.24943712965909262, Val Loss = 11.130999301275406\n",
      "142400.iterasyon, Train Loss = 0.2494160830373149, Val Loss = 11.131194707469104\n",
      "142500.iterasyon, Train Loss = 0.24939505903609085, Val Loss = 11.131390031392732\n",
      "142600.iterasyon, Train Loss = 0.249374057628608, Val Loss = 11.131585273066891\n",
      "142700.iterasyon, Train Loss = 0.24935307878807564, Val Loss = 11.13178043251221\n",
      "142800.iterasyon, Train Loss = 0.24933212248773687, Val Loss = 11.131975509749306\n",
      "142900.iterasyon, Train Loss = 0.24931118870086702, Val Loss = 11.132170504798822\n",
      "143000.iterasyon, Train Loss = 0.249290277400777, Val Loss = 11.132365417681381\n",
      "143100.iterasyon, Train Loss = 0.24926938856079828, Val Loss = 11.132560248417768\n",
      "143200.iterasyon, Train Loss = 0.24924852215430718, Val Loss = 11.132754997028625\n",
      "143300.iterasyon, Train Loss = 0.2492276781546983, Val Loss = 11.132949663534697\n",
      "143400.iterasyon, Train Loss = 0.2492068565354093, Val Loss = 11.133144247956723\n",
      "143500.iterasyon, Train Loss = 0.24918605726990276, Val Loss = 11.133338750315513\n",
      "143600.iterasyon, Train Loss = 0.24916528033167148, Val Loss = 11.133533170631777\n",
      "143700.iterasyon, Train Loss = 0.2491445256942455, Val Loss = 11.133727508926345\n",
      "143800.iterasyon, Train Loss = 0.24912379333117926, Val Loss = 11.13392176522005\n",
      "143900.iterasyon, Train Loss = 0.2491030832160623, Val Loss = 11.134115939533691\n",
      "144000.iterasyon, Train Loss = 0.2490823953225186, Val Loss = 11.134310031888136\n",
      "144100.iterasyon, Train Loss = 0.24906172962419024, Val Loss = 11.134504042304295\n",
      "144200.iterasyon, Train Loss = 0.24904108609477002, Val Loss = 11.134697970803007\n",
      "144300.iterasyon, Train Loss = 0.24902046470796532, Val Loss = 11.134891817405189\n",
      "144400.iterasyon, Train Loss = 0.24899986543751987, Val Loss = 11.135085582131815\n",
      "144500.iterasyon, Train Loss = 0.24897928825720703, Val Loss = 11.135279265003858\n",
      "144600.iterasyon, Train Loss = 0.2489587331408388, Val Loss = 11.135472866042148\n",
      "144700.iterasyon, Train Loss = 0.24893820006225045, Val Loss = 11.135666385267749\n",
      "144800.iterasyon, Train Loss = 0.24891768899530434, Val Loss = 11.135859822701676\n",
      "144900.iterasyon, Train Loss = 0.24889719991390566, Val Loss = 11.136053178364904\n",
      "145000.iterasyon, Train Loss = 0.24887673279197983, Val Loss = 11.136246452278515\n",
      "145100.iterasyon, Train Loss = 0.2488562876034899, Val Loss = 11.136439644463534\n",
      "145200.iterasyon, Train Loss = 0.2488358643224234, Val Loss = 11.136632754941044\n",
      "145300.iterasyon, Train Loss = 0.24881546292280524, Val Loss = 11.136825783732085\n",
      "145400.iterasyon, Train Loss = 0.24879508337868728, Val Loss = 11.137018730857822\n",
      "145500.iterasyon, Train Loss = 0.24877472566414893, Val Loss = 11.13721159633938\n",
      "145600.iterasyon, Train Loss = 0.2487543897533067, Val Loss = 11.137404380197884\n",
      "145700.iterasyon, Train Loss = 0.2487340756203038, Val Loss = 11.137597082454484\n",
      "145800.iterasyon, Train Loss = 0.24871378323931415, Val Loss = 11.137789703130368\n",
      "145900.iterasyon, Train Loss = 0.2486935125845443, Val Loss = 11.137982242246693\n",
      "146000.iterasyon, Train Loss = 0.24867326363022435, Val Loss = 11.138174699824777\n",
      "146100.iterasyon, Train Loss = 0.24865303635062583, Val Loss = 11.138367075885736\n",
      "146200.iterasyon, Train Loss = 0.24863283072004239, Val Loss = 11.138559370450862\n",
      "146300.iterasyon, Train Loss = 0.2486126467128007, Val Loss = 11.138751583541415\n",
      "146400.iterasyon, Train Loss = 0.24859248430325562, Val Loss = 11.1389437151787\n",
      "146500.iterasyon, Train Loss = 0.24857234346579785, Val Loss = 11.139135765383996\n",
      "146600.iterasyon, Train Loss = 0.2485522241748397, Val Loss = 11.139327734178593\n",
      "146700.iterasyon, Train Loss = 0.24853212640483094, Val Loss = 11.13951962158384\n",
      "146800.iterasyon, Train Loss = 0.24851205013024846, Val Loss = 11.139711427621133\n",
      "146900.iterasyon, Train Loss = 0.24849199532560204, Val Loss = 11.1399031523118\n",
      "147000.iterasyon, Train Loss = 0.24847196196542695, Val Loss = 11.140094795677195\n",
      "147100.iterasyon, Train Loss = 0.24845195002428855, Val Loss = 11.140286357738791\n",
      "147200.iterasyon, Train Loss = 0.2484319594767881, Val Loss = 11.140477838517958\n",
      "147300.iterasyon, Train Loss = 0.24841199029755168, Val Loss = 11.140669238036168\n",
      "147400.iterasyon, Train Loss = 0.24839204246123772, Val Loss = 11.14086055631482\n",
      "147500.iterasyon, Train Loss = 0.24837211594253117, Val Loss = 11.141051793375418\n",
      "147600.iterasyon, Train Loss = 0.24835221071615177, Val Loss = 11.141242949239448\n",
      "147700.iterasyon, Train Loss = 0.24833232675684447, Val Loss = 11.141434023928413\n",
      "147800.iterasyon, Train Loss = 0.24831246403938864, Val Loss = 11.141625017463786\n",
      "147900.iterasyon, Train Loss = 0.24829262253858972, Val Loss = 11.14181592986716\n",
      "148000.iterasyon, Train Loss = 0.24827280222928197, Val Loss = 11.142006761160093\n",
      "148100.iterasyon, Train Loss = 0.2482530030863331, Val Loss = 11.142197511364113\n",
      "148200.iterasyon, Train Loss = 0.24823322508463988, Val Loss = 11.142388180500836\n",
      "148300.iterasyon, Train Loss = 0.24821346819912593, Val Loss = 11.142578768591836\n",
      "148400.iterasyon, Train Loss = 0.24819373240474496, Val Loss = 11.142769275658729\n",
      "148500.iterasyon, Train Loss = 0.2481740176764843, Val Loss = 11.14295970172317\n",
      "148600.iterasyon, Train Loss = 0.2481543239893549, Val Loss = 11.143150046806811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148700.iterasyon, Train Loss = 0.24813465131840132, Val Loss = 11.143340310931327\n",
      "148800.iterasyon, Train Loss = 0.24811499963869757, Val Loss = 11.14353049411836\n",
      "148900.iterasyon, Train Loss = 0.2480953689253451, Val Loss = 11.14372059638965\n",
      "149000.iterasyon, Train Loss = 0.24807575915347327, Val Loss = 11.143910617766911\n",
      "149100.iterasyon, Train Loss = 0.2480561702982459, Val Loss = 11.144100558271862\n",
      "149200.iterasyon, Train Loss = 0.24803660233485034, Val Loss = 11.14429041792625\n",
      "149300.iterasyon, Train Loss = 0.24801705523850914, Val Loss = 11.14448019675184\n",
      "149400.iterasyon, Train Loss = 0.24799752898446972, Val Loss = 11.144669894770407\n",
      "149500.iterasyon, Train Loss = 0.24797802354800744, Val Loss = 11.144859512003768\n",
      "149600.iterasyon, Train Loss = 0.2479585389044323, Val Loss = 11.145049048473705\n",
      "149700.iterasyon, Train Loss = 0.2479390750290815, Val Loss = 11.145238504202087\n",
      "149800.iterasyon, Train Loss = 0.24791963189731744, Val Loss = 11.145427879210732\n",
      "149900.iterasyon, Train Loss = 0.24790020948453476, Val Loss = 11.145617173521476\n",
      "150000.iterasyon, Train Loss = 0.24788080776615629, Val Loss = 11.145806387156272\n",
      "150100.iterasyon, Train Loss = 0.24786142671763753, Val Loss = 11.145995520136918\n",
      "150200.iterasyon, Train Loss = 0.24784206631445707, Val Loss = 11.146184572485389\n",
      "150300.iterasyon, Train Loss = 0.2478227265321269, Val Loss = 11.146373544223563\n",
      "150400.iterasyon, Train Loss = 0.24780340734618342, Val Loss = 11.146562435373417\n",
      "150500.iterasyon, Train Loss = 0.24778410873219606, Val Loss = 11.146751245956896\n",
      "150600.iterasyon, Train Loss = 0.2477648306657618, Val Loss = 11.146939975995922\n",
      "150700.iterasyon, Train Loss = 0.24774557312250625, Val Loss = 11.147128625512542\n",
      "150800.iterasyon, Train Loss = 0.24772633607808292, Val Loss = 11.147317194528721\n",
      "150900.iterasyon, Train Loss = 0.2477071195081776, Val Loss = 11.14750568306648\n",
      "151000.iterasyon, Train Loss = 0.24768792338849624, Val Loss = 11.147694091147875\n",
      "151100.iterasyon, Train Loss = 0.24766874769478542, Val Loss = 11.147882418794925\n",
      "151200.iterasyon, Train Loss = 0.24764959240281234, Val Loss = 11.148070666029637\n",
      "151300.iterasyon, Train Loss = 0.24763045748837328, Val Loss = 11.148258832874188\n",
      "151400.iterasyon, Train Loss = 0.24761134292729423, Val Loss = 11.148446919350661\n",
      "151500.iterasyon, Train Loss = 0.24759224869543184, Val Loss = 11.148634925481105\n",
      "151600.iterasyon, Train Loss = 0.24757317476866647, Val Loss = 11.148822851287688\n",
      "151700.iterasyon, Train Loss = 0.24755412112291109, Val Loss = 11.149010696792514\n",
      "151800.iterasyon, Train Loss = 0.2475350877341068, Val Loss = 11.14919846201774\n",
      "151900.iterasyon, Train Loss = 0.247516074578222, Val Loss = 11.149386146985531\n",
      "152000.iterasyon, Train Loss = 0.24749708163125173, Val Loss = 11.149573751718076\n",
      "152100.iterasyon, Train Loss = 0.2474781088692217, Val Loss = 11.149761276237557\n",
      "152200.iterasyon, Train Loss = 0.2474591562681849, Val Loss = 11.149948720566247\n",
      "152300.iterasyon, Train Loss = 0.24744022380422434, Val Loss = 11.1501360847263\n",
      "152400.iterasyon, Train Loss = 0.24742131145344926, Val Loss = 11.150323368739969\n",
      "152500.iterasyon, Train Loss = 0.24740241919199554, Val Loss = 11.150510572629548\n",
      "152600.iterasyon, Train Loss = 0.24738354699603335, Val Loss = 11.150697696417302\n",
      "152700.iterasyon, Train Loss = 0.2473646948417547, Val Loss = 11.150884740125505\n",
      "152800.iterasyon, Train Loss = 0.24734586270538025, Val Loss = 11.15107170377644\n",
      "152900.iterasyon, Train Loss = 0.24732705056316565, Val Loss = 11.151258587392434\n",
      "153000.iterasyon, Train Loss = 0.2473082583913835, Val Loss = 11.151445390995798\n",
      "153100.iterasyon, Train Loss = 0.2472894861663427, Val Loss = 11.151632114608931\n",
      "153200.iterasyon, Train Loss = 0.24727073386437834, Val Loss = 11.15181875825414\n",
      "153300.iterasyon, Train Loss = 0.2472520014618513, Val Loss = 11.152005321953814\n",
      "153400.iterasyon, Train Loss = 0.24723328893515273, Val Loss = 11.152191805730325\n",
      "153500.iterasyon, Train Loss = 0.24721459626070164, Val Loss = 11.152378209606088\n",
      "153600.iterasyon, Train Loss = 0.24719592341494312, Val Loss = 11.152564533603524\n",
      "153700.iterasyon, Train Loss = 0.24717727037434686, Val Loss = 11.152750777745073\n",
      "153800.iterasyon, Train Loss = 0.24715863711542216, Val Loss = 11.152936942053127\n",
      "153900.iterasyon, Train Loss = 0.24714002361469367, Val Loss = 11.153123026550173\n",
      "154000.iterasyon, Train Loss = 0.24712142984871688, Val Loss = 11.153309031258717\n",
      "154100.iterasyon, Train Loss = 0.24710285579408028, Val Loss = 11.153494956201198\n",
      "154200.iterasyon, Train Loss = 0.24708430142739146, Val Loss = 11.153680801400167\n",
      "154300.iterasyon, Train Loss = 0.24706576672529326, Val Loss = 11.1538665668781\n",
      "154400.iterasyon, Train Loss = 0.2470472516644514, Val Loss = 11.154052252657555\n",
      "154500.iterasyon, Train Loss = 0.2470287562215625, Val Loss = 11.154237858761018\n",
      "154600.iterasyon, Train Loss = 0.24701028037334827, Val Loss = 11.154423385211096\n",
      "154700.iterasyon, Train Loss = 0.24699182409655726, Val Loss = 11.154608832030352\n",
      "154800.iterasyon, Train Loss = 0.24697338736796717, Val Loss = 11.154794199241364\n",
      "154900.iterasyon, Train Loss = 0.2469549701643838, Val Loss = 11.1549794868667\n",
      "155000.iterasyon, Train Loss = 0.24693657246263803, Val Loss = 11.15516469492903\n",
      "155100.iterasyon, Train Loss = 0.24691819423959113, Val Loss = 11.155349823450935\n",
      "155200.iterasyon, Train Loss = 0.2468998354721269, Val Loss = 11.15553487245508\n",
      "155300.iterasyon, Train Loss = 0.24688149613716318, Val Loss = 11.155719841964077\n",
      "155400.iterasyon, Train Loss = 0.24686317621163445, Val Loss = 11.15590473200064\n",
      "155500.iterasyon, Train Loss = 0.24684487567251534, Val Loss = 11.156089542587422\n",
      "155600.iterasyon, Train Loss = 0.24682659449679975, Val Loss = 11.15627427374714\n",
      "155700.iterasyon, Train Loss = 0.2468083326615094, Val Loss = 11.156458925502484\n",
      "155800.iterasyon, Train Loss = 0.2467900901436945, Val Loss = 11.156643497876166\n",
      "155900.iterasyon, Train Loss = 0.24677186692043254, Val Loss = 11.156827990890893\n",
      "156000.iterasyon, Train Loss = 0.2467536629688283, Val Loss = 11.157012404569477\n",
      "156100.iterasyon, Train Loss = 0.2467354782660087, Val Loss = 11.157196738934669\n",
      "156200.iterasyon, Train Loss = 0.24671731278913675, Val Loss = 11.157380994009193\n",
      "156300.iterasyon, Train Loss = 0.2466991665153929, Val Loss = 11.157565169815886\n",
      "156400.iterasyon, Train Loss = 0.24668103942199424, Val Loss = 11.157749266377486\n",
      "156500.iterasyon, Train Loss = 0.24666293148617593, Val Loss = 11.157933283716883\n",
      "156600.iterasyon, Train Loss = 0.24664484268520437, Val Loss = 11.158117221856864\n",
      "156700.iterasyon, Train Loss = 0.24662677299637037, Val Loss = 11.158301080820253\n",
      "156800.iterasyon, Train Loss = 0.2466087223969949, Val Loss = 11.15848486062992\n",
      "156900.iterasyon, Train Loss = 0.24659069086442445, Val Loss = 11.158668561308728\n",
      "157000.iterasyon, Train Loss = 0.2465726783760306, Val Loss = 11.15885218287955\n",
      "157100.iterasyon, Train Loss = 0.24655468490921306, Val Loss = 11.159035725365316\n",
      "157200.iterasyon, Train Loss = 0.2465367104413981, Val Loss = 11.1592191887889\n",
      "157300.iterasyon, Train Loss = 0.24651875495003917, Val Loss = 11.1594025731732\n",
      "157400.iterasyon, Train Loss = 0.24650081841261207, Val Loss = 11.159585878541177\n",
      "157500.iterasyon, Train Loss = 0.24648290080662802, Val Loss = 11.159769104915757\n",
      "157600.iterasyon, Train Loss = 0.24646500210961322, Val Loss = 11.159952252319934\n",
      "157700.iterasyon, Train Loss = 0.24644712229913399, Val Loss = 11.160135320776618\n",
      "157800.iterasyon, Train Loss = 0.24642926135277252, Val Loss = 11.160318310308773\n",
      "157900.iterasyon, Train Loss = 0.24641141924813778, Val Loss = 11.160501220939464\n",
      "158000.iterasyon, Train Loss = 0.24639359596287247, Val Loss = 11.160684052691673\n",
      "158100.iterasyon, Train Loss = 0.24637579147463884, Val Loss = 11.160866805588388\n",
      "158200.iterasyon, Train Loss = 0.2463580057611293, Val Loss = 11.16104947965268\n",
      "158300.iterasyon, Train Loss = 0.24634023880006076, Val Loss = 11.16123207490759\n",
      "158400.iterasyon, Train Loss = 0.24632249056917677, Val Loss = 11.161414591376149\n",
      "158500.iterasyon, Train Loss = 0.24630476104624752, Val Loss = 11.161597029081426\n",
      "158600.iterasyon, Train Loss = 0.24628705020906852, Val Loss = 11.161779388046545\n",
      "158700.iterasyon, Train Loss = 0.24626935803546343, Val Loss = 11.161961668294536\n",
      "158800.iterasyon, Train Loss = 0.2462516845032802, Val Loss = 11.16214386984857\n",
      "158900.iterasyon, Train Loss = 0.2462340295903942, Val Loss = 11.162325992731706\n",
      "159000.iterasyon, Train Loss = 0.24621639327470632, Val Loss = 11.1625080369671\n",
      "159100.iterasyon, Train Loss = 0.24619877553414116, Val Loss = 11.16269000257792\n",
      "159200.iterasyon, Train Loss = 0.2461811763466548, Val Loss = 11.162871889587283\n",
      "159300.iterasyon, Train Loss = 0.24616359569022608, Val Loss = 11.163053698018334\n",
      "159400.iterasyon, Train Loss = 0.246146033542858, Val Loss = 11.163235427894286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159500.iterasyon, Train Loss = 0.24612848988258174, Val Loss = 11.163417079238359\n",
      "159600.iterasyon, Train Loss = 0.24611096468745577, Val Loss = 11.163598652073714\n",
      "159700.iterasyon, Train Loss = 0.24609345793556303, Val Loss = 11.163780146423605\n",
      "159800.iterasyon, Train Loss = 0.24607596960500897, Val Loss = 11.163961562311234\n",
      "159900.iterasyon, Train Loss = 0.24605849967393104, Val Loss = 11.164142899759806\n",
      "160000.iterasyon, Train Loss = 0.24604104812049032, Val Loss = 11.164324158792603\n",
      "160100.iterasyon, Train Loss = 0.2460236149228724, Val Loss = 11.16450533943289\n",
      "160200.iterasyon, Train Loss = 0.24600620005928722, Val Loss = 11.16468644170393\n",
      "160300.iterasyon, Train Loss = 0.24598880350797303, Val Loss = 11.164867465629014\n",
      "160400.iterasyon, Train Loss = 0.24597142524719576, Val Loss = 11.165048411231446\n",
      "160500.iterasyon, Train Loss = 0.24595406525523852, Val Loss = 11.165229278534554\n",
      "160600.iterasyon, Train Loss = 0.24593672351042192, Val Loss = 11.16541006756164\n",
      "160700.iterasyon, Train Loss = 0.24591939999108178, Val Loss = 11.16559077833603\n",
      "160800.iterasyon, Train Loss = 0.24590209467558846, Val Loss = 11.165771410881055\n",
      "160900.iterasyon, Train Loss = 0.24588480754232908, Val Loss = 11.165951965220072\n",
      "161000.iterasyon, Train Loss = 0.24586753856972302, Val Loss = 11.16613244137646\n",
      "161100.iterasyon, Train Loss = 0.24585028773620896, Val Loss = 11.16631283937361\n",
      "161200.iterasyon, Train Loss = 0.24583305502025934, Val Loss = 11.166493159234909\n",
      "161300.iterasyon, Train Loss = 0.24581584040036353, Val Loss = 11.166673400983747\n",
      "161400.iterasyon, Train Loss = 0.24579864385504194, Val Loss = 11.166853564643539\n",
      "161500.iterasyon, Train Loss = 0.24578146536283707, Val Loss = 11.167033650237713\n",
      "161600.iterasyon, Train Loss = 0.24576430490231752, Val Loss = 11.167213657789707\n",
      "161700.iterasyon, Train Loss = 0.24574716245208264, Val Loss = 11.167393587322959\n",
      "161800.iterasyon, Train Loss = 0.2457300379907453, Val Loss = 11.16757343886093\n",
      "161900.iterasyon, Train Loss = 0.2457129314969552, Val Loss = 11.167753212427113\n",
      "162000.iterasyon, Train Loss = 0.24569584294938207, Val Loss = 11.167932908044934\n",
      "162100.iterasyon, Train Loss = 0.2456787723267196, Val Loss = 11.168112525737921\n",
      "162200.iterasyon, Train Loss = 0.2456617196076901, Val Loss = 11.168292065529561\n",
      "162300.iterasyon, Train Loss = 0.24564468477103826, Val Loss = 11.168471527443389\n",
      "162400.iterasyon, Train Loss = 0.2456276677955329, Val Loss = 11.168650911502915\n",
      "162500.iterasyon, Train Loss = 0.24561066865997305, Val Loss = 11.168830217731692\n",
      "162600.iterasyon, Train Loss = 0.2455936873431762, Val Loss = 11.16900944615327\n",
      "162700.iterasyon, Train Loss = 0.24557672382399384, Val Loss = 11.169188596791132\n",
      "162800.iterasyon, Train Loss = 0.24555977808128956, Val Loss = 11.169367669668954\n",
      "162900.iterasyon, Train Loss = 0.2455428500939646, Val Loss = 11.169546664810264\n",
      "163000.iterasyon, Train Loss = 0.24552593984093693, Val Loss = 11.169725582238648\n",
      "163100.iterasyon, Train Loss = 0.2455090473011524, Val Loss = 11.169904421977709\n",
      "163200.iterasyon, Train Loss = 0.24549217245358212, Val Loss = 11.170083184051057\n",
      "163300.iterasyon, Train Loss = 0.245475315277221, Val Loss = 11.17026186848231\n",
      "163400.iterasyon, Train Loss = 0.24545847575108956, Val Loss = 11.170440475295123\n",
      "163500.iterasyon, Train Loss = 0.24544165385423333, Val Loss = 11.170619004513094\n",
      "163600.iterasyon, Train Loss = 0.2454248495657199, Val Loss = 11.170797456159933\n",
      "163700.iterasyon, Train Loss = 0.2454080628646437, Val Loss = 11.170975830259255\n",
      "163800.iterasyon, Train Loss = 0.2453912937301251, Val Loss = 11.171154126834786\n",
      "163900.iterasyon, Train Loss = 0.2453745421413062, Val Loss = 11.17133234591018\n",
      "164000.iterasyon, Train Loss = 0.24535780807735622, Val Loss = 11.171510487509135\n",
      "164100.iterasyon, Train Loss = 0.24534109151746833, Val Loss = 11.171688551655333\n",
      "164200.iterasyon, Train Loss = 0.24532439244085757, Val Loss = 11.171866538372562\n",
      "164300.iterasyon, Train Loss = 0.2453077108267713, Val Loss = 11.172044447684446\n",
      "164400.iterasyon, Train Loss = 0.2452910466544679, Val Loss = 11.172222279614864\n",
      "164500.iterasyon, Train Loss = 0.24527439990324204, Val Loss = 11.172400034187458\n",
      "164600.iterasyon, Train Loss = 0.24525777055241296, Val Loss = 11.172577711425987\n",
      "164700.iterasyon, Train Loss = 0.24524115858131731, Val Loss = 11.172755311354221\n",
      "164800.iterasyon, Train Loss = 0.2452245639693175, Val Loss = 11.17293283399599\n",
      "164900.iterasyon, Train Loss = 0.24520798669580338, Val Loss = 11.17311027937504\n",
      "165000.iterasyon, Train Loss = 0.2451914267401889, Val Loss = 11.173287647515167\n",
      "165100.iterasyon, Train Loss = 0.2451748840819102, Val Loss = 11.173464938440192\n",
      "165200.iterasyon, Train Loss = 0.2451583587004288, Val Loss = 11.173642152173976\n",
      "165300.iterasyon, Train Loss = 0.2451418505752303, Val Loss = 11.173819288740312\n",
      "165400.iterasyon, Train Loss = 0.24512535968582302, Val Loss = 11.17399634816301\n",
      "165500.iterasyon, Train Loss = 0.2451088860117446, Val Loss = 11.174173330465957\n",
      "165600.iterasyon, Train Loss = 0.24509242953255148, Val Loss = 11.17435023567301\n",
      "165700.iterasyon, Train Loss = 0.24507599022782722, Val Loss = 11.174527063807975\n",
      "165800.iterasyon, Train Loss = 0.24505956807717624, Val Loss = 11.17470381489481\n",
      "165900.iterasyon, Train Loss = 0.245043163060233, Val Loss = 11.174880488957365\n",
      "166000.iterasyon, Train Loss = 0.24502677515664573, Val Loss = 11.175057086019583\n",
      "166100.iterasyon, Train Loss = 0.24501040434609755, Val Loss = 11.175233606105337\n",
      "166200.iterasyon, Train Loss = 0.24499405060829219, Val Loss = 11.17541004923849\n",
      "166300.iterasyon, Train Loss = 0.24497771392295165, Val Loss = 11.175586415443071\n",
      "166400.iterasyon, Train Loss = 0.24496139426983254, Val Loss = 11.175762704742954\n",
      "166500.iterasyon, Train Loss = 0.24494509162870634, Val Loss = 11.175938917162112\n",
      "166600.iterasyon, Train Loss = 0.2449288059793676, Val Loss = 11.176115052724489\n",
      "166700.iterasyon, Train Loss = 0.24491253730164655, Val Loss = 11.17629111145406\n",
      "166800.iterasyon, Train Loss = 0.24489628557538112, Val Loss = 11.176467093374802\n",
      "166900.iterasyon, Train Loss = 0.24488005078044842, Val Loss = 11.176642998510653\n",
      "167000.iterasyon, Train Loss = 0.24486383289673783, Val Loss = 11.17681882688567\n",
      "167100.iterasyon, Train Loss = 0.24484763190416545, Val Loss = 11.176994578523844\n",
      "167200.iterasyon, Train Loss = 0.2448314477826771, Val Loss = 11.177170253449146\n",
      "167300.iterasyon, Train Loss = 0.24481528051223478, Val Loss = 11.17734585168564\n",
      "167400.iterasyon, Train Loss = 0.24479913007282839, Val Loss = 11.177521373257335\n",
      "167500.iterasyon, Train Loss = 0.24478299644446744, Val Loss = 11.177696818188283\n",
      "167600.iterasyon, Train Loss = 0.24476687960719054, Val Loss = 11.177872186502546\n",
      "167700.iterasyon, Train Loss = 0.24475077954105398, Val Loss = 11.178047478224148\n",
      "167800.iterasyon, Train Loss = 0.24473469622614227, Val Loss = 11.178222693377194\n",
      "167900.iterasyon, Train Loss = 0.2447186296425593, Val Loss = 11.178397831985766\n",
      "168000.iterasyon, Train Loss = 0.24470257977043952, Val Loss = 11.178572894073906\n",
      "168100.iterasyon, Train Loss = 0.24468654658993425, Val Loss = 11.178747879665746\n",
      "168200.iterasyon, Train Loss = 0.24467053008121864, Val Loss = 11.178922788785375\n",
      "168300.iterasyon, Train Loss = 0.24465453022449427, Val Loss = 11.179097621456924\n",
      "168400.iterasyon, Train Loss = 0.2446385469999831, Val Loss = 11.17927237770452\n",
      "168500.iterasyon, Train Loss = 0.2446225803879364, Val Loss = 11.179447057552274\n",
      "168600.iterasyon, Train Loss = 0.24460663036861718, Val Loss = 11.179621661024362\n",
      "168700.iterasyon, Train Loss = 0.24459069692232485, Val Loss = 11.179796188144874\n",
      "168800.iterasyon, Train Loss = 0.2445747800293752, Val Loss = 11.179970638938025\n",
      "168900.iterasyon, Train Loss = 0.24455887967010723, Val Loss = 11.180145013427932\n",
      "169000.iterasyon, Train Loss = 0.24454299582488362, Val Loss = 11.180319311638792\n",
      "169100.iterasyon, Train Loss = 0.24452712847409305, Val Loss = 11.180493533594788\n",
      "169200.iterasyon, Train Loss = 0.24451127759814142, Val Loss = 11.180667679320164\n",
      "169300.iterasyon, Train Loss = 0.24449544317746522, Val Loss = 11.18084174883908\n",
      "169400.iterasyon, Train Loss = 0.2444796251925184, Val Loss = 11.181015742175719\n",
      "169500.iterasyon, Train Loss = 0.24446382362378102, Val Loss = 11.181189659354336\n",
      "169600.iterasyon, Train Loss = 0.24444803845175278, Val Loss = 11.181363500399188\n",
      "169700.iterasyon, Train Loss = 0.24443226965696083, Val Loss = 11.181537265334448\n",
      "169800.iterasyon, Train Loss = 0.24441651721995308, Val Loss = 11.181710954184416\n",
      "169900.iterasyon, Train Loss = 0.24440078112129873, Val Loss = 11.181884566973325\n",
      "170000.iterasyon, Train Loss = 0.24438506134159388, Val Loss = 11.182058103725424\n",
      "170100.iterasyon, Train Loss = 0.2443693578614533, Val Loss = 11.182231564465015\n",
      "170200.iterasyon, Train Loss = 0.24435367066151947, Val Loss = 11.182404949216334\n",
      "170300.iterasyon, Train Loss = 0.2443379997224551, Val Loss = 11.182578258003709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170400.iterasyon, Train Loss = 0.24432234502494102, Val Loss = 11.18275149085142\n",
      "170500.iterasyon, Train Loss = 0.24430670654968953, Val Loss = 11.182924647783759\n",
      "170600.iterasyon, Train Loss = 0.24429108427743096, Val Loss = 11.183097728825064\n",
      "170700.iterasyon, Train Loss = 0.244275478188917, Val Loss = 11.183270733999677\n",
      "170800.iterasyon, Train Loss = 0.24425988826493014, Val Loss = 11.183443663331868\n",
      "170900.iterasyon, Train Loss = 0.2442443144862624, Val Loss = 11.183616516846003\n",
      "171000.iterasyon, Train Loss = 0.24422875683373976, Val Loss = 11.183789294566456\n",
      "171100.iterasyon, Train Loss = 0.24421321528820664, Val Loss = 11.183961996517532\n",
      "171200.iterasyon, Train Loss = 0.24419768983052903, Val Loss = 11.184134622723592\n",
      "171300.iterasyon, Train Loss = 0.24418218044159848, Val Loss = 11.184307173209067\n",
      "171400.iterasyon, Train Loss = 0.24416668710232547, Val Loss = 11.184479647998288\n",
      "171500.iterasyon, Train Loss = 0.24415120979364416, Val Loss = 11.18465204711568\n",
      "171600.iterasyon, Train Loss = 0.2441357484965144, Val Loss = 11.184824370585597\n",
      "171700.iterasyon, Train Loss = 0.24412030319191805, Val Loss = 11.184996618432473\n",
      "171800.iterasyon, Train Loss = 0.24410487386085297, Val Loss = 11.185168790680645\n",
      "171900.iterasyon, Train Loss = 0.24408946048434496, Val Loss = 11.185340887354629\n",
      "172000.iterasyon, Train Loss = 0.24407406304344212, Val Loss = 11.185512908478817\n",
      "172100.iterasyon, Train Loss = 0.24405868151921442, Val Loss = 11.18568485407763\n",
      "172200.iterasyon, Train Loss = 0.2440433158927531, Val Loss = 11.185856724175567\n",
      "172300.iterasyon, Train Loss = 0.24402796614517566, Val Loss = 11.186028518796977\n",
      "172400.iterasyon, Train Loss = 0.24401263225761374, Val Loss = 11.186200237966418\n",
      "172500.iterasyon, Train Loss = 0.24399731421122803, Val Loss = 11.186371881708258\n",
      "172600.iterasyon, Train Loss = 0.2439820119872029, Val Loss = 11.186543450047026\n",
      "172700.iterasyon, Train Loss = 0.24396672556673782, Val Loss = 11.186714943007212\n",
      "172800.iterasyon, Train Loss = 0.24395145493106096, Val Loss = 11.18688636061327\n",
      "172900.iterasyon, Train Loss = 0.24393620006142044, Val Loss = 11.187057702889721\n",
      "173000.iterasyon, Train Loss = 0.24392096093908341, Val Loss = 11.187228969861046\n",
      "173100.iterasyon, Train Loss = 0.2439057375453422, Val Loss = 11.187400161551796\n",
      "173200.iterasyon, Train Loss = 0.2438905298615143, Val Loss = 11.18757127798645\n",
      "173300.iterasyon, Train Loss = 0.24387533786893512, Val Loss = 11.187742319189528\n",
      "173400.iterasyon, Train Loss = 0.24386016154895987, Val Loss = 11.187913285185633\n",
      "173500.iterasyon, Train Loss = 0.243845000882971, Val Loss = 11.18808417599924\n",
      "173600.iterasyon, Train Loss = 0.2438298558523718, Val Loss = 11.188254991654937\n",
      "173700.iterasyon, Train Loss = 0.2438147264385878, Val Loss = 11.188425732177228\n",
      "173800.iterasyon, Train Loss = 0.2437996126230589, Val Loss = 11.188596397590727\n",
      "173900.iterasyon, Train Loss = 0.24378451438725998, Val Loss = 11.188766987919978\n",
      "174000.iterasyon, Train Loss = 0.2437694317126786, Val Loss = 11.188937503189567\n",
      "174100.iterasyon, Train Loss = 0.24375436458082686, Val Loss = 11.189107943424064\n",
      "174200.iterasyon, Train Loss = 0.24373931297323884, Val Loss = 11.189278308648074\n",
      "174300.iterasyon, Train Loss = 0.24372427687146922, Val Loss = 11.189448598886196\n",
      "174400.iterasyon, Train Loss = 0.24370925625709497, Val Loss = 11.189618814163024\n",
      "174500.iterasyon, Train Loss = 0.24369425111171794, Val Loss = 11.189788954503214\n",
      "174600.iterasyon, Train Loss = 0.24367926141695842, Val Loss = 11.189959019931347\n",
      "174700.iterasyon, Train Loss = 0.24366428715445432, Val Loss = 11.190129010472083\n",
      "174800.iterasyon, Train Loss = 0.24364932830587693, Val Loss = 11.190298926150021\n",
      "174900.iterasyon, Train Loss = 0.24363438485290612, Val Loss = 11.19046876698982\n",
      "175000.iterasyon, Train Loss = 0.24361945677725408, Val Loss = 11.190638533016157\n",
      "175100.iterasyon, Train Loss = 0.24360454406064982, Val Loss = 11.190808224253615\n",
      "175200.iterasyon, Train Loss = 0.24358964668484018, Val Loss = 11.19097784072693\n",
      "175300.iterasyon, Train Loss = 0.2435747646316001, Val Loss = 11.191147382460738\n",
      "175400.iterasyon, Train Loss = 0.2435598978827257, Val Loss = 11.191316849479707\n",
      "175500.iterasyon, Train Loss = 0.2435450464200298, Val Loss = 11.191486241808537\n",
      "175600.iterasyon, Train Loss = 0.24353021022534696, Val Loss = 11.191655559471949\n",
      "175700.iterasyon, Train Loss = 0.24351538928054042, Val Loss = 11.191824802494558\n",
      "175800.iterasyon, Train Loss = 0.24350058356748627, Val Loss = 11.191993970901152\n",
      "175900.iterasyon, Train Loss = 0.24348579306808904, Val Loss = 11.192163064716386\n",
      "176000.iterasyon, Train Loss = 0.24347101776426924, Val Loss = 11.192332083965013\n",
      "176100.iterasyon, Train Loss = 0.24345625763797185, Val Loss = 11.192501028671746\n",
      "176200.iterasyon, Train Loss = 0.24344151267115868, Val Loss = 11.192669898861308\n",
      "176300.iterasyon, Train Loss = 0.24342678284582275, Val Loss = 11.192838694558423\n",
      "176400.iterasyon, Train Loss = 0.24341206814396377, Val Loss = 11.193007415787882\n",
      "176500.iterasyon, Train Loss = 0.2433973685476197, Val Loss = 11.193176062574397\n",
      "176600.iterasyon, Train Loss = 0.2433826840388353, Val Loss = 11.193344634942731\n",
      "176700.iterasyon, Train Loss = 0.2433680145996825, Val Loss = 11.193513132917692\n",
      "176800.iterasyon, Train Loss = 0.24335336021225618, Val Loss = 11.193681556523995\n",
      "176900.iterasyon, Train Loss = 0.2433387208586695, Val Loss = 11.193849905786408\n",
      "177000.iterasyon, Train Loss = 0.24332409652105677, Val Loss = 11.194018180729762\n",
      "177100.iterasyon, Train Loss = 0.24330948718157688, Val Loss = 11.194186381378795\n",
      "177200.iterasyon, Train Loss = 0.243294892822401, Val Loss = 11.19435450775834\n",
      "177300.iterasyon, Train Loss = 0.24328031342573334, Val Loss = 11.194522559893201\n",
      "177400.iterasyon, Train Loss = 0.24326574897379163, Val Loss = 11.194690537808158\n",
      "177500.iterasyon, Train Loss = 0.2432511994488162, Val Loss = 11.194858441528034\n",
      "177600.iterasyon, Train Loss = 0.24323666483306924, Val Loss = 11.195026271077689\n",
      "177700.iterasyon, Train Loss = 0.24322214510883136, Val Loss = 11.195194026481886\n",
      "177800.iterasyon, Train Loss = 0.24320764025840594, Val Loss = 11.195361707765521\n",
      "177900.iterasyon, Train Loss = 0.24319315026411936, Val Loss = 11.195529314953408\n",
      "178000.iterasyon, Train Loss = 0.24317867510831348, Val Loss = 11.195696848070416\n",
      "178100.iterasyon, Train Loss = 0.2431642147733559, Val Loss = 11.19586430714133\n",
      "178200.iterasyon, Train Loss = 0.24314976924163353, Val Loss = 11.196031692191053\n",
      "178300.iterasyon, Train Loss = 0.2431353384955549, Val Loss = 11.196199003244434\n",
      "178400.iterasyon, Train Loss = 0.24312092251754655, Val Loss = 11.19636624032637\n",
      "178500.iterasyon, Train Loss = 0.2431065212900594, Val Loss = 11.196533403461716\n",
      "178600.iterasyon, Train Loss = 0.24309213479556047, Val Loss = 11.196700492675372\n",
      "178700.iterasyon, Train Loss = 0.2430777630165427, Val Loss = 11.19686750799222\n",
      "178800.iterasyon, Train Loss = 0.2430634059355177, Val Loss = 11.197034449437169\n",
      "178900.iterasyon, Train Loss = 0.24304906353501676, Val Loss = 11.197201317035086\n",
      "179000.iterasyon, Train Loss = 0.2430347357975925, Val Loss = 11.197368110810894\n",
      "179100.iterasyon, Train Loss = 0.24302042270581692, Val Loss = 11.19753483078948\n",
      "179200.iterasyon, Train Loss = 0.24300612424228718, Val Loss = 11.197701476995773\n",
      "179300.iterasyon, Train Loss = 0.24299184038961394, Val Loss = 11.197868049454723\n",
      "179400.iterasyon, Train Loss = 0.24297757113043586, Val Loss = 11.198034548191222\n",
      "179500.iterasyon, Train Loss = 0.2429633164474041, Val Loss = 11.19820097323022\n",
      "179600.iterasyon, Train Loss = 0.24294907632319787, Val Loss = 11.198367324596667\n",
      "179700.iterasyon, Train Loss = 0.24293485074051377, Val Loss = 11.198533602315505\n",
      "179800.iterasyon, Train Loss = 0.24292063968206482, Val Loss = 11.198699806411721\n",
      "179900.iterasyon, Train Loss = 0.24290644313059315, Val Loss = 11.198865936910224\n",
      "180000.iterasyon, Train Loss = 0.2428922610688542, Val Loss = 11.199031993835957\n",
      "180100.iterasyon, Train Loss = 0.242878093479624, Val Loss = 11.199197977213963\n",
      "180200.iterasyon, Train Loss = 0.24286394034570594, Val Loss = 11.19936388706916\n",
      "180300.iterasyon, Train Loss = 0.2428498016499172, Val Loss = 11.199529723426522\n",
      "180400.iterasyon, Train Loss = 0.24283567737509373, Val Loss = 11.199695486311064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180500.iterasyon, Train Loss = 0.24282156750409786, Val Loss = 11.199861175747756\n",
      "180600.iterasyon, Train Loss = 0.24280747201980835, Val Loss = 11.200026791761616\n",
      "180700.iterasyon, Train Loss = 0.24279339090512464, Val Loss = 11.20019233437765\n",
      "180800.iterasyon, Train Loss = 0.24277932414296863, Val Loss = 11.200357803620856\n",
      "180900.iterasyon, Train Loss = 0.24276527171627663, Val Loss = 11.20052319951623\n",
      "181000.iterasyon, Train Loss = 0.24275123360801543, Val Loss = 11.200688522088807\n",
      "181100.iterasyon, Train Loss = 0.24273720980116023, Val Loss = 11.200853771363604\n",
      "181200.iterasyon, Train Loss = 0.24272320027871266, Val Loss = 11.201018947365641\n",
      "181300.iterasyon, Train Loss = 0.24270920502369628, Val Loss = 11.20118405011998\n",
      "181400.iterasyon, Train Loss = 0.24269522401914956, Val Loss = 11.201349079651635\n",
      "181500.iterasyon, Train Loss = 0.24268125724813394, Val Loss = 11.201514035985639\n",
      "181600.iterasyon, Train Loss = 0.2426673046937279, Val Loss = 11.201678919147062\n",
      "181700.iterasyon, Train Loss = 0.2426533663390392, Val Loss = 11.201843729160936\n",
      "181800.iterasyon, Train Loss = 0.24263944216718225, Val Loss = 11.202008466052352\n",
      "181900.iterasyon, Train Loss = 0.242625532161302, Val Loss = 11.202173129846361\n",
      "182000.iterasyon, Train Loss = 0.24261163630455487, Val Loss = 11.202337720568064\n",
      "182100.iterasyon, Train Loss = 0.2425977545801237, Val Loss = 11.2025022382425\n",
      "182200.iterasyon, Train Loss = 0.24258388697121117, Val Loss = 11.202666682894705\n",
      "182300.iterasyon, Train Loss = 0.24257003346103626, Val Loss = 11.202831054549879\n",
      "182400.iterasyon, Train Loss = 0.242556194032837, Val Loss = 11.202995353233016\n",
      "182500.iterasyon, Train Loss = 0.24254236866987652, Val Loss = 11.203159578969245\n",
      "182600.iterasyon, Train Loss = 0.24252855735543477, Val Loss = 11.203323731783655\n",
      "182700.iterasyon, Train Loss = 0.242514760072808, Val Loss = 11.203487811701372\n",
      "182800.iterasyon, Train Loss = 0.2425009768053196, Val Loss = 11.20365181874748\n",
      "182900.iterasyon, Train Loss = 0.2424872075363059, Val Loss = 11.20381575294713\n",
      "183000.iterasyon, Train Loss = 0.24247345224912875, Val Loss = 11.20397961432537\n",
      "183100.iterasyon, Train Loss = 0.2424597109271616, Val Loss = 11.204143402907425\n",
      "183200.iterasyon, Train Loss = 0.24244598355380745, Val Loss = 11.204307118718326\n",
      "183300.iterasyon, Train Loss = 0.24243227011248156, Val Loss = 11.204470761783254\n",
      "183400.iterasyon, Train Loss = 0.2424185705866219, Val Loss = 11.204634332127377\n",
      "183500.iterasyon, Train Loss = 0.2424048849596869, Val Loss = 11.204797829775758\n",
      "183600.iterasyon, Train Loss = 0.24239121321515034, Val Loss = 11.204961254753634\n",
      "183700.iterasyon, Train Loss = 0.2423775553365091, Val Loss = 11.205124607086097\n",
      "183800.iterasyon, Train Loss = 0.24236391130728005, Val Loss = 11.205287886798319\n",
      "183900.iterasyon, Train Loss = 0.242350281110998, Val Loss = 11.205451093915498\n",
      "184000.iterasyon, Train Loss = 0.2423366647312147, Val Loss = 11.205614228462728\n",
      "184100.iterasyon, Train Loss = 0.24232306215150823, Val Loss = 11.205777290465203\n",
      "184200.iterasyon, Train Loss = 0.2423094733554688, Val Loss = 11.205940279948155\n",
      "184300.iterasyon, Train Loss = 0.2422958983267107, Val Loss = 11.206103196936722\n",
      "184400.iterasyon, Train Loss = 0.2422823370488659, Val Loss = 11.206266041456072\n",
      "184500.iterasyon, Train Loss = 0.2422687895055833, Val Loss = 11.206428813531417\n",
      "184600.iterasyon, Train Loss = 0.24225525568053988, Val Loss = 11.20659151318794\n",
      "184700.iterasyon, Train Loss = 0.24224173555742037, Val Loss = 11.206754140450816\n",
      "184800.iterasyon, Train Loss = 0.24222822911993588, Val Loss = 11.2069166953453\n",
      "184900.iterasyon, Train Loss = 0.24221473635181465, Val Loss = 11.207079177896574\n",
      "185000.iterasyon, Train Loss = 0.24220125723680444, Val Loss = 11.207241588129842\n",
      "185100.iterasyon, Train Loss = 0.24218779175867303, Val Loss = 11.207403926070329\n",
      "185200.iterasyon, Train Loss = 0.24217433990120643, Val Loss = 11.207566191743242\n",
      "185300.iterasyon, Train Loss = 0.24216090164821027, Val Loss = 11.207728385173803\n",
      "185400.iterasyon, Train Loss = 0.242147476983511, Val Loss = 11.207890506387251\n",
      "185500.iterasyon, Train Loss = 0.24213406589094974, Val Loss = 11.208052555408797\n",
      "185600.iterasyon, Train Loss = 0.24212066835438983, Val Loss = 11.20821453226372\n",
      "185700.iterasyon, Train Loss = 0.2421072843577121, Val Loss = 11.208376436977256\n",
      "185800.iterasyon, Train Loss = 0.24209391388482024, Val Loss = 11.208538269574573\n",
      "185900.iterasyon, Train Loss = 0.2420805569196317, Val Loss = 11.208700030081014\n",
      "186000.iterasyon, Train Loss = 0.2420672134460877, Val Loss = 11.208861718521783\n",
      "186100.iterasyon, Train Loss = 0.24205388344814704, Val Loss = 11.209023334922142\n",
      "186200.iterasyon, Train Loss = 0.24204056690978235, Val Loss = 11.209184879307342\n",
      "186300.iterasyon, Train Loss = 0.2420272638149932, Val Loss = 11.209346351702688\n",
      "186400.iterasyon, Train Loss = 0.24201397414779383, Val Loss = 11.209507752133412\n",
      "186500.iterasyon, Train Loss = 0.24200069789221756, Val Loss = 11.20966908062478\n",
      "186600.iterasyon, Train Loss = 0.24198743503231787, Val Loss = 11.209830337202112\n",
      "186700.iterasyon, Train Loss = 0.24197418555216565, Val Loss = 11.20999152189065\n",
      "186800.iterasyon, Train Loss = 0.24196094943585153, Val Loss = 11.210152634715689\n",
      "186900.iterasyon, Train Loss = 0.24194772666748351, Val Loss = 11.210313675702526\n",
      "187000.iterasyon, Train Loss = 0.24193451723119253, Val Loss = 11.210474644876424\n",
      "187100.iterasyon, Train Loss = 0.24192132111112066, Val Loss = 11.210635542262718\n",
      "187200.iterasyon, Train Loss = 0.24190813829143773, Val Loss = 11.210796367886694\n",
      "187300.iterasyon, Train Loss = 0.24189496875632654, Val Loss = 11.210957121773637\n",
      "187400.iterasyon, Train Loss = 0.24188181248998783, Val Loss = 11.21111780394889\n",
      "187500.iterasyon, Train Loss = 0.24186866947664531, Val Loss = 11.21127841443772\n",
      "187600.iterasyon, Train Loss = 0.24185553970054033, Val Loss = 11.211438953265484\n",
      "187700.iterasyon, Train Loss = 0.24184242314593088, Val Loss = 11.211599420457459\n",
      "187800.iterasyon, Train Loss = 0.2418293197970932, Val Loss = 11.211759816039004\n",
      "187900.iterasyon, Train Loss = 0.2418162296383246, Val Loss = 11.211920140035414\n",
      "188000.iterasyon, Train Loss = 0.24180315265393715, Val Loss = 11.212080392472057\n",
      "188100.iterasyon, Train Loss = 0.24179008882826847, Val Loss = 11.212240573374233\n",
      "188200.iterasyon, Train Loss = 0.24177703814566667, Val Loss = 11.212400682767283\n",
      "188300.iterasyon, Train Loss = 0.24176400059050496, Val Loss = 11.21256072067654\n",
      "188400.iterasyon, Train Loss = 0.2417509761471688, Val Loss = 11.212720687127373\n",
      "188500.iterasyon, Train Loss = 0.2417379648000693, Val Loss = 11.212880582145097\n",
      "188600.iterasyon, Train Loss = 0.24172496653362854, Val Loss = 11.213040405755088\n",
      "188700.iterasyon, Train Loss = 0.2417119813322909, Val Loss = 11.213200157982685\n",
      "188800.iterasyon, Train Loss = 0.24169900918052126, Val Loss = 11.213359838853277\n",
      "188900.iterasyon, Train Loss = 0.24168605006279834, Val Loss = 11.213519448392193\n",
      "189000.iterasyon, Train Loss = 0.24167310396362265, Val Loss = 11.213678986624803\n",
      "189100.iterasyon, Train Loss = 0.24166017086751068, Val Loss = 11.21383845357648\n",
      "189200.iterasyon, Train Loss = 0.2416472507589983, Val Loss = 11.213997849272578\n",
      "189300.iterasyon, Train Loss = 0.2416343436226425, Val Loss = 11.2141571737385\n",
      "189400.iterasyon, Train Loss = 0.2416214494430082, Val Loss = 11.214316426999616\n",
      "189500.iterasyon, Train Loss = 0.24160856820469512, Val Loss = 11.214475609081298\n",
      "189600.iterasyon, Train Loss = 0.2415956998923081, Val Loss = 11.214634720008894\n",
      "189700.iterasyon, Train Loss = 0.24158284449047138, Val Loss = 11.214793759807831\n",
      "189800.iterasyon, Train Loss = 0.24157000198383496, Val Loss = 11.214952728503487\n",
      "189900.iterasyon, Train Loss = 0.24155717235706073, Val Loss = 11.215111626121258\n",
      "190000.iterasyon, Train Loss = 0.24154435559482737, Val Loss = 11.215270452686562\n",
      "190100.iterasyon, Train Loss = 0.24153155168183815, Val Loss = 11.215429208224819\n",
      "190200.iterasyon, Train Loss = 0.24151876060280997, Val Loss = 11.215587892761352\n",
      "190300.iterasyon, Train Loss = 0.2415059823424771, Val Loss = 11.215746506321603\n",
      "190400.iterasyon, Train Loss = 0.24149321688559552, Val Loss = 11.215905048931024\n",
      "190500.iterasyon, Train Loss = 0.24148046421693606, Val Loss = 11.216063520614975\n",
      "190600.iterasyon, Train Loss = 0.24146772432128882, Val Loss = 11.216221921398883\n",
      "190700.iterasyon, Train Loss = 0.24145499718346117, Val Loss = 11.216380251308163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190800.iterasyon, Train Loss = 0.24144228278828114, Val Loss = 11.216538510368235\n",
      "190900.iterasyon, Train Loss = 0.2414295811205903, Val Loss = 11.216696698604538\n",
      "191000.iterasyon, Train Loss = 0.24141689216524967, Val Loss = 11.216854816042492\n",
      "191100.iterasyon, Train Loss = 0.24140421590714334, Val Loss = 11.217012862707527\n",
      "191200.iterasyon, Train Loss = 0.24139155233116316, Val Loss = 11.217170838625108\n",
      "191300.iterasyon, Train Loss = 0.24137890142222784, Val Loss = 11.217328743820614\n",
      "191400.iterasyon, Train Loss = 0.24136626316527152, Val Loss = 11.217486578319486\n",
      "191500.iterasyon, Train Loss = 0.2413536375452423, Val Loss = 11.217644342147228\n",
      "191600.iterasyon, Train Loss = 0.24134102454711226, Val Loss = 11.217802035329191\n",
      "191700.iterasyon, Train Loss = 0.24132842415586644, Val Loss = 11.217959657890928\n",
      "191800.iterasyon, Train Loss = 0.241315836356507, Val Loss = 11.2181172098578\n",
      "191900.iterasyon, Train Loss = 0.2413032611340612, Val Loss = 11.218274691255287\n",
      "192000.iterasyon, Train Loss = 0.2412906984735653, Val Loss = 11.218432102108846\n",
      "192100.iterasyon, Train Loss = 0.24127814836007846, Val Loss = 11.218589442443964\n",
      "192200.iterasyon, Train Loss = 0.24126561077867414, Val Loss = 11.218746712286041\n",
      "192300.iterasyon, Train Loss = 0.2412530857144484, Val Loss = 11.21890391166059\n",
      "192400.iterasyon, Train Loss = 0.2412405731525085, Val Loss = 11.219061040593049\n",
      "192500.iterasyon, Train Loss = 0.241228073077984, Val Loss = 11.219218099108915\n",
      "192600.iterasyon, Train Loss = 0.24121558547602007, Val Loss = 11.219375087233638\n",
      "192700.iterasyon, Train Loss = 0.24120311033178113, Val Loss = 11.2195320049927\n",
      "192800.iterasyon, Train Loss = 0.24119064763044748, Val Loss = 11.219688852411544\n",
      "192900.iterasyon, Train Loss = 0.2411781973572197, Val Loss = 11.219845629515667\n",
      "193000.iterasyon, Train Loss = 0.24116575949730942, Val Loss = 11.220002336330587\n",
      "193100.iterasyon, Train Loss = 0.24115333403595193, Val Loss = 11.220158972881745\n",
      "193200.iterasyon, Train Loss = 0.2411409209584006, Val Loss = 11.220315539194619\n",
      "193300.iterasyon, Train Loss = 0.24112852024992162, Val Loss = 11.220472035294737\n",
      "193400.iterasyon, Train Loss = 0.24111613189580006, Val Loss = 11.22062846120757\n",
      "193500.iterasyon, Train Loss = 0.24110375588133934, Val Loss = 11.2207848169586\n",
      "193600.iterasyon, Train Loss = 0.24109139219186293, Val Loss = 11.220941102573343\n",
      "193700.iterasyon, Train Loss = 0.2410790408127036, Val Loss = 11.221097318077279\n",
      "193800.iterasyon, Train Loss = 0.24106670172922018, Val Loss = 11.221253463495923\n",
      "193900.iterasyon, Train Loss = 0.24105437492678358, Val Loss = 11.221409538854802\n",
      "194000.iterasyon, Train Loss = 0.2410420603907872, Val Loss = 11.22156554417936\n",
      "194100.iterasyon, Train Loss = 0.2410297581066354, Val Loss = 11.221721479495113\n",
      "194200.iterasyon, Train Loss = 0.24101746805975124, Val Loss = 11.221877344827618\n",
      "194300.iterasyon, Train Loss = 0.24100519023557995, Val Loss = 11.222033140202297\n",
      "194400.iterasyon, Train Loss = 0.2409929246195808, Val Loss = 11.222188865644737\n",
      "194500.iterasyon, Train Loss = 0.24098067119722535, Val Loss = 11.222344521180453\n",
      "194600.iterasyon, Train Loss = 0.24096842995400983, Val Loss = 11.222500106834914\n",
      "194700.iterasyon, Train Loss = 0.24095620087544448, Val Loss = 11.222655622633711\n",
      "194800.iterasyon, Train Loss = 0.24094398394705607, Val Loss = 11.222811068602304\n",
      "194900.iterasyon, Train Loss = 0.24093177915439015, Val Loss = 11.222966444766262\n",
      "195000.iterasyon, Train Loss = 0.24091958648301157, Val Loss = 11.223121751151083\n",
      "195100.iterasyon, Train Loss = 0.24090740591849277, Val Loss = 11.223276987782308\n",
      "195200.iterasyon, Train Loss = 0.24089523744643407, Val Loss = 11.223432154685465\n",
      "195300.iterasyon, Train Loss = 0.2408830810524495, Val Loss = 11.223587251886048\n",
      "195400.iterasyon, Train Loss = 0.2408709367221651, Val Loss = 11.223742279409645\n",
      "195500.iterasyon, Train Loss = 0.24085880444123264, Val Loss = 11.223897237281747\n",
      "195600.iterasyon, Train Loss = 0.24084668419531285, Val Loss = 11.224052125527948\n",
      "195700.iterasyon, Train Loss = 0.24083457597008795, Val Loss = 11.224206944173739\n",
      "195800.iterasyon, Train Loss = 0.2408224797512556, Val Loss = 11.224361693244663\n",
      "195900.iterasyon, Train Loss = 0.24081039552453296, Val Loss = 11.224516372766274\n",
      "196000.iterasyon, Train Loss = 0.24079832327564799, Val Loss = 11.224670982764145\n",
      "196100.iterasyon, Train Loss = 0.2407862629903489, Val Loss = 11.22482552326381\n",
      "196200.iterasyon, Train Loss = 0.2407742146544072, Val Loss = 11.224979994290772\n",
      "196300.iterasyon, Train Loss = 0.24076217825359758, Val Loss = 11.225134395870633\n",
      "196400.iterasyon, Train Loss = 0.24075015377372674, Val Loss = 11.22528872802895\n",
      "196500.iterasyon, Train Loss = 0.24073814120060633, Val Loss = 11.225442990791237\n",
      "196600.iterasyon, Train Loss = 0.24072614052006766, Val Loss = 11.2255971841831\n",
      "196700.iterasyon, Train Loss = 0.24071415171796587, Val Loss = 11.22575130823004\n",
      "196800.iterasyon, Train Loss = 0.2407021747801634, Val Loss = 11.225905362957649\n",
      "196900.iterasyon, Train Loss = 0.2406902096925427, Val Loss = 11.226059348391484\n",
      "197000.iterasyon, Train Loss = 0.24067825644100738, Val Loss = 11.226213264557076\n",
      "197100.iterasyon, Train Loss = 0.24066631501146796, Val Loss = 11.226367111480053\n",
      "197200.iterasyon, Train Loss = 0.24065438538986328, Val Loss = 11.226520889185954\n",
      "197300.iterasyon, Train Loss = 0.2406424675621408, Val Loss = 11.226674597700312\n",
      "197400.iterasyon, Train Loss = 0.24063056151426562, Val Loss = 11.226828237048723\n",
      "197500.iterasyon, Train Loss = 0.2406186672322234, Val Loss = 11.226981807256774\n",
      "197600.iterasyon, Train Loss = 0.24060678470201227, Val Loss = 11.227135308350022\n",
      "197700.iterasyon, Train Loss = 0.2405949139096467, Val Loss = 11.22728874035405\n",
      "197800.iterasyon, Train Loss = 0.24058305484116238, Val Loss = 11.227442103294413\n",
      "197900.iterasyon, Train Loss = 0.24057120748260777, Val Loss = 11.227595397196696\n",
      "198000.iterasyon, Train Loss = 0.2405593718200475, Val Loss = 11.227748622086489\n",
      "198100.iterasyon, Train Loss = 0.24054754783956525, Val Loss = 11.227901777989349\n",
      "198200.iterasyon, Train Loss = 0.2405357355272564, Val Loss = 11.228054864930893\n",
      "198300.iterasyon, Train Loss = 0.24052393486924142, Val Loss = 11.22820788293666\n",
      "198400.iterasyon, Train Loss = 0.24051214585164812, Val Loss = 11.228360832032264\n",
      "198500.iterasyon, Train Loss = 0.2405003684606242, Val Loss = 11.228513712243306\n",
      "198600.iterasyon, Train Loss = 0.24048860268233688, Val Loss = 11.22866652359532\n",
      "198700.iterasyon, Train Loss = 0.2404768485029651, Val Loss = 11.2288192661139\n",
      "198800.iterasyon, Train Loss = 0.24046510590870684, Val Loss = 11.228971939824676\n",
      "198900.iterasyon, Train Loss = 0.24045337488577537, Val Loss = 11.229124544753196\n",
      "199000.iterasyon, Train Loss = 0.2404416554203991, Val Loss = 11.229277080925105\n",
      "199100.iterasyon, Train Loss = 0.24042994749882685, Val Loss = 11.229429548365934\n",
      "199200.iterasyon, Train Loss = 0.24041825110731707, Val Loss = 11.229581947101359\n",
      "199300.iterasyon, Train Loss = 0.24040656623215073, Val Loss = 11.229734277156927\n",
      "199400.iterasyon, Train Loss = 0.2403948928596227, Val Loss = 11.229886538558205\n",
      "199500.iterasyon, Train Loss = 0.24038323097604516, Val Loss = 11.230038731330833\n",
      "199600.iterasyon, Train Loss = 0.2403715805677423, Val Loss = 11.230190855500403\n",
      "199700.iterasyon, Train Loss = 0.2403599416210622, Val Loss = 11.230342911092466\n",
      "199800.iterasyon, Train Loss = 0.24034831412236088, Val Loss = 11.230494898132656\n",
      "199900.iterasyon, Train Loss = 0.24033669805801353, Val Loss = 11.230646816646626\n",
      "200000.iterasyon, Train Loss = 0.24032509341441324, Val Loss = 11.230798666659908\n",
      "200100.iterasyon, Train Loss = 0.24031350017797123, Val Loss = 11.230950448198167\n",
      "200200.iterasyon, Train Loss = 0.2403019183351064, Val Loss = 11.23110216128694\n",
      "200300.iterasyon, Train Loss = 0.24029034787226247, Val Loss = 11.23125380595187\n",
      "200400.iterasyon, Train Loss = 0.24027878877589556, Val Loss = 11.23140538221856\n",
      "200500.iterasyon, Train Loss = 0.24026724103247532, Val Loss = 11.231556890112632\n",
      "200600.iterasyon, Train Loss = 0.2402557046284921, Val Loss = 11.231708329659687\n",
      "200700.iterasyon, Train Loss = 0.24024417955044766, Val Loss = 11.231859700885328\n",
      "200800.iterasyon, Train Loss = 0.2402326657848672, Val Loss = 11.23201100381514\n",
      "200900.iterasyon, Train Loss = 0.24022116331828344, Val Loss = 11.232162238474787\n",
      "201000.iterasyon, Train Loss = 0.24020967213724742, Val Loss = 11.232313404889833\n",
      "201100.iterasyon, Train Loss = 0.24019819222833175, Val Loss = 11.232464503085929\n",
      "201200.iterasyon, Train Loss = 0.2401867235781138, Val Loss = 11.23261553308869\n",
      "201300.iterasyon, Train Loss = 0.24017526617320129, Val Loss = 11.232766494923712\n",
      "201400.iterasyon, Train Loss = 0.2401638200002034, Val Loss = 11.232917388616618\n",
      "201500.iterasyon, Train Loss = 0.24015238504575373, Val Loss = 11.23306821419301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201600.iterasyon, Train Loss = 0.24014096129650087, Val Loss = 11.233218971678527\n",
      "201700.iterasyon, Train Loss = 0.24012954873910616, Val Loss = 11.233369661098783\n",
      "201800.iterasyon, Train Loss = 0.24011814736025003, Val Loss = 11.233520282479434\n",
      "201900.iterasyon, Train Loss = 0.24010675714662608, Val Loss = 11.233670835846008\n",
      "202000.iterasyon, Train Loss = 0.24009537808494472, Val Loss = 11.233821321224195\n",
      "202100.iterasyon, Train Loss = 0.24008401016193504, Val Loss = 11.233971738639587\n",
      "202200.iterasyon, Train Loss = 0.24007265336433684, Val Loss = 11.234122088117797\n",
      "202300.iterasyon, Train Loss = 0.24006130767890665, Val Loss = 11.234272369684486\n",
      "202400.iterasyon, Train Loss = 0.24004997309241996, Val Loss = 11.234422583365282\n",
      "202500.iterasyon, Train Loss = 0.24003864959166413, Val Loss = 11.234572729185759\n",
      "202600.iterasyon, Train Loss = 0.2400273371634434, Val Loss = 11.23472280717157\n",
      "202700.iterasyon, Train Loss = 0.24001603579458075, Val Loss = 11.234872817348329\n",
      "202800.iterasyon, Train Loss = 0.24000474547190964, Val Loss = 11.235022759741668\n",
      "202900.iterasyon, Train Loss = 0.23999346618227987, Val Loss = 11.235172634377232\n",
      "203000.iterasyon, Train Loss = 0.23998219791256495, Val Loss = 11.235322441280587\n",
      "203100.iterasyon, Train Loss = 0.2399709406496406, Val Loss = 11.235472180477412\n",
      "203200.iterasyon, Train Loss = 0.23995969438040823, Val Loss = 11.23562185199331\n",
      "203300.iterasyon, Train Loss = 0.23994845909178078, Val Loss = 11.235771455853923\n",
      "203400.iterasyon, Train Loss = 0.23993723477068682, Val Loss = 11.235920992084907\n",
      "203500.iterasyon, Train Loss = 0.23992602140407115, Val Loss = 11.236070460711856\n",
      "203600.iterasyon, Train Loss = 0.2399148189788938, Val Loss = 11.236219861760398\n",
      "203700.iterasyon, Train Loss = 0.23990362748213132, Val Loss = 11.236369195256147\n",
      "203800.iterasyon, Train Loss = 0.23989244690077396, Val Loss = 11.236518461224772\n",
      "203900.iterasyon, Train Loss = 0.23988127722182717, Val Loss = 11.23666765969189\n",
      "204000.iterasyon, Train Loss = 0.23987011843231343, Val Loss = 11.236816790683124\n",
      "204100.iterasyon, Train Loss = 0.23985897051926955, Val Loss = 11.236965854224092\n",
      "204200.iterasyon, Train Loss = 0.23984783346975036, Val Loss = 11.237114850340399\n",
      "204300.iterasyon, Train Loss = 0.23983670727081696, Val Loss = 11.23726377905774\n",
      "204400.iterasyon, Train Loss = 0.2398255919095582, Val Loss = 11.23741264040173\n",
      "204500.iterasyon, Train Loss = 0.23981448737307196, Val Loss = 11.23756143439799\n",
      "204600.iterasyon, Train Loss = 0.23980339364847036, Val Loss = 11.237710161072151\n",
      "204700.iterasyon, Train Loss = 0.23979231072288232, Val Loss = 11.237858820449839\n",
      "204800.iterasyon, Train Loss = 0.2397812385834528, Val Loss = 11.238007412556698\n",
      "204900.iterasyon, Train Loss = 0.2397701772173405, Val Loss = 11.238155937418338\n",
      "205000.iterasyon, Train Loss = 0.23975912661171983, Val Loss = 11.238304395060426\n",
      "205100.iterasyon, Train Loss = 0.23974808675377915, Val Loss = 11.238452785508608\n",
      "205200.iterasyon, Train Loss = 0.23973705763072764, Val Loss = 11.238601108788462\n",
      "205300.iterasyon, Train Loss = 0.23972603922978244, Val Loss = 11.2387493649256\n",
      "205400.iterasyon, Train Loss = 0.23971503153817866, Val Loss = 11.23889755394572\n",
      "205500.iterasyon, Train Loss = 0.23970403454316555, Val Loss = 11.239045675874463\n",
      "205600.iterasyon, Train Loss = 0.23969304823201074, Val Loss = 11.239193730737416\n",
      "205700.iterasyon, Train Loss = 0.2396820725919945, Val Loss = 11.239341718560224\n",
      "205800.iterasyon, Train Loss = 0.23967110761041174, Val Loss = 11.239489639368538\n",
      "205900.iterasyon, Train Loss = 0.23966015327457127, Val Loss = 11.239637493187994\n",
      "206000.iterasyon, Train Loss = 0.23964920957180147, Val Loss = 11.23978528004418\n",
      "206100.iterasyon, Train Loss = 0.2396382764894425, Val Loss = 11.23993299996279\n",
      "206200.iterasyon, Train Loss = 0.23962735401484905, Val Loss = 11.24008065296939\n",
      "206300.iterasyon, Train Loss = 0.23961644213539265, Val Loss = 11.240228239089642\n",
      "206400.iterasyon, Train Loss = 0.23960554083845856, Val Loss = 11.240375758349208\n",
      "206500.iterasyon, Train Loss = 0.23959465011144562, Val Loss = 11.240523210773654\n",
      "206600.iterasyon, Train Loss = 0.2395837699417709, Val Loss = 11.240670596388707\n",
      "206700.iterasyon, Train Loss = 0.23957290031686462, Val Loss = 11.240817915219889\n",
      "206800.iterasyon, Train Loss = 0.23956204122417366, Val Loss = 11.240965167292929\n",
      "206900.iterasyon, Train Loss = 0.2395511926511532, Val Loss = 11.241112352633433\n",
      "207000.iterasyon, Train Loss = 0.23954035458528308, Val Loss = 11.241259471266995\n",
      "207100.iterasyon, Train Loss = 0.2395295270140525, Val Loss = 11.241406523219245\n",
      "207200.iterasyon, Train Loss = 0.2395187099249624, Val Loss = 11.24155350851587\n",
      "207300.iterasyon, Train Loss = 0.23950790330553606, Val Loss = 11.241700427182447\n",
      "207400.iterasyon, Train Loss = 0.23949710714330322, Val Loss = 11.241847279244674\n",
      "207500.iterasyon, Train Loss = 0.23948632142581697, Val Loss = 11.24199406472811\n",
      "207600.iterasyon, Train Loss = 0.23947554614064204, Val Loss = 11.24214078365839\n",
      "207700.iterasyon, Train Loss = 0.23946478127535287, Val Loss = 11.24228743606117\n",
      "207800.iterasyon, Train Loss = 0.2394540268175425, Val Loss = 11.242434021962131\n",
      "207900.iterasyon, Train Loss = 0.23944328275482196, Val Loss = 11.24258054138681\n",
      "208000.iterasyon, Train Loss = 0.23943254907481218, Val Loss = 11.242726994360853\n",
      "208100.iterasyon, Train Loss = 0.23942182576514995, Val Loss = 11.242873380909929\n",
      "208200.iterasyon, Train Loss = 0.23941111281348645, Val Loss = 11.24301970105962\n",
      "208300.iterasyon, Train Loss = 0.23940041020749106, Val Loss = 11.243165954835579\n",
      "208400.iterasyon, Train Loss = 0.23938971793484085, Val Loss = 11.243312142263449\n",
      "208500.iterasyon, Train Loss = 0.23937903598323618, Val Loss = 11.243458263368817\n",
      "208600.iterasyon, Train Loss = 0.23936836434038233, Val Loss = 11.243604318177352\n",
      "208700.iterasyon, Train Loss = 0.23935770299400605, Val Loss = 11.243750306714634\n",
      "208800.iterasyon, Train Loss = 0.23934705193184821, Val Loss = 11.243896229006316\n",
      "208900.iterasyon, Train Loss = 0.23933641114165882, Val Loss = 11.244042085078014\n",
      "209000.iterasyon, Train Loss = 0.23932578061121004, Val Loss = 11.244187874955383\n",
      "209100.iterasyon, Train Loss = 0.23931516032828282, Val Loss = 11.244333598664005\n",
      "209200.iterasyon, Train Loss = 0.23930455028067585, Val Loss = 11.244479256229525\n",
      "209300.iterasyon, Train Loss = 0.23929395045619714, Val Loss = 11.244624847677567\n",
      "209400.iterasyon, Train Loss = 0.23928336084267712, Val Loss = 11.244770373033756\n",
      "209500.iterasyon, Train Loss = 0.23927278142795383, Val Loss = 11.244915832323677\n",
      "209600.iterasyon, Train Loss = 0.23926221219988367, Val Loss = 11.245061225572977\n",
      "209700.iterasyon, Train Loss = 0.23925165314633437, Val Loss = 11.24520655280729\n",
      "209800.iterasyon, Train Loss = 0.2392411042551933, Val Loss = 11.2453518140522\n",
      "209900.iterasyon, Train Loss = 0.2392305655143561, Val Loss = 11.245497009333299\n",
      "210000.iterasyon, Train Loss = 0.23922003691173419, Val Loss = 11.245642138676303\n",
      "210100.iterasyon, Train Loss = 0.2392095184352565, Val Loss = 11.24578720210674\n",
      "210200.iterasyon, Train Loss = 0.23919901007286223, Val Loss = 11.245932199650266\n",
      "210300.iterasyon, Train Loss = 0.2391885118125078, Val Loss = 11.246077131332525\n",
      "210400.iterasyon, Train Loss = 0.23917802364216081, Val Loss = 11.246221997179077\n",
      "210500.iterasyon, Train Loss = 0.23916754554980782, Val Loss = 11.246366797215588\n",
      "210600.iterasyon, Train Loss = 0.23915707752344553, Val Loss = 11.246511531467638\n",
      "210700.iterasyon, Train Loss = 0.23914661955108793, Val Loss = 11.246656199960832\n",
      "210800.iterasyon, Train Loss = 0.23913617162076153, Val Loss = 11.246800802720774\n",
      "210900.iterasyon, Train Loss = 0.2391257337205047, Val Loss = 11.246945339773104\n",
      "211000.iterasyon, Train Loss = 0.23911530583837212, Val Loss = 11.247089811143418\n",
      "211100.iterasyon, Train Loss = 0.2391048879624346, Val Loss = 11.24723421685734\n",
      "211200.iterasyon, Train Loss = 0.23909448008077763, Val Loss = 11.24737855694042\n",
      "211300.iterasyon, Train Loss = 0.23908408218149205, Val Loss = 11.247522831418353\n",
      "211400.iterasyon, Train Loss = 0.239073694252696, Val Loss = 11.247667040316717\n",
      "211500.iterasyon, Train Loss = 0.23906331628251087, Val Loss = 11.247811183661065\n",
      "211600.iterasyon, Train Loss = 0.23905294825907955, Val Loss = 11.24795526147703\n",
      "211700.iterasyon, Train Loss = 0.23904259017054993, Val Loss = 11.248099273790244\n",
      "211800.iterasyon, Train Loss = 0.23903224200509346, Val Loss = 11.248243220626327\n",
      "211900.iterasyon, Train Loss = 0.23902190375089194, Val Loss = 11.248387102010845\n",
      "212000.iterasyon, Train Loss = 0.23901157539614173, Val Loss = 11.248530917969358\n",
      "212100.iterasyon, Train Loss = 0.23900125692905197, Val Loss = 11.248674668527476\n",
      "212200.iterasyon, Train Loss = 0.23899094833784365, Val Loss = 11.248818353710886\n",
      "212300.iterasyon, Train Loss = 0.2389806496107584, Val Loss = 11.2489619735451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212400.iterasyon, Train Loss = 0.23897036073604327, Val Loss = 11.24910552805575\n",
      "212500.iterasyon, Train Loss = 0.23896008170196814, Val Loss = 11.249249017268419\n",
      "212600.iterasyon, Train Loss = 0.2389498124968091, Val Loss = 11.249392441208705\n",
      "212700.iterasyon, Train Loss = 0.23893955310885953, Val Loss = 11.249535799902183\n",
      "212800.iterasyon, Train Loss = 0.23892930352643219, Val Loss = 11.249679093374485\n",
      "212900.iterasyon, Train Loss = 0.23891906373783922, Val Loss = 11.249822321651148\n",
      "213000.iterasyon, Train Loss = 0.23890883373141933, Val Loss = 11.249965484757796\n",
      "213100.iterasyon, Train Loss = 0.2388986134955248, Val Loss = 11.250108582719983\n",
      "213200.iterasyon, Train Loss = 0.2388884030185109, Val Loss = 11.250251615563398\n",
      "213300.iterasyon, Train Loss = 0.23887820228875944, Val Loss = 11.250394583313525\n",
      "213400.iterasyon, Train Loss = 0.23886801129465665, Val Loss = 11.250537485995977\n",
      "213500.iterasyon, Train Loss = 0.23885783002461117, Val Loss = 11.250680323636342\n",
      "213600.iterasyon, Train Loss = 0.23884765846703668, Val Loss = 11.250823096260184\n",
      "213700.iterasyon, Train Loss = 0.23883749661036288, Val Loss = 11.250965803893125\n",
      "213800.iterasyon, Train Loss = 0.23882734444303924, Val Loss = 11.251108446560687\n",
      "213900.iterasyon, Train Loss = 0.23881720195352105, Val Loss = 11.251251024288495\n",
      "214000.iterasyon, Train Loss = 0.2388070691302834, Val Loss = 11.251393537102077\n",
      "214100.iterasyon, Train Loss = 0.23879694596180742, Val Loss = 11.251535985027076\n",
      "214200.iterasyon, Train Loss = 0.23878683243659818, Val Loss = 11.251678368089028\n",
      "214300.iterasyon, Train Loss = 0.23877672854316578, Val Loss = 11.251820686313533\n",
      "214400.iterasyon, Train Loss = 0.23876663427003877, Val Loss = 11.251962939726077\n",
      "214500.iterasyon, Train Loss = 0.2387565496057559, Val Loss = 11.25210512835232\n",
      "214600.iterasyon, Train Loss = 0.2387464745388703, Val Loss = 11.252247252217849\n",
      "214700.iterasyon, Train Loss = 0.2387364090579536, Val Loss = 11.252389311348136\n",
      "214800.iterasyon, Train Loss = 0.23872635315158364, Val Loss = 11.2525313057688\n",
      "214900.iterasyon, Train Loss = 0.2387163068083563, Val Loss = 11.2526732355054\n",
      "215000.iterasyon, Train Loss = 0.23870627001687925, Val Loss = 11.252815100583529\n",
      "215100.iterasyon, Train Loss = 0.23869624276577284, Val Loss = 11.2529569010287\n",
      "215200.iterasyon, Train Loss = 0.23868622504367598, Val Loss = 11.25309863686647\n",
      "215300.iterasyon, Train Loss = 0.23867621683923343, Val Loss = 11.253240308122441\n",
      "215400.iterasyon, Train Loss = 0.2386662181411101, Val Loss = 11.253381914822125\n",
      "215500.iterasyon, Train Loss = 0.23865622893797936, Val Loss = 11.253523456991118\n",
      "215600.iterasyon, Train Loss = 0.23864624921853259, Val Loss = 11.253664934654939\n",
      "215700.iterasyon, Train Loss = 0.23863627897146852, Val Loss = 11.253806347839168\n",
      "215800.iterasyon, Train Loss = 0.2386263181855065, Val Loss = 11.253947696569341\n",
      "215900.iterasyon, Train Loss = 0.23861636684937246, Val Loss = 11.254088980871025\n",
      "216000.iterasyon, Train Loss = 0.23860642495181258, Val Loss = 11.25423020076972\n",
      "216100.iterasyon, Train Loss = 0.23859649248158019, Val Loss = 11.254371356291028\n",
      "216200.iterasyon, Train Loss = 0.2385865694274454, Val Loss = 11.254512447460469\n",
      "216300.iterasyon, Train Loss = 0.23857665577818887, Val Loss = 11.25465347430362\n",
      "216400.iterasyon, Train Loss = 0.23856675152260878, Val Loss = 11.25479443684595\n",
      "216500.iterasyon, Train Loss = 0.23855685664951157, Val Loss = 11.254935335113064\n",
      "216600.iterasyon, Train Loss = 0.2385469711477222, Val Loss = 11.255076169130442\n",
      "216700.iterasyon, Train Loss = 0.23853709500607637, Val Loss = 11.25521693892366\n",
      "216800.iterasyon, Train Loss = 0.2385272282134194, Val Loss = 11.25535764451826\n",
      "216900.iterasyon, Train Loss = 0.23851737075861643, Val Loss = 11.255498285939755\n",
      "217000.iterasyon, Train Loss = 0.23850752263054062, Val Loss = 11.255638863213653\n",
      "217100.iterasyon, Train Loss = 0.23849768381808314, Val Loss = 11.255779376365506\n",
      "217200.iterasyon, Train Loss = 0.23848785431014385, Val Loss = 11.255919825420822\n",
      "217300.iterasyon, Train Loss = 0.23847803409563684, Val Loss = 11.256060210405161\n",
      "217400.iterasyon, Train Loss = 0.2384682231634907, Val Loss = 11.256200531344025\n",
      "217500.iterasyon, Train Loss = 0.23845842150264604, Val Loss = 11.256340788262964\n",
      "217600.iterasyon, Train Loss = 0.2384486291020577, Val Loss = 11.25648098118747\n",
      "217700.iterasyon, Train Loss = 0.2384388459506943, Val Loss = 11.256621110143032\n",
      "217800.iterasyon, Train Loss = 0.23842907203753014, Val Loss = 11.256761175155242\n",
      "217900.iterasyon, Train Loss = 0.23841930735156414, Val Loss = 11.256901176249555\n",
      "218000.iterasyon, Train Loss = 0.23840955188180404, Val Loss = 11.257041113451486\n",
      "218100.iterasyon, Train Loss = 0.2383998056172639, Val Loss = 11.257180986786572\n",
      "218200.iterasyon, Train Loss = 0.23839006854697764, Val Loss = 11.257320796280299\n",
      "218300.iterasyon, Train Loss = 0.2383803406599934, Val Loss = 11.257460541958192\n",
      "218400.iterasyon, Train Loss = 0.2383706219453685, Val Loss = 11.257600223845731\n",
      "218500.iterasyon, Train Loss = 0.2383609123921744, Val Loss = 11.257739841968425\n",
      "218600.iterasyon, Train Loss = 0.2383512119894905, Val Loss = 11.25787939635182\n",
      "218700.iterasyon, Train Loss = 0.23834152072642092, Val Loss = 11.25801888702138\n",
      "218800.iterasyon, Train Loss = 0.2383318385920733, Val Loss = 11.25815831400258\n",
      "218900.iterasyon, Train Loss = 0.23832216557557037, Val Loss = 11.258297677320936\n",
      "219000.iterasyon, Train Loss = 0.23831250166604798, Val Loss = 11.258436977001953\n",
      "219100.iterasyon, Train Loss = 0.23830284685265646, Val Loss = 11.258576213071091\n",
      "219200.iterasyon, Train Loss = 0.23829320112455835, Val Loss = 11.25871538555383\n",
      "219300.iterasyon, Train Loss = 0.23828356447092344, Val Loss = 11.258854494475692\n",
      "219400.iterasyon, Train Loss = 0.238273936880945, Val Loss = 11.25899353986212\n",
      "219500.iterasyon, Train Loss = 0.23826431834382025, Val Loss = 11.259132521738636\n",
      "219600.iterasyon, Train Loss = 0.2382547088487632, Val Loss = 11.25927144013068\n",
      "219700.iterasyon, Train Loss = 0.2382451083849977, Val Loss = 11.259410295063796\n",
      "219800.iterasyon, Train Loss = 0.23823551694176404, Val Loss = 11.259549086563426\n",
      "219900.iterasyon, Train Loss = 0.23822593450831486, Val Loss = 11.259687814655006\n",
      "220000.iterasyon, Train Loss = 0.2382163610739134, Val Loss = 11.259826479364046\n",
      "220100.iterasyon, Train Loss = 0.23820679662783392, Val Loss = 11.259965080716015\n",
      "220200.iterasyon, Train Loss = 0.23819724115936916, Val Loss = 11.260103618736375\n",
      "220300.iterasyon, Train Loss = 0.2381876946578202, Val Loss = 11.26024209345059\n",
      "220400.iterasyon, Train Loss = 0.2381781571125018, Val Loss = 11.260380504884127\n",
      "220500.iterasyon, Train Loss = 0.23816862851274176, Val Loss = 11.260518853062447\n",
      "220600.iterasyon, Train Loss = 0.2381591088478805, Val Loss = 11.260657138010993\n",
      "220700.iterasyon, Train Loss = 0.23814959810727043, Val Loss = 11.260795359755198\n",
      "220800.iterasyon, Train Loss = 0.23814009628028163, Val Loss = 11.260933518320579\n",
      "220900.iterasyon, Train Loss = 0.23813060335628414, Val Loss = 11.261071613732552\n",
      "221000.iterasyon, Train Loss = 0.23812111932467336, Val Loss = 11.261209646016566\n",
      "221100.iterasyon, Train Loss = 0.23811164417485353, Val Loss = 11.261347615198062\n",
      "221200.iterasyon, Train Loss = 0.23810217789623758, Val Loss = 11.261485521302486\n",
      "221300.iterasyon, Train Loss = 0.23809272047825694, Val Loss = 11.261623364355296\n",
      "221400.iterasyon, Train Loss = 0.23808327191035192, Val Loss = 11.261761144381927\n",
      "221500.iterasyon, Train Loss = 0.23807383218197428, Val Loss = 11.261898861407811\n",
      "221600.iterasyon, Train Loss = 0.2380644012825913, Val Loss = 11.262036515458371\n",
      "221700.iterasyon, Train Loss = 0.23805497920168125, Val Loss = 11.262174106559057\n",
      "221800.iterasyon, Train Loss = 0.23804556592873682, Val Loss = 11.262311634735282\n",
      "221900.iterasyon, Train Loss = 0.23803616145325582, Val Loss = 11.262449100012484\n",
      "222000.iterasyon, Train Loss = 0.2380267657647623, Val Loss = 11.262586502416067\n",
      "222100.iterasyon, Train Loss = 0.2380173788527782, Val Loss = 11.26272384197152\n",
      "222200.iterasyon, Train Loss = 0.23800800070684794, Val Loss = 11.26286111870416\n",
      "222300.iterasyon, Train Loss = 0.23799863131652219, Val Loss = 11.262998332639482\n",
      "222400.iterasyon, Train Loss = 0.23798927067136885, Val Loss = 11.263135483802882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222500.iterasyon, Train Loss = 0.23797991876096258, Val Loss = 11.263272572219801\n",
      "222600.iterasyon, Train Loss = 0.23797057557489817, Val Loss = 11.263409597915574\n",
      "222700.iterasyon, Train Loss = 0.2379612411027738, Val Loss = 11.26354656091566\n",
      "222800.iterasyon, Train Loss = 0.23795191533420537, Val Loss = 11.26368346124549\n",
      "222900.iterasyon, Train Loss = 0.23794259825882366, Val Loss = 11.263820298930389\n",
      "223000.iterasyon, Train Loss = 0.23793328986626475, Val Loss = 11.263957073995819\n",
      "223100.iterasyon, Train Loss = 0.23792399014618268, Val Loss = 11.264093786467177\n",
      "223200.iterasyon, Train Loss = 0.2379146990882364, Val Loss = 11.264230436369862\n",
      "223300.iterasyon, Train Loss = 0.23790541668210796, Val Loss = 11.264367023729223\n",
      "223400.iterasyon, Train Loss = 0.2378961429174861, Val Loss = 11.26450354857067\n",
      "223500.iterasyon, Train Loss = 0.23788687778406856, Val Loss = 11.264640010919592\n",
      "223600.iterasyon, Train Loss = 0.2378776212715701, Val Loss = 11.264776410801367\n",
      "223700.iterasyon, Train Loss = 0.237868373369716, Val Loss = 11.264912748241404\n",
      "223800.iterasyon, Train Loss = 0.23785913406824233, Val Loss = 11.26504902326505\n",
      "223900.iterasyon, Train Loss = 0.23784990335690004, Val Loss = 11.265185235897697\n",
      "224000.iterasyon, Train Loss = 0.23784068122545057, Val Loss = 11.265321386164699\n",
      "224100.iterasyon, Train Loss = 0.23783146766366872, Val Loss = 11.265457474091429\n",
      "224200.iterasyon, Train Loss = 0.23782226266133957, Val Loss = 11.265593499703254\n",
      "224300.iterasyon, Train Loss = 0.23781306620826279, Val Loss = 11.265729463025597\n",
      "224400.iterasyon, Train Loss = 0.23780387829424335, Val Loss = 11.265865364083758\n",
      "224500.iterasyon, Train Loss = 0.237794698909112, Val Loss = 11.266001202903126\n",
      "224600.iterasyon, Train Loss = 0.2377855280426987, Val Loss = 11.266136979509024\n",
      "224700.iterasyon, Train Loss = 0.2377763656848472, Val Loss = 11.266272693926842\n",
      "224800.iterasyon, Train Loss = 0.2377672118254185, Val Loss = 11.266408346181953\n",
      "224900.iterasyon, Train Loss = 0.23775806645428677, Val Loss = 11.266543936299643\n",
      "225000.iterasyon, Train Loss = 0.23774892956132887, Val Loss = 11.266679464305302\n",
      "225100.iterasyon, Train Loss = 0.23773980113644474, Val Loss = 11.266814930224228\n",
      "225200.iterasyon, Train Loss = 0.23773068116953663, Val Loss = 11.266950334081821\n",
      "225300.iterasyon, Train Loss = 0.2377215696505249, Val Loss = 11.267085675903372\n",
      "225400.iterasyon, Train Loss = 0.2377124665693382, Val Loss = 11.267220955714281\n",
      "225500.iterasyon, Train Loss = 0.23770337191592308, Val Loss = 11.26735617353977\n",
      "225600.iterasyon, Train Loss = 0.2376942856802309, Val Loss = 11.267491329405258\n",
      "225700.iterasyon, Train Loss = 0.23768520785222755, Val Loss = 11.26762642333603\n",
      "225800.iterasyon, Train Loss = 0.23767613842189336, Val Loss = 11.267761455357444\n",
      "225900.iterasyon, Train Loss = 0.23766707737921647, Val Loss = 11.267896425494792\n",
      "226000.iterasyon, Train Loss = 0.23765802471419947, Val Loss = 11.26803133377341\n",
      "226100.iterasyon, Train Loss = 0.23764898041685714, Val Loss = 11.26816618021858\n",
      "226200.iterasyon, Train Loss = 0.23763994447721562, Val Loss = 11.268300964855639\n",
      "226300.iterasyon, Train Loss = 0.237630916885307, Val Loss = 11.268435687709921\n",
      "226400.iterasyon, Train Loss = 0.23762189763118868, Val Loss = 11.268570348806646\n",
      "226500.iterasyon, Train Loss = 0.23761288670491665, Val Loss = 11.26870494817122\n",
      "226600.iterasyon, Train Loss = 0.2376038840965673, Val Loss = 11.268839485828854\n",
      "226700.iterasyon, Train Loss = 0.23759488979621912, Val Loss = 11.268973961804896\n",
      "226800.iterasyon, Train Loss = 0.2375859037939753, Val Loss = 11.269108376124649\n",
      "226900.iterasyon, Train Loss = 0.23757692607994144, Val Loss = 11.269242728813365\n",
      "227000.iterasyon, Train Loss = 0.2375679566442359, Val Loss = 11.26937701989636\n",
      "227100.iterasyon, Train Loss = 0.2375589954769927, Val Loss = 11.269511249398878\n",
      "227200.iterasyon, Train Loss = 0.23755004256835663, Val Loss = 11.269645417346224\n",
      "227300.iterasyon, Train Loss = 0.2375410979084783, Val Loss = 11.269779523763669\n",
      "227400.iterasyon, Train Loss = 0.23753216148752698, Val Loss = 11.269913568676536\n",
      "227500.iterasyon, Train Loss = 0.23752323329568079, Val Loss = 11.270047552110036\n",
      "227600.iterasyon, Train Loss = 0.23751431332312897, Val Loss = 11.27018147408947\n",
      "227700.iterasyon, Train Loss = 0.23750540156007505, Val Loss = 11.270315334640092\n",
      "227800.iterasyon, Train Loss = 0.23749649799673087, Val Loss = 11.270449133787187\n",
      "227900.iterasyon, Train Loss = 0.23748760262332325, Val Loss = 11.270582871555987\n",
      "228000.iterasyon, Train Loss = 0.237478715430088, Val Loss = 11.270716547971746\n",
      "228100.iterasyon, Train Loss = 0.23746983640726946, Val Loss = 11.270850163059745\n",
      "228200.iterasyon, Train Loss = 0.23746096554513374, Val Loss = 11.270983716845175\n",
      "228300.iterasyon, Train Loss = 0.23745210283394635, Val Loss = 11.271117209353358\n",
      "228400.iterasyon, Train Loss = 0.23744324826399535, Val Loss = 11.271250640609479\n",
      "228500.iterasyon, Train Loss = 0.23743440182557188, Val Loss = 11.271384010638817\n",
      "228600.iterasyon, Train Loss = 0.23742556350898195, Val Loss = 11.27151731946659\n",
      "228700.iterasyon, Train Loss = 0.2374167333045426, Val Loss = 11.271650567118037\n",
      "228800.iterasyon, Train Loss = 0.23740791120258542, Val Loss = 11.271783753618406\n",
      "228900.iterasyon, Train Loss = 0.2373990971934449, Val Loss = 11.271916878992867\n",
      "229000.iterasyon, Train Loss = 0.23739029126748054, Val Loss = 11.272049943266667\n",
      "229100.iterasyon, Train Loss = 0.237381493415049, Val Loss = 11.272182946465103\n",
      "229200.iterasyon, Train Loss = 0.23737270362653004, Val Loss = 11.27231588861328\n",
      "229300.iterasyon, Train Loss = 0.23736392189230704, Val Loss = 11.27244876973647\n",
      "229400.iterasyon, Train Loss = 0.23735514820277812, Val Loss = 11.27258158985987\n",
      "229500.iterasyon, Train Loss = 0.2373463825483525, Val Loss = 11.272714349008663\n",
      "229600.iterasyon, Train Loss = 0.23733762491944668, Val Loss = 11.272847047208131\n",
      "229700.iterasyon, Train Loss = 0.23732887530649735, Val Loss = 11.272979684483412\n",
      "229800.iterasyon, Train Loss = 0.23732013369994506, Val Loss = 11.273112260859703\n",
      "229900.iterasyon, Train Loss = 0.2373114000902445, Val Loss = 11.27324477636223\n",
      "230000.iterasyon, Train Loss = 0.23730267446786468, Val Loss = 11.273377231016152\n",
      "230100.iterasyon, Train Loss = 0.23729395682327578, Val Loss = 11.273509624846652\n",
      "230200.iterasyon, Train Loss = 0.23728524714697205, Val Loss = 11.273641957878942\n",
      "230300.iterasyon, Train Loss = 0.23727654542944943, Val Loss = 11.273774230138198\n",
      "230400.iterasyon, Train Loss = 0.23726785166122022, Val Loss = 11.27390644164958\n",
      "230500.iterasyon, Train Loss = 0.2372591658328076, Val Loss = 11.274038592438245\n",
      "230600.iterasyon, Train Loss = 0.23725048793474246, Val Loss = 11.27417068252942\n",
      "230700.iterasyon, Train Loss = 0.23724181795757024, Val Loss = 11.274302711948241\n",
      "230800.iterasyon, Train Loss = 0.2372331558918502, Val Loss = 11.274434680719851\n",
      "230900.iterasyon, Train Loss = 0.23722450172814502, Val Loss = 11.274566588869417\n",
      "231000.iterasyon, Train Loss = 0.23721585545703397, Val Loss = 11.274698436422106\n",
      "231100.iterasyon, Train Loss = 0.23720721706910755, Val Loss = 11.274830223403077\n",
      "231200.iterasyon, Train Loss = 0.23719858655496417, Val Loss = 11.274961949837488\n",
      "231300.iterasyon, Train Loss = 0.23718996390521688, Val Loss = 11.275093615750471\n",
      "231400.iterasyon, Train Loss = 0.23718134911049216, Val Loss = 11.275225221167121\n",
      "231500.iterasyon, Train Loss = 0.2371727421614173, Val Loss = 11.275356766112642\n",
      "231600.iterasyon, Train Loss = 0.23716414304864156, Val Loss = 11.27548825061211\n",
      "231700.iterasyon, Train Loss = 0.237155551762821, Val Loss = 11.27561967469071\n",
      "231800.iterasyon, Train Loss = 0.23714696829462104, Val Loss = 11.27575103837356\n",
      "231900.iterasyon, Train Loss = 0.23713839263472344, Val Loss = 11.275882341685735\n",
      "232000.iterasyon, Train Loss = 0.23712982477381306, Val Loss = 11.276013584652425\n",
      "232100.iterasyon, Train Loss = 0.2371212647025939, Val Loss = 11.276144767298705\n",
      "232200.iterasyon, Train Loss = 0.23711271241177598, Val Loss = 11.276275889649687\n",
      "232300.iterasyon, Train Loss = 0.23710416789208258, Val Loss = 11.276406951730511\n",
      "232400.iterasyon, Train Loss = 0.23709563113424578, Val Loss = 11.276537953566242\n",
      "232500.iterasyon, Train Loss = 0.23708710212901443, Val Loss = 11.276668895182013\n",
      "232600.iterasyon, Train Loss = 0.23707858086713818, Val Loss = 11.276799776602914\n",
      "232700.iterasyon, Train Loss = 0.23707006733938948, Val Loss = 11.276930597854008\n",
      "232800.iterasyon, Train Loss = 0.2370615615365397, Val Loss = 11.277061358960452\n",
      "232900.iterasyon, Train Loss = 0.2370530634493819, Val Loss = 11.277192059947259\n",
      "233000.iterasyon, Train Loss = 0.2370445730687136, Val Loss = 11.277322700839552\n",
      "233100.iterasyon, Train Loss = 0.23703609038534557, Val Loss = 11.277453281662417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233200.iterasyon, Train Loss = 0.2370276153900984, Val Loss = 11.277583802440937\n",
      "233300.iterasyon, Train Loss = 0.2370191480738072, Val Loss = 11.27771426320013\n",
      "233400.iterasyon, Train Loss = 0.2370106884273101, Val Loss = 11.277844663965146\n",
      "233500.iterasyon, Train Loss = 0.2370022364414641, Val Loss = 11.277975004761005\n",
      "233600.iterasyon, Train Loss = 0.23699379210713278, Val Loss = 11.278105285612751\n",
      "233700.iterasyon, Train Loss = 0.23698535541519206, Val Loss = 11.278235506545434\n",
      "233800.iterasyon, Train Loss = 0.23697692635652934, Val Loss = 11.278365667584179\n",
      "233900.iterasyon, Train Loss = 0.23696850492204113, Val Loss = 11.278495768753928\n",
      "234000.iterasyon, Train Loss = 0.2369600911026332, Val Loss = 11.278625810079832\n",
      "234100.iterasyon, Train Loss = 0.2369516848892288, Val Loss = 11.278755791586846\n",
      "234200.iterasyon, Train Loss = 0.23694328627275385, Val Loss = 11.278885713300077\n",
      "234300.iterasyon, Train Loss = 0.23693489524415043, Val Loss = 11.279015575244527\n",
      "234400.iterasyon, Train Loss = 0.23692651179437033, Val Loss = 11.27914537744522\n",
      "234500.iterasyon, Train Loss = 0.23691813591437363, Val Loss = 11.279275119927183\n",
      "234600.iterasyon, Train Loss = 0.23690976759513274, Val Loss = 11.279404802715485\n",
      "234700.iterasyon, Train Loss = 0.23690140682763308, Val Loss = 11.279534425835074\n",
      "234800.iterasyon, Train Loss = 0.23689305360286858, Val Loss = 11.279663989310984\n",
      "234900.iterasyon, Train Loss = 0.23688470791184138, Val Loss = 11.279793493168253\n",
      "235000.iterasyon, Train Loss = 0.23687636974557005, Val Loss = 11.27992293743187\n",
      "235100.iterasyon, Train Loss = 0.23686803909507578, Val Loss = 11.280052322126863\n",
      "235200.iterasyon, Train Loss = 0.23685971595139899, Val Loss = 11.280181647278233\n",
      "235300.iterasyon, Train Loss = 0.23685140030558838, Val Loss = 11.280310912910913\n",
      "235400.iterasyon, Train Loss = 0.23684309214870156, Val Loss = 11.28044011904992\n",
      "235500.iterasyon, Train Loss = 0.23683479147180234, Val Loss = 11.28056926572029\n",
      "235600.iterasyon, Train Loss = 0.2368264982659762, Val Loss = 11.280698352946947\n",
      "235700.iterasyon, Train Loss = 0.23681821252230764, Val Loss = 11.280827380754895\n",
      "235800.iterasyon, Train Loss = 0.23680993423189986, Val Loss = 11.28095634916911\n",
      "235900.iterasyon, Train Loss = 0.23680166338586306, Val Loss = 11.281085258214576\n",
      "236000.iterasyon, Train Loss = 0.23679339997532076, Val Loss = 11.281214107916218\n",
      "236100.iterasyon, Train Loss = 0.23678514399140427, Val Loss = 11.281342898299012\n",
      "236200.iterasyon, Train Loss = 0.23677689542525263, Val Loss = 11.28147162938796\n",
      "236300.iterasyon, Train Loss = 0.23676865426802426, Val Loss = 11.28160030120796\n",
      "236400.iterasyon, Train Loss = 0.2367604205108771, Val Loss = 11.281728913784008\n",
      "236500.iterasyon, Train Loss = 0.23675219414499163, Val Loss = 11.281857467141009\n",
      "236600.iterasyon, Train Loss = 0.23674397516154497, Val Loss = 11.281985961303953\n",
      "236700.iterasyon, Train Loss = 0.23673576355174, Val Loss = 11.282114396297757\n",
      "236800.iterasyon, Train Loss = 0.23672755930677683, Val Loss = 11.282242772147326\n",
      "236900.iterasyon, Train Loss = 0.2367193624178745, Val Loss = 11.28237108887765\n",
      "237000.iterasyon, Train Loss = 0.236711172876256, Val Loss = 11.282499346513585\n",
      "237100.iterasyon, Train Loss = 0.23670299067316103, Val Loss = 11.282627545080118\n",
      "237200.iterasyon, Train Loss = 0.23669481579983434, Val Loss = 11.282755684602133\n",
      "237300.iterasyon, Train Loss = 0.23668664824753619, Val Loss = 11.282883765104575\n",
      "237400.iterasyon, Train Loss = 0.23667848800753355, Val Loss = 11.28301178661229\n",
      "237500.iterasyon, Train Loss = 0.23667033507110471, Val Loss = 11.283139749150239\n",
      "237600.iterasyon, Train Loss = 0.2366621894295378, Val Loss = 11.283267652743309\n",
      "237700.iterasyon, Train Loss = 0.23665405107413232, Val Loss = 11.283395497416405\n",
      "237800.iterasyon, Train Loss = 0.23664591999619558, Val Loss = 11.283523283194429\n",
      "237900.iterasyon, Train Loss = 0.23663779618705438, Val Loss = 11.283651010102169\n",
      "238000.iterasyon, Train Loss = 0.23662967963802997, Val Loss = 11.283778678164659\n",
      "238100.iterasyon, Train Loss = 0.23662157034046757, Val Loss = 11.283906287406673\n",
      "238200.iterasyon, Train Loss = 0.23661346828571586, Val Loss = 11.284033837853157\n",
      "238300.iterasyon, Train Loss = 0.23660537346513844, Val Loss = 11.284161329528915\n",
      "238400.iterasyon, Train Loss = 0.23659728587010279, Val Loss = 11.28428876245888\n",
      "238500.iterasyon, Train Loss = 0.23658920549199383, Val Loss = 11.28441613666787\n",
      "238600.iterasyon, Train Loss = 0.23658113232219946, Val Loss = 11.284543452180793\n",
      "238700.iterasyon, Train Loss = 0.2365730663521236, Val Loss = 11.284670709022427\n",
      "238800.iterasyon, Train Loss = 0.23656500757318136, Val Loss = 11.284797907217664\n",
      "238900.iterasyon, Train Loss = 0.23655695597678658, Val Loss = 11.284925046791384\n",
      "239000.iterasyon, Train Loss = 0.23654891155438063, Val Loss = 11.285052127768394\n",
      "239100.iterasyon, Train Loss = 0.23654087429740056, Val Loss = 11.285179150173521\n",
      "239200.iterasyon, Train Loss = 0.23653284419729972, Val Loss = 11.28530611403163\n",
      "239300.iterasyon, Train Loss = 0.23652482124554455, Val Loss = 11.28543301936751\n",
      "239400.iterasyon, Train Loss = 0.23651680543360565, Val Loss = 11.28555986620601\n",
      "239500.iterasyon, Train Loss = 0.23650879675296663, Val Loss = 11.285686654571943\n",
      "239600.iterasyon, Train Loss = 0.23650079519511966, Val Loss = 11.285813384490147\n",
      "239700.iterasyon, Train Loss = 0.23649280075157164, Val Loss = 11.285940055985368\n",
      "239800.iterasyon, Train Loss = 0.23648481341382963, Val Loss = 11.286066669082514\n",
      "239900.iterasyon, Train Loss = 0.23647683317342358, Val Loss = 11.286193223806313\n",
      "240000.iterasyon, Train Loss = 0.23646886002188325, Val Loss = 11.286319720181599\n",
      "240100.iterasyon, Train Loss = 0.23646089395075473, Val Loss = 11.286446158233163\n",
      "240200.iterasyon, Train Loss = 0.23645293495159184, Val Loss = 11.286572537985752\n",
      "240300.iterasyon, Train Loss = 0.23644498301595648, Val Loss = 11.286698859464176\n",
      "240400.iterasyon, Train Loss = 0.23643703813542397, Val Loss = 11.286825122693234\n",
      "240500.iterasyon, Train Loss = 0.23642910030157563, Val Loss = 11.286951327697684\n",
      "240600.iterasyon, Train Loss = 0.23642116950600986, Val Loss = 11.28707747450229\n",
      "240700.iterasyon, Train Loss = 0.23641324574032538, Val Loss = 11.287203563131882\n",
      "240800.iterasyon, Train Loss = 0.2364053289961426, Val Loss = 11.287329593611105\n",
      "240900.iterasyon, Train Loss = 0.23639741926507796, Val Loss = 11.287455565964803\n",
      "241000.iterasyon, Train Loss = 0.23638951653877105, Val Loss = 11.28758148021772\n",
      "241100.iterasyon, Train Loss = 0.23638162080886005, Val Loss = 11.28770733639458\n",
      "241200.iterasyon, Train Loss = 0.2363737320670026, Val Loss = 11.287833134520167\n",
      "241300.iterasyon, Train Loss = 0.23636585030485935, Val Loss = 11.287958874619182\n",
      "241400.iterasyon, Train Loss = 0.23635797551410728, Val Loss = 11.288084556716369\n",
      "241500.iterasyon, Train Loss = 0.23635010768642828, Val Loss = 11.288210180836451\n",
      "241600.iterasyon, Train Loss = 0.23634224681351287, Val Loss = 11.288335747004181\n",
      "241700.iterasyon, Train Loss = 0.23633439288706914, Val Loss = 11.288461255244284\n",
      "241800.iterasyon, Train Loss = 0.2363265458988046, Val Loss = 11.288586705581462\n",
      "241900.iterasyon, Train Loss = 0.2363187058404456, Val Loss = 11.288712098040437\n",
      "242000.iterasyon, Train Loss = 0.236310872703721, Val Loss = 11.288837432645908\n",
      "242100.iterasyon, Train Loss = 0.23630304648037778, Val Loss = 11.288962709422554\n",
      "242200.iterasyon, Train Loss = 0.23629522716216667, Val Loss = 11.289087928395109\n",
      "242300.iterasyon, Train Loss = 0.23628741474084602, Val Loss = 11.289213089588271\n",
      "242400.iterasyon, Train Loss = 0.23627960920819238, Val Loss = 11.289338193026708\n",
      "242500.iterasyon, Train Loss = 0.23627181055598614, Val Loss = 11.289463238735125\n",
      "242600.iterasyon, Train Loss = 0.23626401877601844, Val Loss = 11.289588226738168\n",
      "242700.iterasyon, Train Loss = 0.23625623386008873, Val Loss = 11.28971315706058\n",
      "242800.iterasyon, Train Loss = 0.23624845580000775, Val Loss = 11.289838029726978\n",
      "242900.iterasyon, Train Loss = 0.23624068458759936, Val Loss = 11.28996284476203\n",
      "243000.iterasyon, Train Loss = 0.23623292021469147, Val Loss = 11.290087602190415\n",
      "243100.iterasyon, Train Loss = 0.2362251626731261, Val Loss = 11.290212302036766\n",
      "243200.iterasyon, Train Loss = 0.23621741195474946, Val Loss = 11.290336944325759\n",
      "243300.iterasyon, Train Loss = 0.2362096680514227, Val Loss = 11.290461529082037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243400.iterasyon, Train Loss = 0.23620193095501452, Val Loss = 11.29058605633026\n",
      "243500.iterasyon, Train Loss = 0.23619420065740712, Val Loss = 11.290710526095014\n",
      "243600.iterasyon, Train Loss = 0.23618647715048322, Val Loss = 11.290834938400982\n",
      "243700.iterasyon, Train Loss = 0.23617876042614472, Val Loss = 11.290959293272781\n",
      "243800.iterasyon, Train Loss = 0.2361710504762986, Val Loss = 11.29108359073501\n",
      "243900.iterasyon, Train Loss = 0.23616334729285782, Val Loss = 11.291207830812347\n",
      "244000.iterasyon, Train Loss = 0.23615565086775603, Val Loss = 11.291332013529338\n",
      "244100.iterasyon, Train Loss = 0.2361479611929244, Val Loss = 11.291456138910638\n",
      "244200.iterasyon, Train Loss = 0.23614027826031078, Val Loss = 11.29158020698085\n",
      "244300.iterasyon, Train Loss = 0.23613260206187203, Val Loss = 11.29170421776458\n",
      "244400.iterasyon, Train Loss = 0.23612493258957276, Val Loss = 11.2918281712864\n",
      "244500.iterasyon, Train Loss = 0.2361172698353864, Val Loss = 11.291952067570918\n",
      "244600.iterasyon, Train Loss = 0.2361096137912969, Val Loss = 11.292075906642705\n",
      "244700.iterasyon, Train Loss = 0.23610196444930154, Val Loss = 11.292199688526317\n",
      "244800.iterasyon, Train Loss = 0.23609432180139966, Val Loss = 11.292323413246375\n",
      "244900.iterasyon, Train Loss = 0.23608668583960565, Val Loss = 11.292447080827435\n",
      "245000.iterasyon, Train Loss = 0.23607905655594166, Val Loss = 11.292570691294065\n",
      "245100.iterasyon, Train Loss = 0.2360714339424401, Val Loss = 11.292694244670855\n",
      "245200.iterasyon, Train Loss = 0.23606381799114143, Val Loss = 11.292817740982306\n",
      "245300.iterasyon, Train Loss = 0.23605620869409558, Val Loss = 11.292941180253047\n",
      "245400.iterasyon, Train Loss = 0.2360486060433639, Val Loss = 11.293064562507553\n",
      "245500.iterasyon, Train Loss = 0.2360410100310155, Val Loss = 11.293187887770383\n",
      "245600.iterasyon, Train Loss = 0.23603342064912794, Val Loss = 11.293311156066082\n",
      "245700.iterasyon, Train Loss = 0.23602583788979414, Val Loss = 11.293434367419168\n",
      "245800.iterasyon, Train Loss = 0.2360182617451063, Val Loss = 11.293557521854229\n",
      "245900.iterasyon, Train Loss = 0.2360106922071766, Val Loss = 11.29368061939573\n",
      "246000.iterasyon, Train Loss = 0.23600312926811684, Val Loss = 11.29380366006821\n",
      "246100.iterasyon, Train Loss = 0.23599557292005624, Val Loss = 11.293926643896155\n",
      "246200.iterasyon, Train Loss = 0.23598802315512826, Val Loss = 11.294049570904129\n",
      "246300.iterasyon, Train Loss = 0.23598047996547944, Val Loss = 11.29417244111659\n",
      "246400.iterasyon, Train Loss = 0.23597294334326246, Val Loss = 11.294295254558055\n",
      "246500.iterasyon, Train Loss = 0.23596541328064166, Val Loss = 11.294418011252999\n",
      "246600.iterasyon, Train Loss = 0.23595788976978904, Val Loss = 11.294540711225924\n",
      "246700.iterasyon, Train Loss = 0.235950372802887, Val Loss = 11.294663354501337\n",
      "246800.iterasyon, Train Loss = 0.23594286237212533, Val Loss = 11.294785941103651\n",
      "246900.iterasyon, Train Loss = 0.23593535846970615, Val Loss = 11.294908471057399\n",
      "247000.iterasyon, Train Loss = 0.2359278610878385, Val Loss = 11.295030944387008\n",
      "247100.iterasyon, Train Loss = 0.23592037021874232, Val Loss = 11.295153361116975\n",
      "247200.iterasyon, Train Loss = 0.23591288585464412, Val Loss = 11.29527572127175\n",
      "247300.iterasyon, Train Loss = 0.2359054079877828, Val Loss = 11.295398024875793\n",
      "247400.iterasyon, Train Loss = 0.23589793661040584, Val Loss = 11.295520271953501\n",
      "247500.iterasyon, Train Loss = 0.2358904717147664, Val Loss = 11.295642462529408\n",
      "247600.iterasyon, Train Loss = 0.23588301329313188, Val Loss = 11.295764596627908\n",
      "247700.iterasyon, Train Loss = 0.23587556133777685, Val Loss = 11.295886674273403\n",
      "247800.iterasyon, Train Loss = 0.23586811584098488, Val Loss = 11.296008695490377\n",
      "247900.iterasyon, Train Loss = 0.23586067679504555, Val Loss = 11.29613066030321\n",
      "248000.iterasyon, Train Loss = 0.2358532441922667, Val Loss = 11.29625256873632\n",
      "248100.iterasyon, Train Loss = 0.23584581802495305, Val Loss = 11.296374420814153\n",
      "248200.iterasyon, Train Loss = 0.23583839828542985, Val Loss = 11.296496216561092\n",
      "248300.iterasyon, Train Loss = 0.23583098496602373, Val Loss = 11.296617956001539\n",
      "248400.iterasyon, Train Loss = 0.23582357805907214, Val Loss = 11.2967396391599\n",
      "248500.iterasyon, Train Loss = 0.2358161775569284, Val Loss = 11.296861266060562\n",
      "248600.iterasyon, Train Loss = 0.23580878345193898, Val Loss = 11.29698283672797\n",
      "248700.iterasyon, Train Loss = 0.23580139573648062, Val Loss = 11.297104351186412\n",
      "248800.iterasyon, Train Loss = 0.23579401440292078, Val Loss = 11.297225809460322\n",
      "248900.iterasyon, Train Loss = 0.23578663944364667, Val Loss = 11.297347211574035\n",
      "249000.iterasyon, Train Loss = 0.23577927085105146, Val Loss = 11.297468557551937\n",
      "249100.iterasyon, Train Loss = 0.2357719086175335, Val Loss = 11.297589847418433\n",
      "249200.iterasyon, Train Loss = 0.2357645527355088, Val Loss = 11.297711081197791\n",
      "249300.iterasyon, Train Loss = 0.23575720319739202, Val Loss = 11.29783225891446\n",
      "249400.iterasyon, Train Loss = 0.23574985999561796, Val Loss = 11.29795338059268\n",
      "249500.iterasyon, Train Loss = 0.23574252312262114, Val Loss = 11.298074446256877\n",
      "249600.iterasyon, Train Loss = 0.23573519257084674, Val Loss = 11.298195455931397\n",
      "249700.iterasyon, Train Loss = 0.23572786833275397, Val Loss = 11.298316409640515\n",
      "249800.iterasyon, Train Loss = 0.23572055040080728, Val Loss = 11.29843730740858\n",
      "249900.iterasyon, Train Loss = 0.2357132387674825, Val Loss = 11.298558149259875\n",
      "250000.iterasyon, Train Loss = 0.23570593342525817, Val Loss = 11.298678935218794\n",
      "250100.iterasyon, Train Loss = 0.23569863436663074, Val Loss = 11.298799665309579\n",
      "250200.iterasyon, Train Loss = 0.2356913415840958, Val Loss = 11.29892033955655\n",
      "250300.iterasyon, Train Loss = 0.23568405507016835, Val Loss = 11.299040957984014\n",
      "250400.iterasyon, Train Loss = 0.23567677481736327, Val Loss = 11.299161520616272\n",
      "250500.iterasyon, Train Loss = 0.235669500818211, Val Loss = 11.299282027477625\n",
      "250600.iterasyon, Train Loss = 0.2356622330652438, Val Loss = 11.29940247859231\n",
      "250700.iterasyon, Train Loss = 0.23565497155101164, Val Loss = 11.29952287398465\n",
      "250800.iterasyon, Train Loss = 0.23564771626806427, Val Loss = 11.299643213678907\n",
      "250900.iterasyon, Train Loss = 0.23564046720896867, Val Loss = 11.299763497699326\n",
      "251000.iterasyon, Train Loss = 0.235633224366292, Val Loss = 11.299883726070213\n",
      "251100.iterasyon, Train Loss = 0.23562598773262, Val Loss = 11.30000389881578\n",
      "251200.iterasyon, Train Loss = 0.23561875730053794, Val Loss = 11.30012401596031\n",
      "251300.iterasyon, Train Loss = 0.23561153306264848, Val Loss = 11.30024407752804\n",
      "251400.iterasyon, Train Loss = 0.23560431501155432, Val Loss = 11.300364083543197\n",
      "251500.iterasyon, Train Loss = 0.23559710313987467, Val Loss = 11.30048403403003\n",
      "251600.iterasyon, Train Loss = 0.23558989744023062, Val Loss = 11.300603929012777\n",
      "251700.iterasyon, Train Loss = 0.23558269790525968, Val Loss = 11.300723768515631\n",
      "251800.iterasyon, Train Loss = 0.23557550452760098, Val Loss = 11.300843552562872\n",
      "251900.iterasyon, Train Loss = 0.23556831729990782, Val Loss = 11.300963281178657\n",
      "252000.iterasyon, Train Loss = 0.23556113621483732, Val Loss = 11.301082954387228\n",
      "252100.iterasyon, Train Loss = 0.23555396126505768, Val Loss = 11.301202572212762\n",
      "252200.iterasyon, Train Loss = 0.23554679244325083, Val Loss = 11.301322134679483\n",
      "252300.iterasyon, Train Loss = 0.23553962974209652, Val Loss = 11.301441641811572\n",
      "252400.iterasyon, Train Loss = 0.2355324731542935, Val Loss = 11.301561093633236\n",
      "252500.iterasyon, Train Loss = 0.23552532267254336, Val Loss = 11.30168049016863\n",
      "252600.iterasyon, Train Loss = 0.23551817828955804, Val Loss = 11.301799831441961\n",
      "252700.iterasyon, Train Loss = 0.2355110399980591, Val Loss = 11.301919117477341\n",
      "252800.iterasyon, Train Loss = 0.23550390779077426, Val Loss = 11.302038348298984\n",
      "252900.iterasyon, Train Loss = 0.23549678166044208, Val Loss = 11.302157523931061\n",
      "253000.iterasyon, Train Loss = 0.2354896615998083, Val Loss = 11.302276644397702\n",
      "253100.iterasyon, Train Loss = 0.23548254760162982, Val Loss = 11.30239570972304\n",
      "253200.iterasyon, Train Loss = 0.23547543965867043, Val Loss = 11.302514719931272\n",
      "253300.iterasyon, Train Loss = 0.2354683377637007, Val Loss = 11.302633675046492\n",
      "253400.iterasyon, Train Loss = 0.23546124190950332, Val Loss = 11.302752575092855\n",
      "253500.iterasyon, Train Loss = 0.23545415208886358, Val Loss = 11.302871420094498\n",
      "253600.iterasyon, Train Loss = 0.23544706829458573, Val Loss = 11.30299021007553\n",
      "253700.iterasyon, Train Loss = 0.23543999051947404, Val Loss = 11.303108945060053\n",
      "253800.iterasyon, Train Loss = 0.2354329187563425, Val Loss = 11.303227625072191\n",
      "253900.iterasyon, Train Loss = 0.2354258529980153, Val Loss = 11.303346250136077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254000.iterasyon, Train Loss = 0.23541879323732673, Val Loss = 11.303464820275769\n",
      "254100.iterasyon, Train Loss = 0.23541173946711483, Val Loss = 11.303583335515395\n",
      "254200.iterasyon, Train Loss = 0.23540469168023215, Val Loss = 11.303701795879014\n",
      "254300.iterasyon, Train Loss = 0.2353976498695343, Val Loss = 11.303820201390737\n",
      "254400.iterasyon, Train Loss = 0.2353906140278873, Val Loss = 11.303938552074625\n",
      "254500.iterasyon, Train Loss = 0.23538358414816749, Val Loss = 11.304056847954755\n",
      "254600.iterasyon, Train Loss = 0.23537656022325915, Val Loss = 11.304175089055178\n",
      "254700.iterasyon, Train Loss = 0.23536954224605, Val Loss = 11.304293275400005\n",
      "254800.iterasyon, Train Loss = 0.235362530209445, Val Loss = 11.304411407013244\n",
      "254900.iterasyon, Train Loss = 0.23535552410635027, Val Loss = 11.30452948391897\n",
      "255000.iterasyon, Train Loss = 0.2353485239296832, Val Loss = 11.304647506141215\n",
      "255100.iterasyon, Train Loss = 0.23534152967237018, Val Loss = 11.304765473704018\n",
      "255200.iterasyon, Train Loss = 0.2353345413273442, Val Loss = 11.304883386631436\n",
      "255300.iterasyon, Train Loss = 0.235327558887547, Val Loss = 11.30500124494746\n",
      "255400.iterasyon, Train Loss = 0.23532058234593226, Val Loss = 11.305119048676136\n",
      "255500.iterasyon, Train Loss = 0.23531361169545695, Val Loss = 11.305236797841493\n",
      "255600.iterasyon, Train Loss = 0.2353066469290899, Val Loss = 11.305354492467513\n",
      "255700.iterasyon, Train Loss = 0.23529968803980642, Val Loss = 11.305472132578194\n",
      "255800.iterasyon, Train Loss = 0.23529273502058923, Val Loss = 11.305589718197629\n",
      "255900.iterasyon, Train Loss = 0.2352857878644342, Val Loss = 11.305707249349695\n",
      "256000.iterasyon, Train Loss = 0.23527884656433784, Val Loss = 11.305824726058447\n",
      "256100.iterasyon, Train Loss = 0.23527191111331444, Val Loss = 11.305942148347839\n",
      "256200.iterasyon, Train Loss = 0.23526498150437694, Val Loss = 11.306059516241874\n",
      "256300.iterasyon, Train Loss = 0.23525805773055541, Val Loss = 11.306176829764505\n",
      "256400.iterasyon, Train Loss = 0.23525113978488277, Val Loss = 11.306294088939708\n",
      "256500.iterasyon, Train Loss = 0.23524422766039857, Val Loss = 11.306411293791456\n",
      "256600.iterasyon, Train Loss = 0.2352373213501575, Val Loss = 11.30652844434368\n",
      "256700.iterasyon, Train Loss = 0.23523042084721732, Val Loss = 11.306645540620336\n",
      "256800.iterasyon, Train Loss = 0.23522352614464562, Val Loss = 11.30676258264537\n",
      "256900.iterasyon, Train Loss = 0.23521663723551625, Val Loss = 11.306879570442737\n",
      "257000.iterasyon, Train Loss = 0.23520975411291475, Val Loss = 11.306996504036317\n",
      "257100.iterasyon, Train Loss = 0.23520287676993304, Val Loss = 11.3071133834501\n",
      "257200.iterasyon, Train Loss = 0.23519600519967307, Val Loss = 11.307230208707978\n",
      "257300.iterasyon, Train Loss = 0.23518913939523775, Val Loss = 11.30734697983387\n",
      "257400.iterasyon, Train Loss = 0.23518227934974903, Val Loss = 11.307463696851688\n",
      "257500.iterasyon, Train Loss = 0.2351754250563308, Val Loss = 11.307580359785325\n",
      "257600.iterasyon, Train Loss = 0.23516857650811396, Val Loss = 11.307696968658695\n",
      "257700.iterasyon, Train Loss = 0.2351617336982418, Val Loss = 11.307813523495676\n",
      "257800.iterasyon, Train Loss = 0.23515489661986047, Val Loss = 11.307930024320187\n",
      "257900.iterasyon, Train Loss = 0.2351480652661334, Val Loss = 11.30804647115605\n",
      "258000.iterasyon, Train Loss = 0.23514123963022093, Val Loss = 11.308162864027203\n",
      "258100.iterasyon, Train Loss = 0.23513441970530027, Val Loss = 11.308279202957445\n",
      "258200.iterasyon, Train Loss = 0.23512760548455172, Val Loss = 11.308395487970708\n",
      "258300.iterasyon, Train Loss = 0.23512079696116292, Val Loss = 11.30851171909081\n",
      "258400.iterasyon, Train Loss = 0.23511399412833703, Val Loss = 11.308627896341598\n",
      "258500.iterasyon, Train Loss = 0.23510719697927665, Val Loss = 11.308744019746966\n",
      "258600.iterasyon, Train Loss = 0.2351004055072011, Val Loss = 11.308860089330668\n",
      "258700.iterasyon, Train Loss = 0.23509361970532602, Val Loss = 11.308976105116647\n",
      "258800.iterasyon, Train Loss = 0.2350868395668838, Val Loss = 11.309092067128693\n",
      "258900.iterasyon, Train Loss = 0.23508006508511495, Val Loss = 11.309207975390581\n",
      "259000.iterasyon, Train Loss = 0.2350732962532647, Val Loss = 11.309323829926175\n",
      "259100.iterasyon, Train Loss = 0.23506653306458955, Val Loss = 11.309439630759261\n",
      "259200.iterasyon, Train Loss = 0.23505977551235127, Val Loss = 11.309555377913691\n",
      "259300.iterasyon, Train Loss = 0.23505302358981758, Val Loss = 11.309671071413188\n",
      "259400.iterasyon, Train Loss = 0.23504627729027106, Val Loss = 11.309786711281625\n",
      "259500.iterasyon, Train Loss = 0.23503953660699653, Val Loss = 11.309902297542752\n",
      "259600.iterasyon, Train Loss = 0.23503280153328898, Val Loss = 11.310017830220353\n",
      "259700.iterasyon, Train Loss = 0.23502607206245088, Val Loss = 11.310133309338207\n",
      "259800.iterasyon, Train Loss = 0.23501934818779083, Val Loss = 11.310248734920098\n",
      "259900.iterasyon, Train Loss = 0.23501262990263141, Val Loss = 11.31036410698977\n",
      "260000.iterasyon, Train Loss = 0.23500591720029707, Val Loss = 11.310479425570962\n",
      "260100.iterasyon, Train Loss = 0.23499921007412394, Val Loss = 11.310594690687461\n",
      "260200.iterasyon, Train Loss = 0.23499250851745201, Val Loss = 11.31070990236301\n",
      "260300.iterasyon, Train Loss = 0.23498581252363135, Val Loss = 11.310825060621346\n",
      "260400.iterasyon, Train Loss = 0.23497912208602365, Val Loss = 11.310940165486192\n",
      "260500.iterasyon, Train Loss = 0.23497243719799035, Val Loss = 11.31105521698132\n",
      "260600.iterasyon, Train Loss = 0.234965757852911, Val Loss = 11.311170215130428\n",
      "260700.iterasyon, Train Loss = 0.23495908404416244, Val Loss = 11.311285159957208\n",
      "260800.iterasyon, Train Loss = 0.2349524157651358, Val Loss = 11.311400051485402\n",
      "260900.iterasyon, Train Loss = 0.2349457530092295, Val Loss = 11.311514889738698\n",
      "261000.iterasyon, Train Loss = 0.23493909576985209, Val Loss = 11.311629674740805\n",
      "261100.iterasyon, Train Loss = 0.23493244404041289, Val Loss = 11.311744406515412\n",
      "261200.iterasyon, Train Loss = 0.23492579781433284, Val Loss = 11.311859085086233\n",
      "261300.iterasyon, Train Loss = 0.2349191570850425, Val Loss = 11.311973710476943\n",
      "261400.iterasyon, Train Loss = 0.23491252184597933, Val Loss = 11.312088282711166\n",
      "261500.iterasyon, Train Loss = 0.23490589209058865, Val Loss = 11.312202801812628\n",
      "261600.iterasyon, Train Loss = 0.23489926781232137, Val Loss = 11.312317267804954\n",
      "261700.iterasyon, Train Loss = 0.23489264900463377, Val Loss = 11.312431680711855\n",
      "261800.iterasyon, Train Loss = 0.23488603566100186, Val Loss = 11.312546040556935\n",
      "261900.iterasyon, Train Loss = 0.23487942777489812, Val Loss = 11.312660347363835\n",
      "262000.iterasyon, Train Loss = 0.23487282533980794, Val Loss = 11.312774601156216\n",
      "262100.iterasyon, Train Loss = 0.23486622834921747, Val Loss = 11.312888801957728\n",
      "262200.iterasyon, Train Loss = 0.23485963679663055, Val Loss = 11.313002949791972\n",
      "262300.iterasyon, Train Loss = 0.23485305067555315, Val Loss = 11.313117044682572\n",
      "262400.iterasyon, Train Loss = 0.23484646997949865, Val Loss = 11.313231086653175\n",
      "262500.iterasyon, Train Loss = 0.2348398947019904, Val Loss = 11.31334507572736\n",
      "262600.iterasyon, Train Loss = 0.2348333248365577, Val Loss = 11.31345901192874\n",
      "262700.iterasyon, Train Loss = 0.23482676037674052, Val Loss = 11.313572895280915\n",
      "262800.iterasyon, Train Loss = 0.23482020131607984, Val Loss = 11.313686725807493\n",
      "262900.iterasyon, Train Loss = 0.2348136476481325, Val Loss = 11.31380050353204\n",
      "263000.iterasyon, Train Loss = 0.23480709936645872, Val Loss = 11.313914228478101\n",
      "263100.iterasyon, Train Loss = 0.23480055646462614, Val Loss = 11.314027900669315\n",
      "263200.iterasyon, Train Loss = 0.23479401893621332, Val Loss = 11.31414152012922\n",
      "263300.iterasyon, Train Loss = 0.23478748677480035, Val Loss = 11.314255086881357\n",
      "263400.iterasyon, Train Loss = 0.23478095997398005, Val Loss = 11.314368600949344\n",
      "263500.iterasyon, Train Loss = 0.23477443852735194, Val Loss = 11.314482062356678\n",
      "263600.iterasyon, Train Loss = 0.2347679224285234, Val Loss = 11.314595471126914\n",
      "263700.iterasyon, Train Loss = 0.23476141167110845, Val Loss = 11.314708827283592\n",
      "263800.iterasyon, Train Loss = 0.23475490624872675, Val Loss = 11.314822130850231\n",
      "263900.iterasyon, Train Loss = 0.23474840615500855, Val Loss = 11.314935381850402\n",
      "264000.iterasyon, Train Loss = 0.23474191138359243, Val Loss = 11.315048580307549\n",
      "264100.iterasyon, Train Loss = 0.23473542192812524, Val Loss = 11.315161726245245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264200.iterasyon, Train Loss = 0.23472893778225357, Val Loss = 11.315274819686989\n",
      "264300.iterasyon, Train Loss = 0.2347224589396408, Val Loss = 11.31538786065627\n",
      "264400.iterasyon, Train Loss = 0.23471598539395241, Val Loss = 11.315500849176578\n",
      "264500.iterasyon, Train Loss = 0.23470951713886576, Val Loss = 11.315613785271413\n",
      "264600.iterasyon, Train Loss = 0.23470305416806028, Val Loss = 11.315726668964246\n",
      "264700.iterasyon, Train Loss = 0.2346965964752303, Val Loss = 11.315839500278571\n",
      "264800.iterasyon, Train Loss = 0.2346901440540691, Val Loss = 11.315952279237846\n",
      "264900.iterasyon, Train Loss = 0.23468369689828106, Val Loss = 11.316065005865525\n",
      "265000.iterasyon, Train Loss = 0.23467725500158232, Val Loss = 11.316177680185069\n",
      "265100.iterasyon, Train Loss = 0.23467081835769202, Val Loss = 11.316290302219931\n",
      "265200.iterasyon, Train Loss = 0.23466438696033418, Val Loss = 11.316402871993565\n",
      "265300.iterasyon, Train Loss = 0.23465796080324627, Val Loss = 11.316515389529439\n",
      "265400.iterasyon, Train Loss = 0.2346515398801714, Val Loss = 11.316627854850944\n",
      "265500.iterasyon, Train Loss = 0.23464512418485886, Val Loss = 11.316740267981524\n",
      "265600.iterasyon, Train Loss = 0.23463871371106393, Val Loss = 11.31685262894461\n",
      "265700.iterasyon, Train Loss = 0.23463230845255334, Val Loss = 11.31696493776359\n",
      "265800.iterasyon, Train Loss = 0.2346259084030989, Val Loss = 11.317077194461877\n",
      "265900.iterasyon, Train Loss = 0.23461951355647925, Val Loss = 11.317189399062892\n",
      "266000.iterasyon, Train Loss = 0.23461312390648126, Val Loss = 11.317301551590033\n",
      "266100.iterasyon, Train Loss = 0.23460673944690272, Val Loss = 11.317413652066646\n",
      "266200.iterasyon, Train Loss = 0.23460036017153993, Val Loss = 11.31752570051618\n",
      "266300.iterasyon, Train Loss = 0.23459398607420723, Val Loss = 11.317637696961985\n",
      "266400.iterasyon, Train Loss = 0.23458761714871515, Val Loss = 11.31774964142741\n",
      "266500.iterasyon, Train Loss = 0.23458125338889216, Val Loss = 11.317861533935854\n",
      "266600.iterasyon, Train Loss = 0.23457489478856927, Val Loss = 11.317973374510602\n",
      "266700.iterasyon, Train Loss = 0.23456854134158348, Val Loss = 11.318085163175114\n",
      "266800.iterasyon, Train Loss = 0.23456219304178194, Val Loss = 11.318196899952705\n",
      "266900.iterasyon, Train Loss = 0.23455584988301706, Val Loss = 11.318308584866648\n",
      "267000.iterasyon, Train Loss = 0.234549511859147, Val Loss = 11.31842021794035\n",
      "267100.iterasyon, Train Loss = 0.23454317896404453, Val Loss = 11.318531799197109\n",
      "267200.iterasyon, Train Loss = 0.23453685119158243, Val Loss = 11.318643328660272\n",
      "267300.iterasyon, Train Loss = 0.23453052853564105, Val Loss = 11.318754806353123\n",
      "267400.iterasyon, Train Loss = 0.234524210990113, Val Loss = 11.318866232298978\n",
      "267500.iterasyon, Train Loss = 0.23451789854889618, Val Loss = 11.318977606521152\n",
      "267600.iterasyon, Train Loss = 0.23451159120589107, Val Loss = 11.319088929042922\n",
      "267700.iterasyon, Train Loss = 0.23450528895501074, Val Loss = 11.319200199887586\n",
      "267800.iterasyon, Train Loss = 0.23449899179017478, Val Loss = 11.319311419078431\n",
      "267900.iterasyon, Train Loss = 0.2344926997053118, Val Loss = 11.319422586638737\n",
      "268000.iterasyon, Train Loss = 0.23448641269435103, Val Loss = 11.319533702591775\n",
      "268100.iterasyon, Train Loss = 0.234480130751234, Val Loss = 11.319644766960796\n",
      "268200.iterasyon, Train Loss = 0.23447385386990935, Val Loss = 11.31975577976909\n",
      "268300.iterasyon, Train Loss = 0.2344675820443305, Val Loss = 11.319866741039885\n",
      "268400.iterasyon, Train Loss = 0.23446131526846173, Val Loss = 11.319977650796423\n",
      "268500.iterasyon, Train Loss = 0.23445505353626936, Val Loss = 11.320088509061993\n",
      "268600.iterasyon, Train Loss = 0.23444879684173456, Val Loss = 11.320199315859762\n",
      "268700.iterasyon, Train Loss = 0.23444254517883853, Val Loss = 11.320310071212953\n",
      "268800.iterasyon, Train Loss = 0.2344362985415692, Val Loss = 11.320420775144875\n",
      "268900.iterasyon, Train Loss = 0.23443005692392702, Val Loss = 11.32053142767865\n",
      "269000.iterasyon, Train Loss = 0.23442382031991993, Val Loss = 11.320642028837502\n",
      "269100.iterasyon, Train Loss = 0.2344175887235567, Val Loss = 11.320752578644672\n",
      "269200.iterasyon, Train Loss = 0.23441136212885863, Val Loss = 11.320863077123342\n",
      "269300.iterasyon, Train Loss = 0.23440514052985065, Val Loss = 11.320973524296685\n",
      "269400.iterasyon, Train Loss = 0.23439892392056838, Val Loss = 11.321083920187899\n",
      "269500.iterasyon, Train Loss = 0.2343927122950501, Val Loss = 11.321194264820141\n",
      "269600.iterasyon, Train Loss = 0.23438650564734487, Val Loss = 11.321304558216614\n",
      "269700.iterasyon, Train Loss = 0.2343803039715097, Val Loss = 11.321414800400454\n",
      "269800.iterasyon, Train Loss = 0.23437410726160524, Val Loss = 11.321524991394822\n",
      "269900.iterasyon, Train Loss = 0.2343679155117011, Val Loss = 11.321635131222864\n",
      "270000.iterasyon, Train Loss = 0.2343617287158732, Val Loss = 11.32174521990773\n",
      "270100.iterasyon, Train Loss = 0.234355546868203, Val Loss = 11.321855257472546\n",
      "270200.iterasyon, Train Loss = 0.2343493699627838, Val Loss = 11.321965243940484\n",
      "270300.iterasyon, Train Loss = 0.23434319799370998, Val Loss = 11.322075179334645\n",
      "270400.iterasyon, Train Loss = 0.23433703095508848, Val Loss = 11.322185063678125\n",
      "270500.iterasyon, Train Loss = 0.23433086884103083, Val Loss = 11.322294896994064\n",
      "270600.iterasyon, Train Loss = 0.23432471164565383, Val Loss = 11.32240467930555\n",
      "270700.iterasyon, Train Loss = 0.23431855936308277, Val Loss = 11.322514410635721\n",
      "270800.iterasyon, Train Loss = 0.2343124119874542, Val Loss = 11.322624091007622\n",
      "270900.iterasyon, Train Loss = 0.23430626951290048, Val Loss = 11.322733720444358\n",
      "271000.iterasyon, Train Loss = 0.23430013193357488, Val Loss = 11.322843298969053\n",
      "271100.iterasyon, Train Loss = 0.23429399924362723, Val Loss = 11.32295282660469\n",
      "271200.iterasyon, Train Loss = 0.23428787143721952, Val Loss = 11.323062303374392\n",
      "271300.iterasyon, Train Loss = 0.23428174850851802, Val Loss = 11.323171729301245\n",
      "271400.iterasyon, Train Loss = 0.2342756304516973, Val Loss = 11.323281104408258\n",
      "271500.iterasyon, Train Loss = 0.234269517260937, Val Loss = 11.32339042871848\n",
      "271600.iterasyon, Train Loss = 0.2342634089304283, Val Loss = 11.323499702254987\n",
      "271700.iterasyon, Train Loss = 0.2342573054543667, Val Loss = 11.32360892504076\n",
      "271800.iterasyon, Train Loss = 0.2342512068269513, Val Loss = 11.323718097098881\n",
      "271900.iterasyon, Train Loss = 0.2342451130423916, Val Loss = 11.323827218452363\n",
      "272000.iterasyon, Train Loss = 0.23423902409490402, Val Loss = 11.323936289124209\n",
      "272100.iterasyon, Train Loss = 0.2342329399787123, Val Loss = 11.324045309137396\n",
      "272200.iterasyon, Train Loss = 0.23422686068804396, Val Loss = 11.324154278515001\n",
      "272300.iterasyon, Train Loss = 0.23422078621713682, Val Loss = 11.32426319727999\n",
      "272400.iterasyon, Train Loss = 0.23421471656023582, Val Loss = 11.324372065455284\n",
      "272500.iterasyon, Train Loss = 0.2342086517115908, Val Loss = 11.324480883063954\n",
      "272600.iterasyon, Train Loss = 0.23420259166545543, Val Loss = 11.324589650128924\n",
      "272700.iterasyon, Train Loss = 0.2341965364160962, Val Loss = 11.324698366673221\n",
      "272800.iterasyon, Train Loss = 0.23419048595778302, Val Loss = 11.324807032719773\n",
      "272900.iterasyon, Train Loss = 0.23418444028479482, Val Loss = 11.324915648291558\n",
      "273000.iterasyon, Train Loss = 0.2341783993914155, Val Loss = 11.325024213411469\n",
      "273100.iterasyon, Train Loss = 0.23417236327193536, Val Loss = 11.32513272810249\n",
      "273200.iterasyon, Train Loss = 0.23416633192065428, Val Loss = 11.325241192387566\n",
      "273300.iterasyon, Train Loss = 0.23416030533187604, Val Loss = 11.325349606289583\n",
      "273400.iterasyon, Train Loss = 0.23415428349990985, Val Loss = 11.325457969831573\n",
      "273500.iterasyon, Train Loss = 0.23414826641908007, Val Loss = 11.325566283036306\n",
      "273600.iterasyon, Train Loss = 0.23414225408370712, Val Loss = 11.325674545926821\n",
      "273700.iterasyon, Train Loss = 0.234136246488122, Val Loss = 11.325782758525968\n",
      "273800.iterasyon, Train Loss = 0.23413024362666765, Val Loss = 11.32589092085663\n",
      "273900.iterasyon, Train Loss = 0.23412424549368713, Val Loss = 11.325999032941745\n",
      "274000.iterasyon, Train Loss = 0.23411825208353287, Val Loss = 11.326107094804168\n",
      "274100.iterasyon, Train Loss = 0.23411226339056426, Val Loss = 11.326215106466782\n",
      "274200.iterasyon, Train Loss = 0.2341062794091471, Val Loss = 11.326323067952464\n",
      "274300.iterasyon, Train Loss = 0.23410030013365127, Val Loss = 11.326430979284062\n",
      "274400.iterasyon, Train Loss = 0.23409432555845902, Val Loss = 11.326538840484448\n",
      "274500.iterasyon, Train Loss = 0.23408835567795663, Val Loss = 11.326646651576471\n",
      "274600.iterasyon, Train Loss = 0.2340823904865307, Val Loss = 11.326754412583021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274700.iterasyon, Train Loss = 0.23407642997858805, Val Loss = 11.326862123526851\n",
      "274800.iterasyon, Train Loss = 0.23407047414853022, Val Loss = 11.32696978443086\n",
      "274900.iterasyon, Train Loss = 0.23406452299077174, Val Loss = 11.327077395317833\n",
      "275000.iterasyon, Train Loss = 0.2340585764997285, Val Loss = 11.327184956210607\n",
      "275100.iterasyon, Train Loss = 0.23405263466983106, Val Loss = 11.327292467131997\n",
      "275200.iterasyon, Train Loss = 0.23404669749550813, Val Loss = 11.327399928104805\n",
      "275300.iterasyon, Train Loss = 0.23404076497120072, Val Loss = 11.32750733915185\n",
      "275400.iterasyon, Train Loss = 0.23403483709135486, Val Loss = 11.3276147002959\n",
      "275500.iterasyon, Train Loss = 0.23402891385041932, Val Loss = 11.327722011559777\n",
      "275600.iterasyon, Train Loss = 0.23402299524285827, Val Loss = 11.327829272966223\n",
      "275700.iterasyon, Train Loss = 0.2340170812631316, Val Loss = 11.327936484538037\n",
      "275800.iterasyon, Train Loss = 0.23401117190571863, Val Loss = 11.328043646297985\n",
      "275900.iterasyon, Train Loss = 0.23400526716509196, Val Loss = 11.32815075826878\n",
      "276000.iterasyon, Train Loss = 0.23399936703573854, Val Loss = 11.328257820473235\n",
      "276100.iterasyon, Train Loss = 0.23399347151215297, Val Loss = 11.328364832934055\n",
      "276200.iterasyon, Train Loss = 0.23398758058882926, Val Loss = 11.328471795674007\n",
      "276300.iterasyon, Train Loss = 0.23398169426027848, Val Loss = 11.328578708715796\n",
      "276400.iterasyon, Train Loss = 0.23397581252100572, Val Loss = 11.328685572082184\n",
      "276500.iterasyon, Train Loss = 0.23396993536553493, Val Loss = 11.328792385795849\n",
      "276600.iterasyon, Train Loss = 0.23396406278838577, Val Loss = 11.328899149879561\n",
      "276700.iterasyon, Train Loss = 0.2339581947840936, Val Loss = 11.329005864355953\n",
      "276800.iterasyon, Train Loss = 0.23395233134719307, Val Loss = 11.329112529247823\n",
      "276900.iterasyon, Train Loss = 0.23394647247223044, Val Loss = 11.329219144577785\n",
      "277000.iterasyon, Train Loss = 0.23394061815375586, Val Loss = 11.329325710368561\n",
      "277100.iterasyon, Train Loss = 0.23393476838632724, Val Loss = 11.329432226642805\n",
      "277200.iterasyon, Train Loss = 0.23392892316450653, Val Loss = 11.329538693423197\n",
      "277300.iterasyon, Train Loss = 0.2339230824828656, Val Loss = 11.329645110732415\n",
      "277400.iterasyon, Train Loss = 0.23391724633598174, Val Loss = 11.329751478593112\n",
      "277500.iterasyon, Train Loss = 0.2339114147184372, Val Loss = 11.329857797027962\n",
      "277600.iterasyon, Train Loss = 0.2339055876248192, Val Loss = 11.329964066059604\n",
      "277700.iterasyon, Train Loss = 0.23389976504972823, Val Loss = 11.330070285710653\n",
      "277800.iterasyon, Train Loss = 0.23389394698776456, Val Loss = 11.33017645600375\n",
      "277900.iterasyon, Train Loss = 0.23388813343353856, Val Loss = 11.33028257696154\n",
      "278000.iterasyon, Train Loss = 0.23388232438166245, Val Loss = 11.330388648606627\n",
      "278100.iterasyon, Train Loss = 0.23387651982676247, Val Loss = 11.330494670961633\n",
      "278200.iterasyon, Train Loss = 0.2338707197634631, Val Loss = 11.330600644049163\n",
      "278300.iterasyon, Train Loss = 0.23386492418640362, Val Loss = 11.330706567891804\n",
      "278400.iterasyon, Train Loss = 0.23385913309021808, Val Loss = 11.330812442512155\n",
      "278500.iterasyon, Train Loss = 0.23385334646956155, Val Loss = 11.330918267932816\n",
      "278600.iterasyon, Train Loss = 0.23384756431908163, Val Loss = 11.331024044176347\n",
      "278700.iterasyon, Train Loss = 0.23384178663344254, Val Loss = 11.331129771265308\n",
      "278800.iterasyon, Train Loss = 0.23383601340730895, Val Loss = 11.331235449222321\n",
      "278900.iterasyon, Train Loss = 0.23383024463535562, Val Loss = 11.331341078069906\n",
      "279000.iterasyon, Train Loss = 0.2338244803122592, Val Loss = 11.331446657830645\n",
      "279100.iterasyon, Train Loss = 0.23381872043270516, Val Loss = 11.331552188527068\n",
      "279200.iterasyon, Train Loss = 0.2338129649913906, Val Loss = 11.331657670181688\n",
      "279300.iterasyon, Train Loss = 0.23380721398300783, Val Loss = 11.331763102817035\n",
      "279400.iterasyon, Train Loss = 0.23380146740226634, Val Loss = 11.331868486455665\n",
      "279500.iterasyon, Train Loss = 0.23379572524387285, Val Loss = 11.3319738211201\n",
      "279600.iterasyon, Train Loss = 0.23378998750254754, Val Loss = 11.332079106832841\n",
      "279700.iterasyon, Train Loss = 0.23378425417301416, Val Loss = 11.33218434361635\n",
      "279800.iterasyon, Train Loss = 0.23377852525000062, Val Loss = 11.332289531493203\n",
      "279900.iterasyon, Train Loss = 0.2337728007282438, Val Loss = 11.332394670485858\n",
      "280000.iterasyon, Train Loss = 0.23376708060248774, Val Loss = 11.332499760616763\n",
      "280100.iterasyon, Train Loss = 0.23376136486747826, Val Loss = 11.332604801908431\n",
      "280200.iterasyon, Train Loss = 0.2337556535179718, Val Loss = 11.332709794383355\n",
      "280300.iterasyon, Train Loss = 0.23374994654873057, Val Loss = 11.332814738063991\n",
      "280400.iterasyon, Train Loss = 0.23374424395451956, Val Loss = 11.33291963297278\n",
      "280500.iterasyon, Train Loss = 0.23373854573011574, Val Loss = 11.333024479132147\n",
      "280600.iterasyon, Train Loss = 0.2337328518702979, Val Loss = 11.333129276564566\n",
      "280700.iterasyon, Train Loss = 0.2337271623698509, Val Loss = 11.333234025292487\n",
      "280800.iterasyon, Train Loss = 0.23372147722356598, Val Loss = 11.333338725338335\n",
      "280900.iterasyon, Train Loss = 0.2337157964262465, Val Loss = 11.333443376724503\n",
      "281000.iterasyon, Train Loss = 0.2337101199726944, Val Loss = 11.333547979473451\n",
      "281100.iterasyon, Train Loss = 0.23370444785772235, Val Loss = 11.333652533607555\n",
      "281200.iterasyon, Train Loss = 0.23369878007614284, Val Loss = 11.333757039149239\n",
      "281300.iterasyon, Train Loss = 0.23369311662278458, Val Loss = 11.333861496120887\n",
      "281400.iterasyon, Train Loss = 0.2336874574924753, Val Loss = 11.333965904544911\n",
      "281500.iterasyon, Train Loss = 0.23368180268005148, Val Loss = 11.334070264443652\n",
      "281600.iterasyon, Train Loss = 0.23367615218035243, Val Loss = 11.33417457583953\n",
      "281700.iterasyon, Train Loss = 0.2336705059882286, Val Loss = 11.334278838754877\n",
      "281800.iterasyon, Train Loss = 0.23366486409853526, Val Loss = 11.334383053212093\n",
      "281900.iterasyon, Train Loss = 0.23365922650612872, Val Loss = 11.334487219233512\n",
      "282000.iterasyon, Train Loss = 0.23365359320588144, Val Loss = 11.334591336841445\n",
      "282100.iterasyon, Train Loss = 0.2336479641926603, Val Loss = 11.334695406058316\n",
      "282200.iterasyon, Train Loss = 0.23364233946134602, Val Loss = 11.334799426906397\n",
      "282300.iterasyon, Train Loss = 0.23363671900682437, Val Loss = 11.334903399408063\n",
      "282400.iterasyon, Train Loss = 0.233631102823988, Val Loss = 11.335007323585574\n",
      "282500.iterasyon, Train Loss = 0.23362549090772614, Val Loss = 11.335111199461284\n",
      "282600.iterasyon, Train Loss = 0.23361988325295316, Val Loss = 11.335215027057462\n",
      "282700.iterasyon, Train Loss = 0.23361427985457137, Val Loss = 11.335318806396467\n",
      "282800.iterasyon, Train Loss = 0.23360868070749352, Val Loss = 11.335422537500605\n",
      "282900.iterasyon, Train Loss = 0.23360308580664832, Val Loss = 11.335526220392103\n",
      "283000.iterasyon, Train Loss = 0.23359749514695963, Val Loss = 11.335629855093277\n",
      "283100.iterasyon, Train Loss = 0.23359190872335964, Val Loss = 11.335733441626347\n",
      "283200.iterasyon, Train Loss = 0.23358632653079023, Val Loss = 11.335836980013632\n",
      "283300.iterasyon, Train Loss = 0.23358074856419553, Val Loss = 11.335940470277386\n",
      "283400.iterasyon, Train Loss = 0.23357517481852896, Val Loss = 11.336043912439848\n",
      "283500.iterasyon, Train Loss = 0.2335696052887453, Val Loss = 11.336147306523278\n",
      "283600.iterasyon, Train Loss = 0.23356403996981168, Val Loss = 11.336250652549873\n",
      "283700.iterasyon, Train Loss = 0.23355847885669298, Val Loss = 11.33635395054193\n",
      "283800.iterasyon, Train Loss = 0.2335529219443709, Val Loss = 11.336457200521622\n",
      "283900.iterasyon, Train Loss = 0.23354736922782257, Val Loss = 11.336560402511203\n",
      "284000.iterasyon, Train Loss = 0.23354182070203658, Val Loss = 11.336663556532853\n",
      "284100.iterasyon, Train Loss = 0.23353627636200638, Val Loss = 11.336766662608804\n",
      "284200.iterasyon, Train Loss = 0.23353073620273435, Val Loss = 11.336869720761223\n",
      "284300.iterasyon, Train Loss = 0.23352520021922332, Val Loss = 11.336972731012327\n",
      "284400.iterasyon, Train Loss = 0.23351966840648256, Val Loss = 11.3370756933843\n",
      "284500.iterasyon, Train Loss = 0.23351414075953772, Val Loss = 11.337178607899281\n",
      "284600.iterasyon, Train Loss = 0.23350861727340153, Val Loss = 11.33728147457948\n",
      "284700.iterasyon, Train Loss = 0.23350309794311275, Val Loss = 11.33738429344703\n",
      "284800.iterasyon, Train Loss = 0.2334975827637015, Val Loss = 11.337487064524142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284900.iterasyon, Train Loss = 0.2334920717302089, Val Loss = 11.337589787832885\n",
      "285000.iterasyon, Train Loss = 0.23348656483768684, Val Loss = 11.337692463395475\n",
      "285100.iterasyon, Train Loss = 0.23348106208118402, Val Loss = 11.337795091233989\n",
      "285200.iterasyon, Train Loss = 0.23347556345575735, Val Loss = 11.337897671370587\n",
      "285300.iterasyon, Train Loss = 0.23347006895647893, Val Loss = 11.33800020382738\n",
      "285400.iterasyon, Train Loss = 0.23346457857841305, Val Loss = 11.33810268862651\n",
      "285500.iterasyon, Train Loss = 0.23345909231663972, Val Loss = 11.338205125790017\n",
      "285600.iterasyon, Train Loss = 0.23345361016624097, Val Loss = 11.338307515340091\n",
      "285700.iterasyon, Train Loss = 0.23344813212230453, Val Loss = 11.338409857298739\n",
      "285800.iterasyon, Train Loss = 0.23344265817992613, Val Loss = 11.338512151688086\n",
      "285900.iterasyon, Train Loss = 0.23343718833420424, Val Loss = 11.338614398530222\n",
      "286000.iterasyon, Train Loss = 0.2334317225802434, Val Loss = 11.338716597847224\n",
      "286100.iterasyon, Train Loss = 0.23342626091315824, Val Loss = 11.338818749661122\n",
      "286200.iterasyon, Train Loss = 0.23342080332806725, Val Loss = 11.338920853993994\n",
      "286300.iterasyon, Train Loss = 0.23341534982009146, Val Loss = 11.339022910867893\n",
      "286400.iterasyon, Train Loss = 0.23340990038436096, Val Loss = 11.339124920304858\n",
      "286500.iterasyon, Train Loss = 0.23340445501601081, Val Loss = 11.339226882326956\n",
      "286600.iterasyon, Train Loss = 0.23339901371018168, Val Loss = 11.339328796956172\n",
      "286700.iterasyon, Train Loss = 0.23339357646202008, Val Loss = 11.339430664214559\n",
      "286800.iterasyon, Train Loss = 0.23338814326668145, Val Loss = 11.339532484124092\n",
      "286900.iterasyon, Train Loss = 0.2333827141193211, Val Loss = 11.339634256706839\n",
      "287000.iterasyon, Train Loss = 0.2333772890151031, Val Loss = 11.339735981984775\n",
      "287100.iterasyon, Train Loss = 0.23337186794919962, Val Loss = 11.33983765997986\n",
      "287200.iterasyon, Train Loss = 0.23336645091678299, Val Loss = 11.339939290714158\n",
      "287300.iterasyon, Train Loss = 0.2333610379130373, Val Loss = 11.340040874209599\n",
      "287400.iterasyon, Train Loss = 0.23335562893314846, Val Loss = 11.340142410488165\n",
      "287500.iterasyon, Train Loss = 0.23335022397231045, Val Loss = 11.340243899571856\n",
      "287600.iterasyon, Train Loss = 0.23334482302572054, Val Loss = 11.340345341482598\n",
      "287700.iterasyon, Train Loss = 0.23333942608858402, Val Loss = 11.340446736242317\n",
      "287800.iterasyon, Train Loss = 0.233334033156109, Val Loss = 11.340548083873044\n",
      "287900.iterasyon, Train Loss = 0.23332864422351493, Val Loss = 11.340649384396645\n",
      "288000.iterasyon, Train Loss = 0.23332325928602005, Val Loss = 11.3407506378351\n",
      "288100.iterasyon, Train Loss = 0.2333178783388561, Val Loss = 11.340851844210292\n",
      "288200.iterasyon, Train Loss = 0.23331250137725024, Val Loss = 11.340953003544174\n",
      "288300.iterasyon, Train Loss = 0.2333071283964463, Val Loss = 11.341054115858602\n",
      "288400.iterasyon, Train Loss = 0.23330175939168396, Val Loss = 11.341155181175543\n",
      "288500.iterasyon, Train Loss = 0.2332963943582174, Val Loss = 11.341256199516867\n",
      "288600.iterasyon, Train Loss = 0.233291033291299, Val Loss = 11.341357170904464\n",
      "288700.iterasyon, Train Loss = 0.23328567618619264, Val Loss = 11.341458095360226\n",
      "288800.iterasyon, Train Loss = 0.23328032303816407, Val Loss = 11.341558972906034\n",
      "288900.iterasyon, Train Loss = 0.23327497384248605, Val Loss = 11.341659803563756\n",
      "289000.iterasyon, Train Loss = 0.23326962859443354, Val Loss = 11.341760587355239\n",
      "289100.iterasyon, Train Loss = 0.2332642872892976, Val Loss = 11.34186132430233\n",
      "289200.iterasyon, Train Loss = 0.23325894992236337, Val Loss = 11.341962014426917\n",
      "289300.iterasyon, Train Loss = 0.23325361648892717, Val Loss = 11.3420626577508\n",
      "289400.iterasyon, Train Loss = 0.23324828698428854, Val Loss = 11.34216325429582\n",
      "289500.iterasyon, Train Loss = 0.23324296140375425, Val Loss = 11.342263804083863\n",
      "289600.iterasyon, Train Loss = 0.23323763974263892, Val Loss = 11.34236430713667\n",
      "289700.iterasyon, Train Loss = 0.23323232199625654, Val Loss = 11.342464763476082\n",
      "289800.iterasyon, Train Loss = 0.23322700815993083, Val Loss = 11.34256517312394\n",
      "289900.iterasyon, Train Loss = 0.2332216982289921, Val Loss = 11.342665536101995\n",
      "290000.iterasyon, Train Loss = 0.23321639219877763, Val Loss = 11.34276585243204\n",
      "290100.iterasyon, Train Loss = 0.23321109006462, Val Loss = 11.3428661221359\n",
      "290200.iterasyon, Train Loss = 0.23320579182187096, Val Loss = 11.342966345235315\n",
      "290300.iterasyon, Train Loss = 0.23320049746587837, Val Loss = 11.343066521752078\n",
      "290400.iterasyon, Train Loss = 0.23319520699200227, Val Loss = 11.34316665170794\n",
      "290500.iterasyon, Train Loss = 0.23318992039560013, Val Loss = 11.343266735124676\n",
      "290600.iterasyon, Train Loss = 0.23318463767204145, Val Loss = 11.343366772024002\n",
      "290700.iterasyon, Train Loss = 0.2331793588167036, Val Loss = 11.343466762427683\n",
      "290800.iterasyon, Train Loss = 0.2331740838249604, Val Loss = 11.343566706357429\n",
      "290900.iterasyon, Train Loss = 0.23316881269219894, Val Loss = 11.343666603834981\n",
      "291000.iterasyon, Train Loss = 0.23316354541380788, Val Loss = 11.343766454882058\n",
      "291100.iterasyon, Train Loss = 0.23315828198518043, Val Loss = 11.343866259520446\n",
      "291200.iterasyon, Train Loss = 0.23315302240172145, Val Loss = 11.34396601777174\n",
      "291300.iterasyon, Train Loss = 0.2331477666588372, Val Loss = 11.34406572965766\n",
      "291400.iterasyon, Train Loss = 0.23314251475193545, Val Loss = 11.34416539519993\n",
      "291500.iterasyon, Train Loss = 0.23313726667643597, Val Loss = 11.344265014420255\n",
      "291600.iterasyon, Train Loss = 0.23313202242776285, Val Loss = 11.344364587340293\n",
      "291700.iterasyon, Train Loss = 0.2331267820013416, Val Loss = 11.34446411398173\n",
      "291800.iterasyon, Train Loss = 0.23312154539260832, Val Loss = 11.344563594366166\n",
      "291900.iterasyon, Train Loss = 0.2331163125970012, Val Loss = 11.344663028515312\n",
      "292000.iterasyon, Train Loss = 0.23311108360996283, Val Loss = 11.344762416450838\n",
      "292100.iterasyon, Train Loss = 0.23310585842694723, Val Loss = 11.34486175819435\n",
      "292200.iterasyon, Train Loss = 0.2331006370434078, Val Loss = 11.344961053767493\n",
      "292300.iterasyon, Train Loss = 0.23309541945480716, Val Loss = 11.345060303191874\n",
      "292400.iterasyon, Train Loss = 0.23309020565660685, Val Loss = 11.34515950648916\n",
      "292500.iterasyon, Train Loss = 0.233084995644283, Val Loss = 11.345258663680918\n",
      "292600.iterasyon, Train Loss = 0.23307978941331287, Val Loss = 11.345357774788786\n",
      "292700.iterasyon, Train Loss = 0.23307458695917616, Val Loss = 11.345456839834364\n",
      "292800.iterasyon, Train Loss = 0.23306938827736634, Val Loss = 11.345555858839226\n",
      "292900.iterasyon, Train Loss = 0.23306419336336862, Val Loss = 11.34565483182498\n",
      "293000.iterasyon, Train Loss = 0.2330590022126877, Val Loss = 11.3457537588132\n",
      "293100.iterasyon, Train Loss = 0.2330538148208293, Val Loss = 11.345852639825415\n",
      "293200.iterasyon, Train Loss = 0.23304863118329577, Val Loss = 11.345951474883229\n",
      "293300.iterasyon, Train Loss = 0.23304345129560888, Val Loss = 11.346050264008191\n",
      "293400.iterasyon, Train Loss = 0.233038275153284, Val Loss = 11.346149007221868\n",
      "293500.iterasyon, Train Loss = 0.23303310275184905, Val Loss = 11.346247704545803\n",
      "293600.iterasyon, Train Loss = 0.23302793408683764, Val Loss = 11.346346356001511\n",
      "293700.iterasyon, Train Loss = 0.23302276915378192, Val Loss = 11.346444961610532\n",
      "293800.iterasyon, Train Loss = 0.23301760794822238, Val Loss = 11.346543521394402\n",
      "293900.iterasyon, Train Loss = 0.2330124504657093, Val Loss = 11.346642035374586\n",
      "294000.iterasyon, Train Loss = 0.23300729670179612, Val Loss = 11.346740503572606\n",
      "294100.iterasyon, Train Loss = 0.2330021466520363, Val Loss = 11.346838926009987\n",
      "294200.iterasyon, Train Loss = 0.23299700031199547, Val Loss = 11.346937302708247\n",
      "294300.iterasyon, Train Loss = 0.2329918576772423, Val Loss = 11.3470356336888\n",
      "294400.iterasyon, Train Loss = 0.23298671874334684, Val Loss = 11.347133918973183\n",
      "294500.iterasyon, Train Loss = 0.2329815835058929, Val Loss = 11.347232158582841\n",
      "294600.iterasyon, Train Loss = 0.2329764519604601, Val Loss = 11.347330352539243\n",
      "294700.iterasyon, Train Loss = 0.2329713241026394, Val Loss = 11.347428500863879\n",
      "294800.iterasyon, Train Loss = 0.23296619992802456, Val Loss = 11.34752660357815\n",
      "294900.iterasyon, Train Loss = 0.2329610794322181, Val Loss = 11.347624660703527\n",
      "295000.iterasyon, Train Loss = 0.23295596261082271, Val Loss = 11.347722672261419\n",
      "295100.iterasyon, Train Loss = 0.2329508494594491, Val Loss = 11.347820638273268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295200.iterasyon, Train Loss = 0.23294573997371462, Val Loss = 11.347918558760497\n",
      "295300.iterasyon, Train Loss = 0.2329406341492389, Val Loss = 11.34801643374452\n",
      "295400.iterasyon, Train Loss = 0.2329355319816473, Val Loss = 11.348114263246755\n",
      "295500.iterasyon, Train Loss = 0.2329304334665719, Val Loss = 11.348212047288568\n",
      "295600.iterasyon, Train Loss = 0.23292533859964973, Val Loss = 11.348309785891411\n",
      "295700.iterasyon, Train Loss = 0.23292024737652162, Val Loss = 11.348407479076615\n",
      "295800.iterasyon, Train Loss = 0.23291515979283603, Val Loss = 11.348505126865556\n",
      "295900.iterasyon, Train Loss = 0.23291007584424325, Val Loss = 11.348602729279655\n",
      "296000.iterasyon, Train Loss = 0.23290499552640032, Val Loss = 11.348700286340264\n",
      "296100.iterasyon, Train Loss = 0.23289991883497432, Val Loss = 11.348797798068688\n",
      "296200.iterasyon, Train Loss = 0.23289484576562855, Val Loss = 11.34889526448631\n",
      "296300.iterasyon, Train Loss = 0.2328897763140382, Val Loss = 11.348992685614451\n",
      "296400.iterasyon, Train Loss = 0.23288471047588075, Val Loss = 11.349090061474493\n",
      "296500.iterasyon, Train Loss = 0.2328796482468366, Val Loss = 11.349187392087742\n",
      "296600.iterasyon, Train Loss = 0.23287458962259833, Val Loss = 11.349284677475518\n",
      "296700.iterasyon, Train Loss = 0.2328695345988597, Val Loss = 11.349381917659107\n",
      "296800.iterasyon, Train Loss = 0.23286448317131625, Val Loss = 11.349479112659857\n",
      "296900.iterasyon, Train Loss = 0.23285943533567335, Val Loss = 11.349576262499017\n",
      "297000.iterasyon, Train Loss = 0.23285439108764047, Val Loss = 11.349673367197964\n",
      "297100.iterasyon, Train Loss = 0.2328493504229311, Val Loss = 11.349770426777905\n",
      "297200.iterasyon, Train Loss = 0.23284431333726674, Val Loss = 11.349867441260114\n",
      "297300.iterasyon, Train Loss = 0.23283927982636854, Val Loss = 11.349964410665901\n",
      "297400.iterasyon, Train Loss = 0.23283424988596743, Val Loss = 11.350061335016509\n",
      "297500.iterasyon, Train Loss = 0.23282922351179727, Val Loss = 11.35015821433321\n",
      "297600.iterasyon, Train Loss = 0.2328242006996, Val Loss = 11.35025504863723\n",
      "297700.iterasyon, Train Loss = 0.2328191814451188, Val Loss = 11.350351837949841\n",
      "297800.iterasyon, Train Loss = 0.2328141657441026, Val Loss = 11.350448582292263\n",
      "297900.iterasyon, Train Loss = 0.2328091535923086, Val Loss = 11.3505452816857\n",
      "298000.iterasyon, Train Loss = 0.23280414498549773, Val Loss = 11.350641936151401\n",
      "298100.iterasyon, Train Loss = 0.2327991399194307, Val Loss = 11.350738545710549\n",
      "298200.iterasyon, Train Loss = 0.23279413838988017, Val Loss = 11.350835110384368\n",
      "298300.iterasyon, Train Loss = 0.23278914039262313, Val Loss = 11.350931630194076\n",
      "298400.iterasyon, Train Loss = 0.2327841459234388, Val Loss = 11.351028105160818\n",
      "298500.iterasyon, Train Loss = 0.23277915497811028, Val Loss = 11.351124535305818\n",
      "298600.iterasyon, Train Loss = 0.23277416755243122, Val Loss = 11.351220920650222\n",
      "298700.iterasyon, Train Loss = 0.23276918364219387, Val Loss = 11.351317261215222\n",
      "298800.iterasyon, Train Loss = 0.23276420324320016, Val Loss = 11.351413557021978\n",
      "298900.iterasyon, Train Loss = 0.23275922635125726, Val Loss = 11.351509808091635\n",
      "299000.iterasyon, Train Loss = 0.2327542529621747, Val Loss = 11.351606014445338\n",
      "299100.iterasyon, Train Loss = 0.2327492830717662, Val Loss = 11.351702176104219\n",
      "299200.iterasyon, Train Loss = 0.2327443166758536, Val Loss = 11.35179829308943\n",
      "299300.iterasyon, Train Loss = 0.23273935377026275, Val Loss = 11.351894365422105\n",
      "299400.iterasyon, Train Loss = 0.2327343943508241, Val Loss = 11.351990393123312\n",
      "299500.iterasyon, Train Loss = 0.23272943841337335, Val Loss = 11.352086376214196\n",
      "299600.iterasyon, Train Loss = 0.23272448595374817, Val Loss = 11.352182314715867\n",
      "299700.iterasyon, Train Loss = 0.23271953696780082, Val Loss = 11.352278208649407\n",
      "299800.iterasyon, Train Loss = 0.23271459145137174, Val Loss = 11.352374058035933\n",
      "299900.iterasyon, Train Loss = 0.23270964940032438, Val Loss = 11.352469862896486\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterasyon):\n",
    "    for j in range(x_train.shape[0]):\n",
    "        thetas = gradient_descent_reg(thetas,x_train[j:j+1],y_train[j:j+1],lr=0.3,\n",
    "                                      reg_katsayi=reg_values[son_losslar_train.index(min(son_losslar_train))])\n",
    "    train_losses.append(loss_hesapla_reg(thetas,x_train,y_train,\n",
    "                                      reg_katsayi=reg_values[son_losslar_train.index(min(son_losslar_train))]))\n",
    "    val_losses.append(loss_hesapla(thetas,x_val,y_val))\n",
    "    if(i%100 == 0):\n",
    "        train_loss = loss_hesapla_reg(thetas,x_train,y_train,\n",
    "                                      reg_katsayi=reg_values[son_losslar_train.index(min(son_losslar_train))])\n",
    "        val_loss = loss_hesapla(thetas,x_val,y_val)\n",
    "        print(\"{0}.iterasyon, Train Loss = {1}, Val Loss = {2}\".format(i,train_loss,val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.iterasyon, Train Loss = 0.23042247159550877, Val Loss = 11.396013843790522\n",
      "100.iterasyon, Train Loss = 0.22076635869248873, Val Loss = 11.753204115854478\n",
      "200.iterasyon, Train Loss = 0.22010633917189704, Val Loss = 11.757288246521222\n",
      "300.iterasyon, Train Loss = 0.2196048217227549, Val Loss = 11.758251335979388\n",
      "400.iterasyon, Train Loss = 0.2192047165111879, Val Loss = 11.7584469678868\n",
      "500.iterasyon, Train Loss = 0.21888345680859475, Val Loss = 11.758448513181229\n",
      "600.iterasyon, Train Loss = 0.21862480550460353, Val Loss = 11.758392963433714\n",
      "700.iterasyon, Train Loss = 0.2184160294031752, Val Loss = 11.75831179545288\n",
      "800.iterasyon, Train Loss = 0.21824702477801924, Val Loss = 11.758211612310102\n",
      "900.iterasyon, Train Loss = 0.21810975420963177, Val Loss = 11.758093596183452\n",
      "1000.iterasyon, Train Loss = 0.21799781890219394, Val Loss = 11.757958075176996\n",
      "1100.iterasyon, Train Loss = 0.217906122717616, Val Loss = 11.757805499823448\n",
      "1200.iterasyon, Train Loss = 0.21783060591111023, Val Loss = 11.75763657640742\n",
      "1300.iterasyon, Train Loss = 0.21776803355067076, Val Loss = 11.757452220112791\n",
      "1400.iterasyon, Train Loss = 0.2177158272484755, Val Loss = 11.757253484069524\n",
      "1500.iterasyon, Train Loss = 0.21767193129778536, Val Loss = 11.757041497917376\n",
      "1600.iterasyon, Train Loss = 0.21763470616546535, Val Loss = 11.756817420586424\n",
      "1700.iterasyon, Train Loss = 0.21760284374163086, Val Loss = 11.756582405694338\n",
      "1800.iterasyon, Train Loss = 0.21757529989575583, Val Loss = 11.756337577020439\n",
      "1900.iterasyon, Train Loss = 0.21755124080001656, Val Loss = 11.756084011756242\n",
      "2000.iterasyon, Train Loss = 0.21753000020503352, Val Loss = 11.755822729663626\n",
      "2100.iterasyon, Train Loss = 0.21751104542923083, Val Loss = 11.755554686665873\n",
      "2200.iterasyon, Train Loss = 0.2174939502810512, Val Loss = 11.755280771726273\n",
      "2300.iterasyon, Train Loss = 0.21747837349763768, Val Loss = 11.755001806126115\n",
      "2400.iterasyon, Train Loss = 0.21746404157325164, Val Loss = 11.754718544459813\n",
      "2500.iterasyon, Train Loss = 0.21745073508117796, Val Loss = 11.754431676825996\n",
      "2600.iterasyon, Train Loss = 0.21743827777611327, Val Loss = 11.754141831816423\n",
      "2700.iterasyon, Train Loss = 0.21742652790977737, Val Loss = 11.753849580005458\n",
      "2800.iterasyon, Train Loss = 0.21741537130849298, Val Loss = 11.75355543771586\n",
      "2900.iterasyon, Train Loss = 0.21740471585360754, Val Loss = 11.753259870897272\n",
      "3000.iterasyon, Train Loss = 0.21739448707905606, Val Loss = 11.752963298998063\n",
      "3100.iterasyon, Train Loss = 0.21738462465868366, Val Loss = 11.752666098746955\n",
      "3200.iterasyon, Train Loss = 0.21737507960236302, Val Loss = 11.752368607785906\n",
      "3300.iterasyon, Train Loss = 0.21736581201687086, Val Loss = 11.752071128117581\n",
      "3400.iterasyon, Train Loss = 0.21735678931687188, Val Loss = 11.751773929344129\n",
      "3500.iterasyon, Train Loss = 0.21734798479474307, Val Loss = 11.751477251686389\n",
      "3600.iterasyon, Train Loss = 0.21733937647651955, Val Loss = 11.751181308780277\n",
      "3700.iterasyon, Train Loss = 0.21733094620611274, Val Loss = 11.750886290252982\n",
      "3800.iterasyon, Train Loss = 0.2173226789116899, Val Loss = 11.75059236408595\n",
      "3900.iterasyon, Train Loss = 0.2173145620174608, Val Loss = 11.750299678773942\n",
      "4000.iterasyon, Train Loss = 0.2173065849716307, Val Loss = 11.750008365291789\n",
      "4100.iterasyon, Train Loss = 0.21729873886716525, Val Loss = 11.749718538881055\n",
      "4200.iterasyon, Train Loss = 0.21729101613678736, Val Loss = 11.749430300669038\n",
      "4300.iterasyon, Train Loss = 0.21728341030735276, Val Loss = 11.74914373913348\n",
      "4400.iterasyon, Train Loss = 0.21727591580177383, Val Loss = 11.748858931424735\n",
      "4500.iterasyon, Train Loss = 0.21726852777904862, Val Loss = 11.748575944557828\n",
      "4600.iterasyon, Train Loss = 0.21726124200484834, Val Loss = 11.748294836485572\n",
      "4700.iterasyon, Train Loss = 0.21725405474664392, Val Loss = 11.748015657064006\n",
      "4800.iterasyon, Train Loss = 0.21724696268852797, Val Loss = 11.747738448919053\n",
      "4900.iterasyon, Train Loss = 0.21723996286192127, Val Loss = 11.7474632482251\n",
      "5000.iterasyon, Train Loss = 0.21723305258905223, Val Loss = 11.747190085402519\n",
      "5100.iterasyon, Train Loss = 0.2172262294367333, Val Loss = 11.746918985743513\n",
      "5200.iterasyon, Train Loss = 0.21721949117848946, Val Loss = 11.746649969972054\n",
      "5300.iterasyon, Train Loss = 0.21721283576340691, Val Loss = 11.746383054745458\n",
      "5400.iterasyon, Train Loss = 0.21720626129046358, Val Loss = 11.746118253102999\n",
      "5500.iterasyon, Train Loss = 0.21719976598728966, Val Loss = 11.745855574867173\n",
      "5600.iterasyon, Train Loss = 0.21719334819255265, Val Loss = 11.745595027002373\n",
      "5700.iterasyon, Train Loss = 0.2171870063412794, Val Loss = 11.745336613935331\n",
      "5800.iterasyon, Train Loss = 0.21718073895260603, Val Loss = 11.745080337841408\n",
      "5900.iterasyon, Train Loss = 0.21717454461948624, Val Loss = 11.744826198900618\n",
      "6000.iterasyon, Train Loss = 0.21716842200004605, Val Loss = 11.744574195525354\n",
      "6100.iterasyon, Train Loss = 0.21716236981026757, Val Loss = 11.744324324564726\n",
      "6200.iterasyon, Train Loss = 0.21715638681777896, Val Loss = 11.74407658148598\n",
      "6300.iterasyon, Train Loss = 0.21715047183657996, Val Loss = 11.743830960536938\n",
      "6400.iterasyon, Train Loss = 0.21714462372251006, Val Loss = 11.743587454890589\n",
      "6500.iterasyon, Train Loss = 0.2171388413693789, Val Loss = 11.743346056774177\n",
      "6600.iterasyon, Train Loss = 0.21713312370561402, Val Loss = 11.743106757584247\n",
      "6700.iterasyon, Train Loss = 0.21712746969136884, Val Loss = 11.742869547989143\n",
      "6800.iterasyon, Train Loss = 0.21712187831600824, Val Loss = 11.742634418020502\n",
      "6900.iterasyon, Train Loss = 0.21711634859591625, Val Loss = 11.742401357154606\n",
      "7000.iterasyon, Train Loss = 0.2171108795725718, Val Loss = 11.742170354384996\n",
      "7100.iterasyon, Train Loss = 0.2171054703108618, Val Loss = 11.741941398287109\n",
      "7200.iterasyon, Train Loss = 0.21710011989759626, Val Loss = 11.741714477075739\n",
      "7300.iterasyon, Train Loss = 0.2170948274401844, Val Loss = 11.741489578656351\n",
      "7400.iterasyon, Train Loss = 0.21708959206546505, Val Loss = 11.741266690670612\n",
      "7500.iterasyon, Train Loss = 0.21708441291866995, Val Loss = 11.741045800537064\n",
      "7600.iterasyon, Train Loss = 0.21707928916247768, Val Loss = 11.740826895487029\n",
      "7700.iterasyon, Train Loss = 0.21707421997618384, Val Loss = 11.740609962596977\n",
      "7800.iterasyon, Train Loss = 0.21706920455494358, Val Loss = 11.740394988816627\n",
      "7900.iterasyon, Train Loss = 0.21706424210908556, Val Loss = 11.74018196099473\n",
      "8000.iterasyon, Train Loss = 0.21705933186350101, Val Loss = 11.73997086590137\n",
      "8100.iterasyon, Train Loss = 0.21705447305707104, Val Loss = 11.739761690247894\n",
      "8200.iterasyon, Train Loss = 0.2170496649421592, Val Loss = 11.739554420704682\n",
      "8300.iterasyon, Train Loss = 0.21704490678414157, Val Loss = 11.739349043916983\n",
      "8400.iterasyon, Train Loss = 0.21704019786097126, Val Loss = 11.739145546518671\n",
      "8500.iterasyon, Train Loss = 0.21703553746278278, Val Loss = 11.738943915144622\n",
      "8600.iterasyon, Train Loss = 0.21703092489152542, Val Loss = 11.738744136441706\n",
      "8700.iterasyon, Train Loss = 0.21702635946061932, Val Loss = 11.738546197078255\n",
      "8800.iterasyon, Train Loss = 0.21702184049464204, Val Loss = 11.738350083752824\n",
      "8900.iterasyon, Train Loss = 0.21701736732902846, Val Loss = 11.738155783201451\n",
      "9000.iterasyon, Train Loss = 0.2170129393098054, Val Loss = 11.737963282204241\n",
      "9100.iterasyon, Train Loss = 0.21700855579331466, Val Loss = 11.737772567591504\n",
      "9200.iterasyon, Train Loss = 0.21700421614598908, Val Loss = 11.737583626248432\n",
      "9300.iterasyon, Train Loss = 0.2169999197441078, Val Loss = 11.737396445119938\n",
      "9400.iterasyon, Train Loss = 0.2169956659735841, Val Loss = 11.737211011214134\n",
      "9500.iterasyon, Train Loss = 0.21699145422976376, Val Loss = 11.737027311606125\n",
      "9600.iterasyon, Train Loss = 0.21698728391722136, Val Loss = 11.736845333440609\n",
      "9700.iterasyon, Train Loss = 0.2169831544495797, Val Loss = 11.736665063934813\n",
      "9800.iterasyon, Train Loss = 0.2169790652493269, Val Loss = 11.736486490380255\n",
      "9900.iterasyon, Train Loss = 0.21697501574765476, Val Loss = 11.73630960014488\n",
      "10000.iterasyon, Train Loss = 0.21697100538427963, Val Loss = 11.736134380674601\n",
      "10100.iterasyon, Train Loss = 0.21696703360730418, Val Loss = 11.735960819494617\n",
      "10200.iterasyon, Train Loss = 0.21696309987304896, Val Loss = 11.73578890421056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10300.iterasyon, Train Loss = 0.21695920364592158, Val Loss = 11.73561862250937\n",
      "10400.iterasyon, Train Loss = 0.21695534439826103, Val Loss = 11.735449962160146\n",
      "10500.iterasyon, Train Loss = 0.2169515216102179, Val Loss = 11.735282911014611\n",
      "10600.iterasyon, Train Loss = 0.2169477347696048, Val Loss = 11.735117457007622\n",
      "10700.iterasyon, Train Loss = 0.21694398337178364, Val Loss = 11.73495358815757\n",
      "10800.iterasyon, Train Loss = 0.21694026691952897, Val Loss = 11.734791292566538\n",
      "10900.iterasyon, Train Loss = 0.21693658492291598, Val Loss = 11.734630558420346\n",
      "11000.iterasyon, Train Loss = 0.2169329368991982, Val Loss = 11.734471373988749\n",
      "11100.iterasyon, Train Loss = 0.2169293223726911, Val Loss = 11.734313727625278\n",
      "11200.iterasyon, Train Loss = 0.21692574087466607, Val Loss = 11.73415760776732\n",
      "11300.iterasyon, Train Loss = 0.21692219194323178, Val Loss = 11.734003002935687\n",
      "11400.iterasyon, Train Loss = 0.21691867512323837, Val Loss = 11.733849901734652\n",
      "11500.iterasyon, Train Loss = 0.21691518996616457, Val Loss = 11.733698292851576\n",
      "11600.iterasyon, Train Loss = 0.21691173603001557, Val Loss = 11.733548165056487\n",
      "11700.iterasyon, Train Loss = 0.21690831287922777, Val Loss = 11.733399507202053\n",
      "11800.iterasyon, Train Loss = 0.21690492008456522, Val Loss = 11.733252308223033\n",
      "11900.iterasyon, Train Loss = 0.21690155722302906, Val Loss = 11.733106557135839\n",
      "12000.iterasyon, Train Loss = 0.21689822387775648, Val Loss = 11.732962243038216\n",
      "12100.iterasyon, Train Loss = 0.21689491963793395, Val Loss = 11.732819355108742\n",
      "12200.iterasyon, Train Loss = 0.2168916440987011, Val Loss = 11.732677882606485\n",
      "12300.iterasyon, Train Loss = 0.21688839686106673, Val Loss = 11.732537814870454\n",
      "12400.iterasyon, Train Loss = 0.21688517753181766, Val Loss = 11.732399141319007\n",
      "12500.iterasyon, Train Loss = 0.21688198572343334, Val Loss = 11.732261851449682\n",
      "12600.iterasyon, Train Loss = 0.21687882105400047, Val Loss = 11.732125934838292\n",
      "12700.iterasyon, Train Loss = 0.21687568314713132, Val Loss = 11.731991381138705\n",
      "12800.iterasyon, Train Loss = 0.21687257163188056, Val Loss = 11.731858180082263\n",
      "12900.iterasyon, Train Loss = 0.2168694861426644, Val Loss = 11.731726321477227\n",
      "13000.iterasyon, Train Loss = 0.21686642631918396, Val Loss = 11.731595795208165\n",
      "13100.iterasyon, Train Loss = 0.21686339180634592, Val Loss = 11.731466591235716\n",
      "13200.iterasyon, Train Loss = 0.21686038225418414, Val Loss = 11.731338699595659\n",
      "13300.iterasyon, Train Loss = 0.2168573973177859, Val Loss = 11.731212110398673\n",
      "13400.iterasyon, Train Loss = 0.21685443665721796, Val Loss = 11.73108681382966\n",
      "13500.iterasyon, Train Loss = 0.21685149993745179, Val Loss = 11.730962800147433\n",
      "13600.iterasyon, Train Loss = 0.21684858682829566, Val Loss = 11.730840059683771\n",
      "13700.iterasyon, Train Loss = 0.21684569700431913, Val Loss = 11.730718582843291\n",
      "13800.iterasyon, Train Loss = 0.21684283014478503, Val Loss = 11.730598360102674\n",
      "13900.iterasyon, Train Loss = 0.21683998593358114, Val Loss = 11.73047938201024\n",
      "14000.iterasyon, Train Loss = 0.21683716405915351, Val Loss = 11.730361639185372\n",
      "14100.iterasyon, Train Loss = 0.21683436421443594, Val Loss = 11.730245122318005\n",
      "14200.iterasyon, Train Loss = 0.21683158609679232, Val Loss = 11.730129822168117\n",
      "14300.iterasyon, Train Loss = 0.21682882940794282, Val Loss = 11.730015729565206\n",
      "14400.iterasyon, Train Loss = 0.21682609385390503, Val Loss = 11.72990283540756\n",
      "14500.iterasyon, Train Loss = 0.21682337914493333, Val Loss = 11.729791130662214\n",
      "14600.iterasyon, Train Loss = 0.21682068499545074, Val Loss = 11.729680606363894\n",
      "14700.iterasyon, Train Loss = 0.2168180111239961, Val Loss = 11.72957125361493\n",
      "14800.iterasyon, Train Loss = 0.21681535725315523, Val Loss = 11.72946306358443\n",
      "14900.iterasyon, Train Loss = 0.21681272310951116, Val Loss = 11.729356027508052\n",
      "15000.iterasyon, Train Loss = 0.21681010842357998, Val Loss = 11.729250136687243\n",
      "15100.iterasyon, Train Loss = 0.21680751292975353, Val Loss = 11.729145382488968\n",
      "15200.iterasyon, Train Loss = 0.21680493636624787, Val Loss = 11.729041756344937\n",
      "15300.iterasyon, Train Loss = 0.21680237847504158, Val Loss = 11.728939249751507\n",
      "15400.iterasyon, Train Loss = 0.21679983900182562, Val Loss = 11.728837854268722\n",
      "15500.iterasyon, Train Loss = 0.21679731769594554, Val Loss = 11.728737561520271\n",
      "15600.iterasyon, Train Loss = 0.21679481431035405, Val Loss = 11.728638363192598\n",
      "15700.iterasyon, Train Loss = 0.21679232860155143, Val Loss = 11.728540251034786\n",
      "15800.iterasyon, Train Loss = 0.21678986032953707, Val Loss = 11.72844321685786\n",
      "15900.iterasyon, Train Loss = 0.21678740925776388, Val Loss = 11.728347252534347\n",
      "16000.iterasyon, Train Loss = 0.21678497515307693, Val Loss = 11.728252349997875\n",
      "16100.iterasyon, Train Loss = 0.21678255778567637, Val Loss = 11.728158501242643\n",
      "16200.iterasyon, Train Loss = 0.21678015692905753, Val Loss = 11.72806569832295\n",
      "16300.iterasyon, Train Loss = 0.21677777235997178, Val Loss = 11.727973933352793\n",
      "16400.iterasyon, Train Loss = 0.21677540385837427, Val Loss = 11.727883198505339\n",
      "16500.iterasyon, Train Loss = 0.21677305120738033, Val Loss = 11.727793486012635\n",
      "16600.iterasyon, Train Loss = 0.21677071419321833, Val Loss = 11.727704788164893\n",
      "16700.iterasyon, Train Loss = 0.21676839260517985, Val Loss = 11.72761709731027\n",
      "16800.iterasyon, Train Loss = 0.21676608623558533, Val Loss = 11.727530405854347\n",
      "16900.iterasyon, Train Loss = 0.21676379487973232, Val Loss = 11.727444706259698\n",
      "17000.iterasyon, Train Loss = 0.2167615183358537, Val Loss = 11.727359991045436\n",
      "17100.iterasyon, Train Loss = 0.21675925640507707, Val Loss = 11.727276252786716\n",
      "17200.iterasyon, Train Loss = 0.21675700889137928, Val Loss = 11.727193484114547\n",
      "17300.iterasyon, Train Loss = 0.21675477560154968, Val Loss = 11.727111677715069\n",
      "17400.iterasyon, Train Loss = 0.2167525563451488, Val Loss = 11.727030826329303\n",
      "17500.iterasyon, Train Loss = 0.21675035093446005, Val Loss = 11.726950922752692\n",
      "17600.iterasyon, Train Loss = 0.21674815918445953, Val Loss = 11.726871959834778\n",
      "17700.iterasyon, Train Loss = 0.21674598091277877, Val Loss = 11.726793930478541\n",
      "17800.iterasyon, Train Loss = 0.21674381593965228, Val Loss = 11.726716827640333\n",
      "17900.iterasyon, Train Loss = 0.2167416640878945, Val Loss = 11.726640644329176\n",
      "18000.iterasyon, Train Loss = 0.2167395251828567, Val Loss = 11.726565373606427\n",
      "18100.iterasyon, Train Loss = 0.21673739905238898, Val Loss = 11.726491008585581\n",
      "18200.iterasyon, Train Loss = 0.21673528552680457, Val Loss = 11.72641754243163\n",
      "18300.iterasyon, Train Loss = 0.21673318443884512, Val Loss = 11.726344968360742\n",
      "18400.iterasyon, Train Loss = 0.2167310956236487, Val Loss = 11.726273279639948\n",
      "18500.iterasyon, Train Loss = 0.21672901891870547, Val Loss = 11.726202469586651\n",
      "18600.iterasyon, Train Loss = 0.21672695416383625, Val Loss = 11.726132531568354\n",
      "18700.iterasyon, Train Loss = 0.21672490120114452, Val Loss = 11.726063459002106\n",
      "18800.iterasyon, Train Loss = 0.21672285987499895, Val Loss = 11.725995245354303\n",
      "18900.iterasyon, Train Loss = 0.21672083003198742, Val Loss = 11.725927884140281\n",
      "19000.iterasyon, Train Loss = 0.21671881152089265, Val Loss = 11.725861368923827\n",
      "19100.iterasyon, Train Loss = 0.21671680419265663, Val Loss = 11.725795693316941\n",
      "19200.iterasyon, Train Loss = 0.21671480790034922, Val Loss = 11.725730850979387\n",
      "19300.iterasyon, Train Loss = 0.21671282249914295, Val Loss = 11.725666835618341\n",
      "19400.iterasyon, Train Loss = 0.21671084784627273, Val Loss = 11.725603640988155\n",
      "19500.iterasyon, Train Loss = 0.2167088838010182, Val Loss = 11.725541260889761\n",
      "19600.iterasyon, Train Loss = 0.21670693022465967, Val Loss = 11.725479689170497\n",
      "19700.iterasyon, Train Loss = 0.21670498698046206, Val Loss = 11.725418919723792\n",
      "19800.iterasyon, Train Loss = 0.21670305393364012, Val Loss = 11.725358946488608\n",
      "19900.iterasyon, Train Loss = 0.21670113095132945, Val Loss = 11.725299763449293\n",
      "20000.iterasyon, Train Loss = 0.2166992179025653, Val Loss = 11.725241364635128\n",
      "20100.iterasyon, Train Loss = 0.21669731465824138, Val Loss = 11.725183744119999\n",
      "20200.iterasyon, Train Loss = 0.21669542109110043, Val Loss = 11.725126896022193\n",
      "20300.iterasyon, Train Loss = 0.2166935370756928, Val Loss = 11.725070814503821\n",
      "20400.iterasyon, Train Loss = 0.21669166248835992, Val Loss = 11.72501549377068\n",
      "20500.iterasyon, Train Loss = 0.2166897972072024, Val Loss = 11.724960928071804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20600.iterasyon, Train Loss = 0.21668794111205694, Val Loss = 11.724907111699242\n",
      "20700.iterasyon, Train Loss = 0.21668609408447378, Val Loss = 11.724854038987678\n",
      "20800.iterasyon, Train Loss = 0.21668425600768615, Val Loss = 11.724801704314064\n",
      "20900.iterasyon, Train Loss = 0.21668242676658903, Val Loss = 11.72475010209733\n",
      "21000.iterasyon, Train Loss = 0.21668060624771815, Val Loss = 11.724699226798124\n",
      "21100.iterasyon, Train Loss = 0.21667879433921944, Val Loss = 11.72464907291849\n",
      "21200.iterasyon, Train Loss = 0.2166769909308329, Val Loss = 11.724599635001482\n",
      "21300.iterasyon, Train Loss = 0.21667519591386256, Val Loss = 11.72455090763076\n",
      "21400.iterasyon, Train Loss = 0.21667340918116196, Val Loss = 11.724502885430649\n",
      "21500.iterasyon, Train Loss = 0.21667163062710307, Val Loss = 11.724455563065385\n",
      "21600.iterasyon, Train Loss = 0.2166698601475614, Val Loss = 11.72440893523914\n",
      "21700.iterasyon, Train Loss = 0.21666809763988962, Val Loss = 11.724362996695598\n",
      "21800.iterasyon, Train Loss = 0.21666634300289775, Val Loss = 11.724317742217583\n",
      "21900.iterasyon, Train Loss = 0.21666459613683628, Val Loss = 11.724273166626919\n",
      "22000.iterasyon, Train Loss = 0.21666285694336498, Val Loss = 11.72422926478397\n",
      "22100.iterasyon, Train Loss = 0.2166611253255443, Val Loss = 11.724186031587541\n",
      "22200.iterasyon, Train Loss = 0.21665940118780705, Val Loss = 11.724143461974462\n",
      "22300.iterasyon, Train Loss = 0.21665768443594322, Val Loss = 11.724101550919197\n",
      "22400.iterasyon, Train Loss = 0.21665597497707623, Val Loss = 11.724060293433922\n",
      "22500.iterasyon, Train Loss = 0.21665427271964566, Val Loss = 11.724019684567702\n",
      "22600.iterasyon, Train Loss = 0.2166525775733936, Val Loss = 11.723979719406753\n",
      "22700.iterasyon, Train Loss = 0.21665088944933317, Val Loss = 11.723940393073834\n",
      "22800.iterasyon, Train Loss = 0.21664920825974676, Val Loss = 11.723901700728032\n",
      "22900.iterasyon, Train Loss = 0.2166475339181496, Val Loss = 11.72386363756461\n",
      "23000.iterasyon, Train Loss = 0.2166458663392861, Val Loss = 11.723826198814487\n",
      "23100.iterasyon, Train Loss = 0.21664420543910498, Val Loss = 11.72378937974431\n",
      "23200.iterasyon, Train Loss = 0.21664255113474565, Val Loss = 11.723753175655835\n",
      "23300.iterasyon, Train Loss = 0.21664090334452016, Val Loss = 11.723717581885916\n",
      "23400.iterasyon, Train Loss = 0.21663926198789285, Val Loss = 11.723682593806066\n",
      "23500.iterasyon, Train Loss = 0.21663762698546848, Val Loss = 11.723648206822315\n",
      "23600.iterasyon, Train Loss = 0.21663599825897106, Val Loss = 11.72361441637498\n",
      "23700.iterasyon, Train Loss = 0.2166343757312322, Val Loss = 11.723581217938266\n",
      "23800.iterasyon, Train Loss = 0.21663275932617368, Val Loss = 11.72354860702013\n",
      "23900.iterasyon, Train Loss = 0.21663114896879143, Val Loss = 11.7235165791619\n",
      "24000.iterasyon, Train Loss = 0.2166295445851365, Val Loss = 11.723485129938231\n",
      "24100.iterasyon, Train Loss = 0.2166279461023081, Val Loss = 11.723454254956584\n",
      "24200.iterasyon, Train Loss = 0.21662635344842893, Val Loss = 11.723423949857247\n",
      "24300.iterasyon, Train Loss = 0.2166247665526396, Val Loss = 11.723394210312884\n",
      "24400.iterasyon, Train Loss = 0.21662318534507527, Val Loss = 11.723365032028402\n",
      "24500.iterasyon, Train Loss = 0.2166216097568596, Val Loss = 11.72333641074071\n",
      "24600.iterasyon, Train Loss = 0.21662003972008337, Val Loss = 11.72330834221836\n",
      "24700.iterasyon, Train Loss = 0.21661847516779423, Val Loss = 11.723280822261527\n",
      "24800.iterasyon, Train Loss = 0.216616916033983, Val Loss = 11.723253846701502\n",
      "24900.iterasyon, Train Loss = 0.21661536225356906, Val Loss = 11.72322741140071\n",
      "25000.iterasyon, Train Loss = 0.2166138137623864, Val Loss = 11.723201512252233\n",
      "25100.iterasyon, Train Loss = 0.2166122704971741, Val Loss = 11.723176145179862\n",
      "25200.iterasyon, Train Loss = 0.21661073239555614, Val Loss = 11.723151306137613\n",
      "25300.iterasyon, Train Loss = 0.21660919939603704, Val Loss = 11.723126991109586\n",
      "25400.iterasyon, Train Loss = 0.2166076714379833, Val Loss = 11.723103196109783\n",
      "25500.iterasyon, Train Loss = 0.21660614846161455, Val Loss = 11.723079917181902\n",
      "25600.iterasyon, Train Loss = 0.21660463040798705, Val Loss = 11.723057150399027\n",
      "25700.iterasyon, Train Loss = 0.21660311721898637, Val Loss = 11.723034891863467\n",
      "25800.iterasyon, Train Loss = 0.21660160883730917, Val Loss = 11.723013137706465\n",
      "25900.iterasyon, Train Loss = 0.21660010520646267, Val Loss = 11.722991884088053\n",
      "26000.iterasyon, Train Loss = 0.21659860627074035, Val Loss = 11.722971127196828\n",
      "26100.iterasyon, Train Loss = 0.21659711197522039, Val Loss = 11.722950863249704\n",
      "26200.iterasyon, Train Loss = 0.21659562226574472, Val Loss = 11.722931088491746\n",
      "26300.iterasyon, Train Loss = 0.21659413708891873, Val Loss = 11.722911799195966\n",
      "26400.iterasyon, Train Loss = 0.21659265639209377, Val Loss = 11.72289299166294\n",
      "26500.iterasyon, Train Loss = 0.21659118012335665, Val Loss = 11.72287466222092\n",
      "26600.iterasyon, Train Loss = 0.2165897082315208, Val Loss = 11.722856807225323\n",
      "26700.iterasyon, Train Loss = 0.21658824066611645, Val Loss = 11.722839423058737\n",
      "26800.iterasyon, Train Loss = 0.21658677737737642, Val Loss = 11.72282250613055\n",
      "26900.iterasyon, Train Loss = 0.21658531831622954, Val Loss = 11.722806052876898\n",
      "27000.iterasyon, Train Loss = 0.21658386343429462, Val Loss = 11.722790059760346\n",
      "27100.iterasyon, Train Loss = 0.216582412683858, Val Loss = 11.722774523269827\n",
      "27200.iterasyon, Train Loss = 0.21658096601787805, Val Loss = 11.722759439920237\n",
      "27300.iterasyon, Train Loss = 0.21657952338996525, Val Loss = 11.72274480625248\n",
      "27400.iterasyon, Train Loss = 0.2165780847543783, Val Loss = 11.722730618833102\n",
      "27500.iterasyon, Train Loss = 0.21657665006601393, Val Loss = 11.7227168742541\n",
      "27600.iterasyon, Train Loss = 0.216575219280397, Val Loss = 11.722703569132952\n",
      "27700.iterasyon, Train Loss = 0.2165737923536708, Val Loss = 11.722690700112077\n",
      "27800.iterasyon, Train Loss = 0.2165723692425873, Val Loss = 11.722678263858976\n",
      "27900.iterasyon, Train Loss = 0.21657094990450373, Val Loss = 11.72266625706573\n",
      "28000.iterasyon, Train Loss = 0.21656953429736914, Val Loss = 11.722654676449174\n",
      "28100.iterasyon, Train Loss = 0.2165681223797141, Val Loss = 11.722643518750406\n",
      "28200.iterasyon, Train Loss = 0.21656671411065057, Val Loss = 11.722632780734774\n",
      "28300.iterasyon, Train Loss = 0.21656530944985256, Val Loss = 11.722622459191628\n",
      "28400.iterasyon, Train Loss = 0.2165639083575592, Val Loss = 11.72261255093405\n",
      "28500.iterasyon, Train Loss = 0.2165625107945566, Val Loss = 11.722603052799048\n",
      "28600.iterasyon, Train Loss = 0.21656111672217807, Val Loss = 11.722593961646854\n",
      "28700.iterasyon, Train Loss = 0.2165597261022899, Val Loss = 11.722585274361135\n",
      "28800.iterasyon, Train Loss = 0.21655833889729012, Val Loss = 11.722576987848624\n",
      "28900.iterasyon, Train Loss = 0.21655695507009518, Val Loss = 11.722569099039106\n",
      "29000.iterasyon, Train Loss = 0.216555574584134, Val Loss = 11.722561604885021\n",
      "29100.iterasyon, Train Loss = 0.21655419740334383, Val Loss = 11.722554502361607\n",
      "29200.iterasyon, Train Loss = 0.2165528234921599, Val Loss = 11.722547788466358\n",
      "29300.iterasyon, Train Loss = 0.216551452815507, Val Loss = 11.722541460219233\n",
      "29400.iterasyon, Train Loss = 0.21655008533879552, Val Loss = 11.72253551466209\n",
      "29500.iterasyon, Train Loss = 0.21654872102791636, Val Loss = 11.722529948858993\n",
      "29600.iterasyon, Train Loss = 0.21654735984922646, Val Loss = 11.722524759895633\n",
      "29700.iterasyon, Train Loss = 0.21654600176954933, Val Loss = 11.722519944879293\n",
      "29800.iterasyon, Train Loss = 0.21654464675616503, Val Loss = 11.722515500938842\n",
      "29900.iterasyon, Train Loss = 0.2165432947768063, Val Loss = 11.722511425224347\n",
      "30000.iterasyon, Train Loss = 0.21654194579964683, Val Loss = 11.72250771490715\n",
      "30100.iterasyon, Train Loss = 0.21654059979330217, Val Loss = 11.722504367179388\n",
      "30200.iterasyon, Train Loss = 0.21653925672681662, Val Loss = 11.722501379254167\n",
      "30300.iterasyon, Train Loss = 0.21653791656966134, Val Loss = 11.72249874836524\n",
      "30400.iterasyon, Train Loss = 0.21653657929172598, Val Loss = 11.722496471766819\n",
      "30500.iterasyon, Train Loss = 0.21653524486331652, Val Loss = 11.722494546733605\n",
      "30600.iterasyon, Train Loss = 0.21653391325514335, Val Loss = 11.722492970560445\n",
      "30700.iterasyon, Train Loss = 0.21653258443831983, Val Loss = 11.722491740562148\n",
      "30800.iterasyon, Train Loss = 0.2165312583843572, Val Loss = 11.72249085407364\n",
      "30900.iterasyon, Train Loss = 0.21652993506515258, Val Loss = 11.7224903084494\n",
      "31000.iterasyon, Train Loss = 0.21652861445299396, Val Loss = 11.722490101063732\n",
      "31100.iterasyon, Train Loss = 0.2165272965205416, Val Loss = 11.72249022931024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31200.iterasyon, Train Loss = 0.21652598124083808, Val Loss = 11.722490690601992\n",
      "31300.iterasyon, Train Loss = 0.21652466858728578, Val Loss = 11.722491482371085\n",
      "31400.iterasyon, Train Loss = 0.2165233585336562, Val Loss = 11.722492602068849\n",
      "31500.iterasyon, Train Loss = 0.21652205105407987, Val Loss = 11.722494047165346\n",
      "31600.iterasyon, Train Loss = 0.21652074612303496, Val Loss = 11.722495815149447\n",
      "31700.iterasyon, Train Loss = 0.21651944371535156, Val Loss = 11.722497903528684\n",
      "31800.iterasyon, Train Loss = 0.21651814380620307, Val Loss = 11.72250030982894\n",
      "31900.iterasyon, Train Loss = 0.21651684637109878, Val Loss = 11.722503031594703\n",
      "32000.iterasyon, Train Loss = 0.2165155513858824, Val Loss = 11.72250606638838\n",
      "32100.iterasyon, Train Loss = 0.21651425882672815, Val Loss = 11.722509411790545\n",
      "32200.iterasyon, Train Loss = 0.21651296867012915, Val Loss = 11.722513065399802\n",
      "32300.iterasyon, Train Loss = 0.21651168089290485, Val Loss = 11.722517024832426\n",
      "32400.iterasyon, Train Loss = 0.21651039547218395, Val Loss = 11.722521287722415\n",
      "32500.iterasyon, Train Loss = 0.21650911238540735, Val Loss = 11.722525851721294\n",
      "32600.iterasyon, Train Loss = 0.2165078316103226, Val Loss = 11.722530714498005\n",
      "32700.iterasyon, Train Loss = 0.21650655312497732, Val Loss = 11.72253587373879\n",
      "32800.iterasyon, Train Loss = 0.21650527690771718, Val Loss = 11.722541327146983\n",
      "32900.iterasyon, Train Loss = 0.21650400293718164, Val Loss = 11.722547072442959\n",
      "33000.iterasyon, Train Loss = 0.21650273119229554, Val Loss = 11.722553107364064\n",
      "33100.iterasyon, Train Loss = 0.2165014616522745, Val Loss = 11.722559429664242\n",
      "33200.iterasyon, Train Loss = 0.2165001942966084, Val Loss = 11.72256603711423\n",
      "33300.iterasyon, Train Loss = 0.2164989291050684, Val Loss = 11.722572927501286\n",
      "33400.iterasyon, Train Loss = 0.21649766605769752, Val Loss = 11.722580098628965\n",
      "33500.iterasyon, Train Loss = 0.21649640513480525, Val Loss = 11.722587548317152\n",
      "33600.iterasyon, Train Loss = 0.21649514631696973, Val Loss = 11.722595274401852\n",
      "33700.iterasyon, Train Loss = 0.21649388958502813, Val Loss = 11.722603274735203\n",
      "33800.iterasyon, Train Loss = 0.2164926349200763, Val Loss = 11.72261154718513\n",
      "33900.iterasyon, Train Loss = 0.21649138230346274, Val Loss = 11.722620089635441\n",
      "34000.iterasyon, Train Loss = 0.21649013171678855, Val Loss = 11.72262889998556\n",
      "34100.iterasyon, Train Loss = 0.21648888314189838, Val Loss = 11.722637976150585\n",
      "34200.iterasyon, Train Loss = 0.216487636560885, Val Loss = 11.722647316060934\n",
      "34300.iterasyon, Train Loss = 0.21648639195607752, Val Loss = 11.72265691766239\n",
      "34400.iterasyon, Train Loss = 0.21648514931004093, Val Loss = 11.722666778916013\n",
      "34500.iterasyon, Train Loss = 0.2164839086055759, Val Loss = 11.72267689779786\n",
      "34600.iterasyon, Train Loss = 0.21648266982571174, Val Loss = 11.722687272299115\n",
      "34700.iterasyon, Train Loss = 0.21648143295370295, Val Loss = 11.722697900425757\n",
      "34800.iterasyon, Train Loss = 0.21648019797303056, Val Loss = 11.722708780198499\n",
      "34900.iterasyon, Train Loss = 0.21647896486739363, Val Loss = 11.722719909652843\n",
      "35000.iterasyon, Train Loss = 0.21647773362070843, Val Loss = 11.722731286838755\n",
      "35100.iterasyon, Train Loss = 0.21647650421710551, Val Loss = 11.722742909820669\n",
      "35200.iterasyon, Train Loss = 0.21647527664092758, Val Loss = 11.722754776677284\n",
      "35300.iterasyon, Train Loss = 0.2164740508767232, Val Loss = 11.722766885501656\n",
      "35400.iterasyon, Train Loss = 0.21647282690924877, Val Loss = 11.72277923440088\n",
      "35500.iterasyon, Train Loss = 0.2164716047234588, Val Loss = 11.722791821496061\n",
      "35600.iterasyon, Train Loss = 0.21647038430451196, Val Loss = 11.722804644922261\n",
      "35700.iterasyon, Train Loss = 0.2164691656377601, Val Loss = 11.72281770282841\n",
      "35800.iterasyon, Train Loss = 0.21646794870875016, Val Loss = 11.722830993376979\n",
      "35900.iterasyon, Train Loss = 0.21646673350321796, Val Loss = 11.72284451474426\n",
      "36000.iterasyon, Train Loss = 0.21646552000709327, Val Loss = 11.722858265119836\n",
      "36100.iterasyon, Train Loss = 0.2164643082064824, Val Loss = 11.7228722427069\n",
      "36200.iterasyon, Train Loss = 0.21646309808768405, Val Loss = 11.722886445721782\n",
      "36300.iterasyon, Train Loss = 0.21646188963717095, Val Loss = 11.72290087239412\n",
      "36400.iterasyon, Train Loss = 0.21646068284159778, Val Loss = 11.722915520966653\n",
      "36500.iterasyon, Train Loss = 0.21645947768779217, Val Loss = 11.722930389695097\n",
      "36600.iterasyon, Train Loss = 0.2164582741627558, Val Loss = 11.72294547684811\n",
      "36700.iterasyon, Train Loss = 0.21645707225365943, Val Loss = 11.722960780707187\n",
      "36800.iterasyon, Train Loss = 0.21645587194784432, Val Loss = 11.72297629956648\n",
      "36900.iterasyon, Train Loss = 0.21645467323281556, Val Loss = 11.72299203173282\n",
      "37000.iterasyon, Train Loss = 0.21645347609624188, Val Loss = 11.723007975525668\n",
      "37100.iterasyon, Train Loss = 0.21645228052595442, Val Loss = 11.723024129276743\n",
      "37200.iterasyon, Train Loss = 0.21645108650994072, Val Loss = 11.723040491330224\n",
      "37300.iterasyon, Train Loss = 0.21644989403634807, Val Loss = 11.723057060042578\n",
      "37400.iterasyon, Train Loss = 0.21644870309347675, Val Loss = 11.723073833782374\n",
      "37500.iterasyon, Train Loss = 0.2164475136697764, Val Loss = 11.723090810930326\n",
      "37600.iterasyon, Train Loss = 0.21644632575385228, Val Loss = 11.72310798987906\n",
      "37700.iterasyon, Train Loss = 0.21644513933445333, Val Loss = 11.72312536903314\n",
      "37800.iterasyon, Train Loss = 0.216443954400476, Val Loss = 11.723142946809023\n",
      "37900.iterasyon, Train Loss = 0.2164427709409611, Val Loss = 11.723160721634722\n",
      "38000.iterasyon, Train Loss = 0.21644158894509002, Val Loss = 11.723178691950034\n",
      "38100.iterasyon, Train Loss = 0.21644040840218556, Val Loss = 11.723196856206268\n",
      "38200.iterasyon, Train Loss = 0.21643922930170653, Val Loss = 11.723215212866148\n",
      "38300.iterasyon, Train Loss = 0.21643805163324922, Val Loss = 11.723233760403861\n",
      "38400.iterasyon, Train Loss = 0.21643687538654297, Val Loss = 11.723252497304816\n",
      "38500.iterasyon, Train Loss = 0.21643570055145153, Val Loss = 11.723271422065746\n",
      "38600.iterasyon, Train Loss = 0.2164345271179661, Val Loss = 11.72329053319441\n",
      "38700.iterasyon, Train Loss = 0.21643335507620667, Val Loss = 11.723309829209565\n",
      "38800.iterasyon, Train Loss = 0.21643218441642278, Val Loss = 11.723329308641079\n",
      "38900.iterasyon, Train Loss = 0.2164310151289857, Val Loss = 11.723348970029672\n",
      "39000.iterasyon, Train Loss = 0.21642984720439254, Val Loss = 11.723368811926804\n",
      "39100.iterasyon, Train Loss = 0.21642868063325946, Val Loss = 11.723388832894646\n",
      "39200.iterasyon, Train Loss = 0.21642751540632185, Val Loss = 11.723409031506181\n",
      "39300.iterasyon, Train Loss = 0.216426351514438, Val Loss = 11.723429406344794\n",
      "39400.iterasyon, Train Loss = 0.2164251889485785, Val Loss = 11.72344995600437\n",
      "39500.iterasyon, Train Loss = 0.21642402769982974, Val Loss = 11.723470679089262\n",
      "39600.iterasyon, Train Loss = 0.21642286775939146, Val Loss = 11.723491574214158\n",
      "39700.iterasyon, Train Loss = 0.21642170911857483, Val Loss = 11.723512640003904\n",
      "39800.iterasyon, Train Loss = 0.21642055176880093, Val Loss = 11.723533875093654\n",
      "39900.iterasyon, Train Loss = 0.2164193957016002, Val Loss = 11.72355527812865\n",
      "40000.iterasyon, Train Loss = 0.2164182409086106, Val Loss = 11.723576847764049\n",
      "40100.iterasyon, Train Loss = 0.21641708738157248, Val Loss = 11.723598582665092\n",
      "40200.iterasyon, Train Loss = 0.21641593511233428, Val Loss = 11.723620481506888\n",
      "40300.iterasyon, Train Loss = 0.2164147840928454, Val Loss = 11.723642542974236\n",
      "40400.iterasyon, Train Loss = 0.21641363431515637, Val Loss = 11.723664765761809\n",
      "40500.iterasyon, Train Loss = 0.21641248577141745, Val Loss = 11.723687148573879\n",
      "40600.iterasyon, Train Loss = 0.21641133845387697, Val Loss = 11.72370969012436\n",
      "40700.iterasyon, Train Loss = 0.21641019235488038, Val Loss = 11.723732389136622\n",
      "40800.iterasyon, Train Loss = 0.2164090474668694, Val Loss = 11.723755244343565\n",
      "40900.iterasyon, Train Loss = 0.2164079037823818, Val Loss = 11.72377825448737\n",
      "41000.iterasyon, Train Loss = 0.21640676129404549, Val Loss = 11.723801418319628\n",
      "41100.iterasyon, Train Loss = 0.21640561999458133, Val Loss = 11.72382473460112\n",
      "41200.iterasyon, Train Loss = 0.2164044798767983, Val Loss = 11.723848202101838\n",
      "41300.iterasyon, Train Loss = 0.21640334093360178, Val Loss = 11.723871819600793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41400.iterasyon, Train Loss = 0.2164022031579744, Val Loss = 11.723895585886162\n",
      "41500.iterasyon, Train Loss = 0.21640106654299684, Val Loss = 11.723919499755029\n",
      "41600.iterasyon, Train Loss = 0.21639993108182712, Val Loss = 11.723943560013396\n",
      "41700.iterasyon, Train Loss = 0.21639879676771212, Val Loss = 11.723967765476072\n",
      "41800.iterasyon, Train Loss = 0.21639766359397794, Val Loss = 11.723992114966721\n",
      "41900.iterasyon, Train Loss = 0.21639653155403907, Val Loss = 11.724016607317578\n",
      "42000.iterasyon, Train Loss = 0.21639540064138352, Val Loss = 11.724041241369735\n",
      "42100.iterasyon, Train Loss = 0.21639427084958449, Val Loss = 11.724066015972628\n",
      "42200.iterasyon, Train Loss = 0.21639314217229333, Val Loss = 11.724090929984387\n",
      "42300.iterasyon, Train Loss = 0.21639201460323418, Val Loss = 11.724115982271544\n",
      "42400.iterasyon, Train Loss = 0.21639088813621596, Val Loss = 11.724141171709006\n",
      "42500.iterasyon, Train Loss = 0.21638976276511523, Val Loss = 11.724166497180036\n",
      "42600.iterasyon, Train Loss = 0.21638863848388848, Val Loss = 11.724191957576156\n",
      "42700.iterasyon, Train Loss = 0.21638751528656164, Val Loss = 11.724217551797087\n",
      "42800.iterasyon, Train Loss = 0.21638639316723615, Val Loss = 11.724243278750727\n",
      "42900.iterasyon, Train Loss = 0.21638527212008474, Val Loss = 11.724269137353016\n",
      "43000.iterasyon, Train Loss = 0.2163841521393471, Val Loss = 11.724295126527982\n",
      "43100.iterasyon, Train Loss = 0.21638303321933786, Val Loss = 11.724321245207584\n",
      "43200.iterasyon, Train Loss = 0.21638191535443538, Val Loss = 11.724347492331653\n",
      "43300.iterasyon, Train Loss = 0.21638079853908795, Val Loss = 11.724373866847952\n",
      "43400.iterasyon, Train Loss = 0.21637968276781058, Val Loss = 11.724400367711972\n",
      "43500.iterasyon, Train Loss = 0.21637856803518277, Val Loss = 11.724426993886985\n",
      "43600.iterasyon, Train Loss = 0.21637745433584918, Val Loss = 11.72445374434392\n",
      "43700.iterasyon, Train Loss = 0.21637634166452077, Val Loss = 11.724480618061312\n",
      "43800.iterasyon, Train Loss = 0.21637523001596756, Val Loss = 11.724507614025335\n",
      "43900.iterasyon, Train Loss = 0.21637411938502513, Val Loss = 11.72453473122956\n",
      "44000.iterasyon, Train Loss = 0.2163730097665885, Val Loss = 11.724561968675063\n",
      "44100.iterasyon, Train Loss = 0.2163719011556149, Val Loss = 11.724589325370392\n",
      "44200.iterasyon, Train Loss = 0.21637079354711955, Val Loss = 11.724616800331294\n",
      "44300.iterasyon, Train Loss = 0.21636968693617809, Val Loss = 11.724644392580908\n",
      "44400.iterasyon, Train Loss = 0.2163685813179215, Val Loss = 11.724672101149576\n",
      "44500.iterasyon, Train Loss = 0.21636747668754244, Val Loss = 11.724699925074914\n",
      "44600.iterasyon, Train Loss = 0.2163663730402852, Val Loss = 11.724727863401469\n",
      "44700.iterasyon, Train Loss = 0.21636527037145456, Val Loss = 11.724755915181019\n",
      "44800.iterasyon, Train Loss = 0.21636416867640584, Val Loss = 11.724784079472418\n",
      "44900.iterasyon, Train Loss = 0.21636306795055182, Val Loss = 11.724812355341257\n",
      "45000.iterasyon, Train Loss = 0.21636196818935693, Val Loss = 11.724840741860305\n",
      "45100.iterasyon, Train Loss = 0.2163608693883401, Val Loss = 11.724869238109019\n",
      "45200.iterasyon, Train Loss = 0.2163597715430697, Val Loss = 11.724897843173782\n",
      "45300.iterasyon, Train Loss = 0.21635867464916772, Val Loss = 11.724926556147713\n",
      "45400.iterasyon, Train Loss = 0.2163575787023068, Val Loss = 11.724955376130639\n",
      "45500.iterasyon, Train Loss = 0.2163564836982066, Val Loss = 11.724984302229107\n",
      "45600.iterasyon, Train Loss = 0.21635538963263978, Val Loss = 11.725013333556163\n",
      "45700.iterasyon, Train Loss = 0.21635429650142582, Val Loss = 11.725042469231578\n",
      "45800.iterasyon, Train Loss = 0.21635320430043117, Val Loss = 11.725071708381513\n",
      "45900.iterasyon, Train Loss = 0.21635211302557342, Val Loss = 11.7251010501387\n",
      "46000.iterasyon, Train Loss = 0.21635102267281048, Val Loss = 11.725130493642249\n",
      "46100.iterasyon, Train Loss = 0.21634993323815166, Val Loss = 11.725160038037696\n",
      "46200.iterasyon, Train Loss = 0.216348844717649, Val Loss = 11.725189682476822\n",
      "46300.iterasyon, Train Loss = 0.21634775710740176, Val Loss = 11.725219426117752\n",
      "46400.iterasyon, Train Loss = 0.2163466704035499, Val Loss = 11.72524926812487\n",
      "46500.iterasyon, Train Loss = 0.21634558460228082, Val Loss = 11.725279207668734\n",
      "46600.iterasyon, Train Loss = 0.21634449969981992, Val Loss = 11.725309243926011\n",
      "46700.iterasyon, Train Loss = 0.2163434156924413, Val Loss = 11.725339376079493\n",
      "46800.iterasyon, Train Loss = 0.2163423325764553, Val Loss = 11.725369603318068\n",
      "46900.iterasyon, Train Loss = 0.21634125034821652, Val Loss = 11.725399924836609\n",
      "47000.iterasyon, Train Loss = 0.21634016900411945, Val Loss = 11.725430339835963\n",
      "47100.iterasyon, Train Loss = 0.21633908854059913, Val Loss = 11.725460847522843\n",
      "47200.iterasyon, Train Loss = 0.21633800895412922, Val Loss = 11.725491447109874\n",
      "47300.iterasyon, Train Loss = 0.21633693024122488, Val Loss = 11.725522137815622\n",
      "47400.iterasyon, Train Loss = 0.2163358523984352, Val Loss = 11.725552918864308\n",
      "47500.iterasyon, Train Loss = 0.21633477542235116, Val Loss = 11.725583789485972\n",
      "47600.iterasyon, Train Loss = 0.21633369930960264, Val Loss = 11.725614748916316\n",
      "47700.iterasyon, Train Loss = 0.21633262405685225, Val Loss = 11.725645796396746\n",
      "47800.iterasyon, Train Loss = 0.21633154966080148, Val Loss = 11.72567693117431\n",
      "47900.iterasyon, Train Loss = 0.216330476118188, Val Loss = 11.725708152501607\n",
      "48000.iterasyon, Train Loss = 0.2163294034257832, Val Loss = 11.725739459636758\n",
      "48100.iterasyon, Train Loss = 0.21632833158039677, Val Loss = 11.72577085184343\n",
      "48200.iterasyon, Train Loss = 0.21632726057886964, Val Loss = 11.725802328390749\n",
      "48300.iterasyon, Train Loss = 0.21632619041807885, Val Loss = 11.725833888553227\n",
      "48400.iterasyon, Train Loss = 0.21632512109493537, Val Loss = 11.725865531610745\n",
      "48500.iterasyon, Train Loss = 0.21632405260638235, Val Loss = 11.725897256848523\n",
      "48600.iterasyon, Train Loss = 0.21632298494939572, Val Loss = 11.725929063557198\n",
      "48700.iterasyon, Train Loss = 0.21632191812098464, Val Loss = 11.725960951032556\n",
      "48800.iterasyon, Train Loss = 0.2163208521181894, Val Loss = 11.725992918575637\n",
      "48900.iterasyon, Train Loss = 0.2163197869380826, Val Loss = 11.726024965492611\n",
      "49000.iterasyon, Train Loss = 0.21631872257776755, Val Loss = 11.726057091094878\n",
      "49100.iterasyon, Train Loss = 0.2163176590343776, Val Loss = 11.726089294698875\n",
      "49200.iterasyon, Train Loss = 0.21631659630507785, Val Loss = 11.726121575626207\n",
      "49300.iterasyon, Train Loss = 0.2163155343870616, Val Loss = 11.726153933203415\n",
      "49400.iterasyon, Train Loss = 0.21631447327755282, Val Loss = 11.726186366762066\n",
      "49500.iterasyon, Train Loss = 0.21631341297380322, Val Loss = 11.726218875638686\n",
      "49600.iterasyon, Train Loss = 0.21631235347309632, Val Loss = 11.72625145917476\n",
      "49700.iterasyon, Train Loss = 0.21631129477274003, Val Loss = 11.726284116716636\n",
      "49800.iterasyon, Train Loss = 0.21631023687007234, Val Loss = 11.726316847615474\n",
      "49900.iterasyon, Train Loss = 0.21630917976245703, Val Loss = 11.726349651227311\n",
      "50000.iterasyon, Train Loss = 0.2163081234472901, Val Loss = 11.726382526912882\n",
      "50100.iterasyon, Train Loss = 0.21630706792198795, Val Loss = 11.726415474037758\n",
      "50200.iterasyon, Train Loss = 0.21630601318399525, Val Loss = 11.726448491972203\n",
      "50300.iterasyon, Train Loss = 0.21630495923078685, Val Loss = 11.726481580091077\n",
      "50400.iterasyon, Train Loss = 0.21630390605985805, Val Loss = 11.726514737773968\n",
      "50500.iterasyon, Train Loss = 0.21630285366873306, Val Loss = 11.726547964404967\n",
      "50600.iterasyon, Train Loss = 0.21630180205496083, Val Loss = 11.726581259372905\n",
      "50700.iterasyon, Train Loss = 0.21630075121611395, Val Loss = 11.726614622070974\n",
      "50800.iterasyon, Train Loss = 0.21629970114978903, Val Loss = 11.726648051896952\n",
      "50900.iterasyon, Train Loss = 0.21629865185360744, Val Loss = 11.726681548253095\n",
      "51000.iterasyon, Train Loss = 0.21629760332521708, Val Loss = 11.726715110546069\n",
      "51100.iterasyon, Train Loss = 0.21629655556228514, Val Loss = 11.726748738187014\n",
      "51200.iterasyon, Train Loss = 0.21629550856250324, Val Loss = 11.726782430591332\n",
      "51300.iterasyon, Train Loss = 0.2162944623235893, Val Loss = 11.726816187178835\n",
      "51400.iterasyon, Train Loss = 0.2162934168432771, Val Loss = 11.7268500073736\n",
      "51500.iterasyon, Train Loss = 0.2162923721193312, Val Loss = 11.726883890604089\n",
      "51600.iterasyon, Train Loss = 0.21629132814952956, Val Loss = 11.72691783630288\n",
      "51700.iterasyon, Train Loss = 0.21629028493167957, Val Loss = 11.726951843906821\n",
      "51800.iterasyon, Train Loss = 0.21628924246360404, Val Loss = 11.726985912856934\n",
      "51900.iterasyon, Train Loss = 0.2162882007431511, Val Loss = 11.727020042598438\n",
      "52000.iterasyon, Train Loss = 0.2162871597681865, Val Loss = 11.727054232580574\n",
      "52100.iterasyon, Train Loss = 0.21628611953659962, Val Loss = 11.727088482256752\n",
      "52200.iterasyon, Train Loss = 0.2162850800462989, Val Loss = 11.727122791084469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52300.iterasyon, Train Loss = 0.21628404129521148, Val Loss = 11.727157158525152\n",
      "52400.iterasyon, Train Loss = 0.21628300328128622, Val Loss = 11.727191584044352\n",
      "52500.iterasyon, Train Loss = 0.21628196600249253, Val Loss = 11.727226067111435\n",
      "52600.iterasyon, Train Loss = 0.21628092945681676, Val Loss = 11.727260607199899\n",
      "52700.iterasyon, Train Loss = 0.21627989364226266, Val Loss = 11.727295203786971\n",
      "52800.iterasyon, Train Loss = 0.21627885855685872, Val Loss = 11.727329856353931\n",
      "52900.iterasyon, Train Loss = 0.2162778241986474, Val Loss = 11.727364564385763\n",
      "53000.iterasyon, Train Loss = 0.21627679056569138, Val Loss = 11.72739932737135\n",
      "53100.iterasyon, Train Loss = 0.2162757576560699, Val Loss = 11.727434144803462\n",
      "53200.iterasyon, Train Loss = 0.2162747254678815, Val Loss = 11.727469016178476\n",
      "53300.iterasyon, Train Loss = 0.2162736939992416, Val Loss = 11.727503940996604\n",
      "53400.iterasyon, Train Loss = 0.21627266324828356, Val Loss = 11.72753891876176\n",
      "53500.iterasyon, Train Loss = 0.21627163321315704, Val Loss = 11.727573948981556\n",
      "53600.iterasyon, Train Loss = 0.2162706038920309, Val Loss = 11.727609031167244\n",
      "53700.iterasyon, Train Loss = 0.21626957528308877, Val Loss = 11.727644164833738\n",
      "53800.iterasyon, Train Loss = 0.21626854738453108, Val Loss = 11.727679349499525\n",
      "53900.iterasyon, Train Loss = 0.21626752019457418, Val Loss = 11.727714584686698\n",
      "54000.iterasyon, Train Loss = 0.2162664937114512, Val Loss = 11.727749869920878\n",
      "54100.iterasyon, Train Loss = 0.2162654679334116, Val Loss = 11.727785204731287\n",
      "54200.iterasyon, Train Loss = 0.2162644428587208, Val Loss = 11.727820588650534\n",
      "54300.iterasyon, Train Loss = 0.21626341848565764, Val Loss = 11.72785602121479\n",
      "54400.iterasyon, Train Loss = 0.2162623948125184, Val Loss = 11.727891501963681\n",
      "54500.iterasyon, Train Loss = 0.2162613718376134, Val Loss = 11.72792703044024\n",
      "54600.iterasyon, Train Loss = 0.21626034955926943, Val Loss = 11.72796260619084\n",
      "54700.iterasyon, Train Loss = 0.21625932797582628, Val Loss = 11.727998228765351\n",
      "54800.iterasyon, Train Loss = 0.21625830708563687, Val Loss = 11.728033897716848\n",
      "54900.iterasyon, Train Loss = 0.21625728688707402, Val Loss = 11.728069612601834\n",
      "55000.iterasyon, Train Loss = 0.21625626737851836, Val Loss = 11.728105372980155\n",
      "55100.iterasyon, Train Loss = 0.21625524855837022, Val Loss = 11.72814117841482\n",
      "55200.iterasyon, Train Loss = 0.2162542304250374, Val Loss = 11.728177028472084\n",
      "55300.iterasyon, Train Loss = 0.21625321297694744, Val Loss = 11.728212922721603\n",
      "55400.iterasyon, Train Loss = 0.216252196212539, Val Loss = 11.72824886073605\n",
      "55500.iterasyon, Train Loss = 0.2162511801302618, Val Loss = 11.728284842091375\n",
      "55600.iterasyon, Train Loss = 0.21625016472858324, Val Loss = 11.728320866366614\n",
      "55700.iterasyon, Train Loss = 0.21624915000598022, Val Loss = 11.72835693314399\n",
      "55800.iterasyon, Train Loss = 0.21624813596094317, Val Loss = 11.728393042008918\n",
      "55900.iterasyon, Train Loss = 0.216247122591976, Val Loss = 11.728429192549731\n",
      "56000.iterasyon, Train Loss = 0.21624610989759582, Val Loss = 11.728465384357943\n",
      "56100.iterasyon, Train Loss = 0.21624509787632776, Val Loss = 11.728501617028101\n",
      "56200.iterasyon, Train Loss = 0.21624408652671642, Val Loss = 11.7285378901577\n",
      "56300.iterasyon, Train Loss = 0.21624307584731137, Val Loss = 11.728574203347366\n",
      "56400.iterasyon, Train Loss = 0.21624206583667827, Val Loss = 11.728610556200532\n",
      "56500.iterasyon, Train Loss = 0.21624105649339162, Val Loss = 11.72864694832379\n",
      "56600.iterasyon, Train Loss = 0.21624004781604267, Val Loss = 11.728683379326444\n",
      "56700.iterasyon, Train Loss = 0.2162390398032283, Val Loss = 11.728719848820905\n",
      "56800.iterasyon, Train Loss = 0.21623803245355963, Val Loss = 11.728756356422304\n",
      "56900.iterasyon, Train Loss = 0.21623702576565865, Val Loss = 11.728792901748813\n",
      "57000.iterasyon, Train Loss = 0.21623601973815668, Val Loss = 11.728829484421329\n",
      "57100.iterasyon, Train Loss = 0.2162350143696992, Val Loss = 11.728866104063583\n",
      "57200.iterasyon, Train Loss = 0.2162340096589412, Val Loss = 11.728902760302143\n",
      "57300.iterasyon, Train Loss = 0.21623300560454442, Val Loss = 11.728939452766365\n",
      "57400.iterasyon, Train Loss = 0.2162320022051882, Val Loss = 11.728976181088372\n",
      "57500.iterasyon, Train Loss = 0.2162309994595577, Val Loss = 11.729012944903014\n",
      "57600.iterasyon, Train Loss = 0.2162299973663467, Val Loss = 11.729049743847899\n",
      "57700.iterasyon, Train Loss = 0.21622899592426403, Val Loss = 11.729086577563242\n",
      "57800.iterasyon, Train Loss = 0.21622799513202595, Val Loss = 11.729123445692117\n",
      "57900.iterasyon, Train Loss = 0.21622699498835884, Val Loss = 11.72916034788003\n",
      "58000.iterasyon, Train Loss = 0.21622599549199725, Val Loss = 11.729197283775338\n",
      "58100.iterasyon, Train Loss = 0.21622499664168757, Val Loss = 11.729234253028956\n",
      "58200.iterasyon, Train Loss = 0.21622399843618376, Val Loss = 11.729271255294359\n",
      "58300.iterasyon, Train Loss = 0.21622300087425247, Val Loss = 11.729308290227637\n",
      "58400.iterasyon, Train Loss = 0.21622200395466515, Val Loss = 11.72934535748752\n",
      "58500.iterasyon, Train Loss = 0.2162210076762081, Val Loss = 11.729382456735127\n",
      "58600.iterasyon, Train Loss = 0.21622001203766997, Val Loss = 11.72941958763425\n",
      "58700.iterasyon, Train Loss = 0.21621901703785337, Val Loss = 11.729456749851133\n",
      "58800.iterasyon, Train Loss = 0.21621802267556944, Val Loss = 11.729493943054484\n",
      "58900.iterasyon, Train Loss = 0.2162170289496338, Val Loss = 11.729531166915605\n",
      "59000.iterasyon, Train Loss = 0.2162160358588768, Val Loss = 11.729568421108093\n",
      "59100.iterasyon, Train Loss = 0.21621504340213135, Val Loss = 11.729605705308126\n",
      "59200.iterasyon, Train Loss = 0.21621405157824378, Val Loss = 11.729643019194198\n",
      "59300.iterasyon, Train Loss = 0.21621306038606505, Val Loss = 11.729680362447251\n",
      "59400.iterasyon, Train Loss = 0.21621206982445693, Val Loss = 11.729717734750624\n",
      "59500.iterasyon, Train Loss = 0.2162110798922882, Val Loss = 11.729755135789963\n",
      "59600.iterasyon, Train Loss = 0.2162100905884348, Val Loss = 11.729792565253348\n",
      "59700.iterasyon, Train Loss = 0.21620910191178244, Val Loss = 11.729830022831196\n",
      "59800.iterasyon, Train Loss = 0.21620811386122454, Val Loss = 11.729867508216072\n",
      "59900.iterasyon, Train Loss = 0.2162071264356603, Val Loss = 11.729905021103036\n",
      "60000.iterasyon, Train Loss = 0.2162061396339987, Val Loss = 11.729942561189365\n",
      "60100.iterasyon, Train Loss = 0.216205153455154, Val Loss = 11.729980128174551\n",
      "60200.iterasyon, Train Loss = 0.21620416789805127, Val Loss = 11.73001772176038\n",
      "60300.iterasyon, Train Loss = 0.21620318296161842, Val Loss = 11.730055341650875\n",
      "60400.iterasyon, Train Loss = 0.21620219864479592, Val Loss = 11.73009298755222\n",
      "60500.iterasyon, Train Loss = 0.2162012149465279, Val Loss = 11.730130659172902\n",
      "60600.iterasyon, Train Loss = 0.2162002318657664, Val Loss = 11.73016835622348\n",
      "60700.iterasyon, Train Loss = 0.21619924940147048, Val Loss = 11.730206078416765\n",
      "60800.iterasyon, Train Loss = 0.21619826755260615, Val Loss = 11.730243825467614\n",
      "60900.iterasyon, Train Loss = 0.21619728631814683, Val Loss = 11.730281597093096\n",
      "61000.iterasyon, Train Loss = 0.21619630569707085, Val Loss = 11.730319393012449\n",
      "61100.iterasyon, Train Loss = 0.21619532568836686, Val Loss = 11.730357212946897\n",
      "61200.iterasyon, Train Loss = 0.21619434629102807, Val Loss = 11.730395056619788\n",
      "61300.iterasyon, Train Loss = 0.21619336750405338, Val Loss = 11.730432923756636\n",
      "61400.iterasyon, Train Loss = 0.2161923893264468, Val Loss = 11.730470814084798\n",
      "61500.iterasyon, Train Loss = 0.21619141175722642, Val Loss = 11.730508727333929\n",
      "61600.iterasyon, Train Loss = 0.21619043479540667, Val Loss = 11.73054666323555\n",
      "61700.iterasyon, Train Loss = 0.2161894584400137, Val Loss = 11.730584621523228\n",
      "61800.iterasyon, Train Loss = 0.2161884826900795, Val Loss = 11.730622601932565\n",
      "61900.iterasyon, Train Loss = 0.21618750754464275, Val Loss = 11.730660604201072\n",
      "62000.iterasyon, Train Loss = 0.21618653300274482, Val Loss = 11.730698628068293\n",
      "62100.iterasyon, Train Loss = 0.21618555906343717, Val Loss = 11.730736673275763\n",
      "62200.iterasyon, Train Loss = 0.21618458572577393, Val Loss = 11.730774739566776\n",
      "62300.iterasyon, Train Loss = 0.21618361298881675, Val Loss = 11.730812826686753\n",
      "62400.iterasyon, Train Loss = 0.21618264085163327, Val Loss = 11.73085093438288\n",
      "62500.iterasyon, Train Loss = 0.2161816693132973, Val Loss = 11.730889062404392\n",
      "62600.iterasyon, Train Loss = 0.21618069837288423, Val Loss = 11.730927210502216\n",
      "62700.iterasyon, Train Loss = 0.2161797280294808, Val Loss = 11.730965378429328\n",
      "62800.iterasyon, Train Loss = 0.21617875828217634, Val Loss = 11.731003565940425\n",
      "62900.iterasyon, Train Loss = 0.2161777891300646, Val Loss = 11.7310417727921\n",
      "63000.iterasyon, Train Loss = 0.21617682057224577, Val Loss = 11.731079998742803\n",
      "63100.iterasyon, Train Loss = 0.2161758526078281, Val Loss = 11.731118243552762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63200.iterasyon, Train Loss = 0.21617488523592066, Val Loss = 11.731156506984021\n",
      "63300.iterasyon, Train Loss = 0.21617391845564038, Val Loss = 11.73119478880036\n",
      "63400.iterasyon, Train Loss = 0.216172952266108, Val Loss = 11.731233088767441\n",
      "63500.iterasyon, Train Loss = 0.2161719866664503, Val Loss = 11.731271406652557\n",
      "63600.iterasyon, Train Loss = 0.21617102165579882, Val Loss = 11.731309742224843\n",
      "63700.iterasyon, Train Loss = 0.21617005723329055, Val Loss = 11.731348095255113\n",
      "63800.iterasyon, Train Loss = 0.21616909339806623, Val Loss = 11.731386465515962\n",
      "63900.iterasyon, Train Loss = 0.21616813014927225, Val Loss = 11.731424852781636\n",
      "64000.iterasyon, Train Loss = 0.2161671674860591, Val Loss = 11.731463256828166\n",
      "64100.iterasyon, Train Loss = 0.2161662054075837, Val Loss = 11.731501677433174\n",
      "64200.iterasyon, Train Loss = 0.21616524391300676, Val Loss = 11.731540114375967\n",
      "64300.iterasyon, Train Loss = 0.21616428300149262, Val Loss = 11.731578567437504\n",
      "64400.iterasyon, Train Loss = 0.21616332267221183, Val Loss = 11.731617036400495\n",
      "64500.iterasyon, Train Loss = 0.2161623629243377, Val Loss = 11.731655521049172\n",
      "64600.iterasyon, Train Loss = 0.2161614037570511, Val Loss = 11.731694021169432\n",
      "64700.iterasyon, Train Loss = 0.21616044516953425, Val Loss = 11.731732536548801\n",
      "64800.iterasyon, Train Loss = 0.21615948716097447, Val Loss = 11.731771066976401\n",
      "64900.iterasyon, Train Loss = 0.21615852973056435, Val Loss = 11.7318096122429\n",
      "65000.iterasyon, Train Loss = 0.21615757287750045, Val Loss = 11.731848172140571\n",
      "65100.iterasyon, Train Loss = 0.21615661660098237, Val Loss = 11.731886746463243\n",
      "65200.iterasyon, Train Loss = 0.21615566090021607, Val Loss = 11.731925335006336\n",
      "65300.iterasyon, Train Loss = 0.21615470577441162, Val Loss = 11.73196393756678\n",
      "65400.iterasyon, Train Loss = 0.21615375122278022, Val Loss = 11.732002553943017\n",
      "65500.iterasyon, Train Loss = 0.21615279724453962, Val Loss = 11.732041183935056\n",
      "65600.iterasyon, Train Loss = 0.2161518438389128, Val Loss = 11.73207982734433\n",
      "65700.iterasyon, Train Loss = 0.21615089100512277, Val Loss = 11.732118483973899\n",
      "65800.iterasyon, Train Loss = 0.2161499387424002, Val Loss = 11.732157153628178\n",
      "65900.iterasyon, Train Loss = 0.2161489870499784, Val Loss = 11.732195836113153\n",
      "66000.iterasyon, Train Loss = 0.21614803592709209, Val Loss = 11.732234531236278\n",
      "66100.iterasyon, Train Loss = 0.21614708537298455, Val Loss = 11.732273238806329\n",
      "66200.iterasyon, Train Loss = 0.2161461353869016, Val Loss = 11.73231195863368\n",
      "66300.iterasyon, Train Loss = 0.21614518596808704, Val Loss = 11.732350690530012\n",
      "66400.iterasyon, Train Loss = 0.21614423711579664, Val Loss = 11.732389434308528\n",
      "66500.iterasyon, Train Loss = 0.21614328882928713, Val Loss = 11.732428189783807\n",
      "66600.iterasyon, Train Loss = 0.21614234110781438, Val Loss = 11.732466956771832\n",
      "66700.iterasyon, Train Loss = 0.21614139395064236, Val Loss = 11.732505735089939\n",
      "66800.iterasyon, Train Loss = 0.2161404473570394, Val Loss = 11.732544524556904\n",
      "66900.iterasyon, Train Loss = 0.21613950132627435, Val Loss = 11.73258332499281\n",
      "67000.iterasyon, Train Loss = 0.21613855585762062, Val Loss = 11.732622136219137\n",
      "67100.iterasyon, Train Loss = 0.21613761095035408, Val Loss = 11.732660958058727\n",
      "67200.iterasyon, Train Loss = 0.21613666660375705, Val Loss = 11.732699790335722\n",
      "67300.iterasyon, Train Loss = 0.21613572281711124, Val Loss = 11.732738632875668\n",
      "67400.iterasyon, Train Loss = 0.2161347795897072, Val Loss = 11.73277748550533\n",
      "67500.iterasyon, Train Loss = 0.2161338369208302, Val Loss = 11.732816348052856\n",
      "67600.iterasyon, Train Loss = 0.21613289480977835, Val Loss = 11.732855220347655\n",
      "67700.iterasyon, Train Loss = 0.21613195325584522, Val Loss = 11.732894102220435\n",
      "67800.iterasyon, Train Loss = 0.21613101225833298, Val Loss = 11.732932993503251\n",
      "67900.iterasyon, Train Loss = 0.2161300718165424, Val Loss = 11.732971894029324\n",
      "68000.iterasyon, Train Loss = 0.21612913192978303, Val Loss = 11.733010803633196\n",
      "68100.iterasyon, Train Loss = 0.21612819259736213, Val Loss = 11.733049722150662\n",
      "68200.iterasyon, Train Loss = 0.21612725381859288, Val Loss = 11.733088649418768\n",
      "68300.iterasyon, Train Loss = 0.21612631559279027, Val Loss = 11.733127585275792\n",
      "68400.iterasyon, Train Loss = 0.2161253779192732, Val Loss = 11.733166529561192\n",
      "68500.iterasyon, Train Loss = 0.21612444079736237, Val Loss = 11.733205482115697\n",
      "68600.iterasyon, Train Loss = 0.21612350422638435, Val Loss = 11.733244442781256\n",
      "68700.iterasyon, Train Loss = 0.2161225682056653, Val Loss = 11.733283411400938\n",
      "68800.iterasyon, Train Loss = 0.21612163273453242, Val Loss = 11.733322387819122\n",
      "68900.iterasyon, Train Loss = 0.2161206978123228, Val Loss = 11.733361371881232\n",
      "69000.iterasyon, Train Loss = 0.21611976343837172, Val Loss = 11.733400363433955\n",
      "69100.iterasyon, Train Loss = 0.2161188296120163, Val Loss = 11.733439362325125\n",
      "69200.iterasyon, Train Loss = 0.21611789633259917, Val Loss = 11.733478368403727\n",
      "69300.iterasyon, Train Loss = 0.21611696359946306, Val Loss = 11.733517381519887\n",
      "69400.iterasyon, Train Loss = 0.21611603141195834, Val Loss = 11.733556401524867\n",
      "69500.iterasyon, Train Loss = 0.21611509976943055, Val Loss = 11.73359542827114\n",
      "69600.iterasyon, Train Loss = 0.2161141686712351, Val Loss = 11.733634461612134\n",
      "69700.iterasyon, Train Loss = 0.21611323811672456, Val Loss = 11.733673501402544\n",
      "69800.iterasyon, Train Loss = 0.2161123081052575, Val Loss = 11.73371254749811\n",
      "69900.iterasyon, Train Loss = 0.21611137863619442, Val Loss = 11.733751599755701\n",
      "70000.iterasyon, Train Loss = 0.21611044970889684, Val Loss = 11.733790658033143\n",
      "70100.iterasyon, Train Loss = 0.21610952132272973, Val Loss = 11.73382972218955\n",
      "70200.iterasyon, Train Loss = 0.21610859347706313, Val Loss = 11.733868792084987\n",
      "70300.iterasyon, Train Loss = 0.21610766617126417, Val Loss = 11.733907867580568\n",
      "70400.iterasyon, Train Loss = 0.21610673940470818, Val Loss = 11.733946948538541\n",
      "70500.iterasyon, Train Loss = 0.21610581317676822, Val Loss = 11.733986034822141\n",
      "70600.iterasyon, Train Loss = 0.21610488748682233, Val Loss = 11.734025126295641\n",
      "70700.iterasyon, Train Loss = 0.21610396233425203, Val Loss = 11.734064222824456\n",
      "70800.iterasyon, Train Loss = 0.21610303771843778, Val Loss = 11.73410332427488\n",
      "70900.iterasyon, Train Loss = 0.21610211363876372, Val Loss = 11.734142430514376\n",
      "71000.iterasyon, Train Loss = 0.21610119009461806, Val Loss = 11.734181541411283\n",
      "71100.iterasyon, Train Loss = 0.21610026708538918, Val Loss = 11.734220656834966\n",
      "71200.iterasyon, Train Loss = 0.2160993446104691, Val Loss = 11.73425977665588\n",
      "71300.iterasyon, Train Loss = 0.2160984226692507, Val Loss = 11.734298900745378\n",
      "71400.iterasyon, Train Loss = 0.2160975012611308, Val Loss = 11.734338028975879\n",
      "71500.iterasyon, Train Loss = 0.21609658038550802, Val Loss = 11.734377161220623\n",
      "71600.iterasyon, Train Loss = 0.21609566004178127, Val Loss = 11.734416297354054\n",
      "71700.iterasyon, Train Loss = 0.21609474022935227, Val Loss = 11.73445543725132\n",
      "71800.iterasyon, Train Loss = 0.21609382094762963, Val Loss = 11.734494580788697\n",
      "71900.iterasyon, Train Loss = 0.21609290219601565, Val Loss = 11.73453372784338\n",
      "72000.iterasyon, Train Loss = 0.21609198397392285, Val Loss = 11.734572878293477\n",
      "72100.iterasyon, Train Loss = 0.21609106628076072, Val Loss = 11.734612032018003\n",
      "72200.iterasyon, Train Loss = 0.21609014911594118, Val Loss = 11.734651188896931\n",
      "72300.iterasyon, Train Loss = 0.21608923247888115, Val Loss = 11.734690348811158\n",
      "72400.iterasyon, Train Loss = 0.21608831636899758, Val Loss = 11.734729511642508\n",
      "72500.iterasyon, Train Loss = 0.2160874007857079, Val Loss = 11.734768677273705\n",
      "72600.iterasyon, Train Loss = 0.2160864857284354, Val Loss = 11.734807845588282\n",
      "72700.iterasyon, Train Loss = 0.21608557119660207, Val Loss = 11.734847016470816\n",
      "72800.iterasyon, Train Loss = 0.21608465718963388, Val Loss = 11.73488618980661\n",
      "72900.iterasyon, Train Loss = 0.2160837437069573, Val Loss = 11.734925365481997\n",
      "73000.iterasyon, Train Loss = 0.21608283074800014, Val Loss = 11.734964543384072\n",
      "73100.iterasyon, Train Loss = 0.21608191831219478, Val Loss = 11.735003723400887\n",
      "73200.iterasyon, Train Loss = 0.21608100639897468, Val Loss = 11.735042905421272\n",
      "73300.iterasyon, Train Loss = 0.21608009500777323, Val Loss = 11.735082089334933\n",
      "73400.iterasyon, Train Loss = 0.2160791841380271, Val Loss = 11.735121275032437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73500.iterasyon, Train Loss = 0.21607827378917518, Val Loss = 11.735160462405242\n",
      "73600.iterasyon, Train Loss = 0.21607736396065688, Val Loss = 11.735199651345535\n",
      "73700.iterasyon, Train Loss = 0.21607645465191408, Val Loss = 11.73523884174641\n",
      "73800.iterasyon, Train Loss = 0.21607554586239242, Val Loss = 11.735278033501732\n",
      "73900.iterasyon, Train Loss = 0.21607463759153353, Val Loss = 11.735317226506249\n",
      "74000.iterasyon, Train Loss = 0.21607372983878825, Val Loss = 11.735356420655469\n",
      "74100.iterasyon, Train Loss = 0.21607282260360486, Val Loss = 11.735395615845674\n",
      "74200.iterasyon, Train Loss = 0.21607191588543326, Val Loss = 11.735434811974045\n",
      "74300.iterasyon, Train Loss = 0.21607100968372797, Val Loss = 11.735474008938477\n",
      "74400.iterasyon, Train Loss = 0.21607010399793902, Val Loss = 11.735513206637659\n",
      "74500.iterasyon, Train Loss = 0.21606919882752595, Val Loss = 11.735552404971084\n",
      "74600.iterasyon, Train Loss = 0.2160682941719455, Val Loss = 11.735591603839024\n",
      "74700.iterasyon, Train Loss = 0.21606739003065634, Val Loss = 11.735630803142458\n",
      "74800.iterasyon, Train Loss = 0.21606648640311973, Val Loss = 11.735670002783273\n",
      "74900.iterasyon, Train Loss = 0.21606558328879658, Val Loss = 11.735709202663925\n",
      "75000.iterasyon, Train Loss = 0.21606468068715246, Val Loss = 11.735748402687797\n",
      "75100.iterasyon, Train Loss = 0.2160637785976541, Val Loss = 11.735787602758872\n",
      "75200.iterasyon, Train Loss = 0.21606287701976656, Val Loss = 11.735826802781986\n",
      "75300.iterasyon, Train Loss = 0.21606197595295976, Val Loss = 11.735866002662638\n",
      "75400.iterasyon, Train Loss = 0.2160610753967039, Val Loss = 11.735905202307128\n",
      "75500.iterasyon, Train Loss = 0.2160601753504711, Val Loss = 11.73594440162242\n",
      "75600.iterasyon, Train Loss = 0.21605927581373244, Val Loss = 11.73598360051619\n",
      "75700.iterasyon, Train Loss = 0.21605837678596718, Val Loss = 11.73602279889691\n",
      "75800.iterasyon, Train Loss = 0.21605747826664892, Val Loss = 11.736061996673687\n",
      "75900.iterasyon, Train Loss = 0.21605658025525665, Val Loss = 11.736101193756362\n",
      "76000.iterasyon, Train Loss = 0.21605568275126974, Val Loss = 11.736140390055505\n",
      "76100.iterasyon, Train Loss = 0.21605478575416673, Val Loss = 11.736179585482262\n",
      "76200.iterasyon, Train Loss = 0.21605388926343427, Val Loss = 11.73621877994858\n",
      "76300.iterasyon, Train Loss = 0.2160529932785518, Val Loss = 11.736257973367087\n",
      "76400.iterasyon, Train Loss = 0.21605209779900783, Val Loss = 11.7362971656511\n",
      "76500.iterasyon, Train Loss = 0.21605120282428877, Val Loss = 11.736336356714466\n",
      "76600.iterasyon, Train Loss = 0.21605030835388028, Val Loss = 11.736375546471892\n",
      "76700.iterasyon, Train Loss = 0.21604941438727385, Val Loss = 11.736414734838633\n",
      "76800.iterasyon, Train Loss = 0.21604852092396032, Val Loss = 11.736453921730629\n",
      "76900.iterasyon, Train Loss = 0.2160476279634305, Val Loss = 11.736493107064506\n",
      "77000.iterasyon, Train Loss = 0.21604673550518072, Val Loss = 11.736532290757495\n",
      "77100.iterasyon, Train Loss = 0.21604584354870196, Val Loss = 11.736571472727501\n",
      "77200.iterasyon, Train Loss = 0.21604495209349453, Val Loss = 11.736610652893022\n",
      "77300.iterasyon, Train Loss = 0.2160440611390526, Val Loss = 11.736649831173247\n",
      "77400.iterasyon, Train Loss = 0.21604317068487824, Val Loss = 11.73668900748797\n",
      "77500.iterasyon, Train Loss = 0.2160422807304692, Val Loss = 11.736728181757647\n",
      "77600.iterasyon, Train Loss = 0.21604139127532848, Val Loss = 11.736767353903286\n",
      "77700.iterasyon, Train Loss = 0.21604050231895913, Val Loss = 11.736806523846568\n",
      "77800.iterasyon, Train Loss = 0.21603961386086382, Val Loss = 11.736845691509773\n",
      "77900.iterasyon, Train Loss = 0.2160387259005485, Val Loss = 11.736884856815777\n",
      "78000.iterasyon, Train Loss = 0.21603783843752022, Val Loss = 11.736924019688075\n",
      "78100.iterasyon, Train Loss = 0.21603695147128874, Val Loss = 11.736963180050713\n",
      "78200.iterasyon, Train Loss = 0.21603606500135897, Val Loss = 11.737002337828367\n",
      "78300.iterasyon, Train Loss = 0.2160351790272457, Val Loss = 11.737041492946375\n",
      "78400.iterasyon, Train Loss = 0.21603429354845657, Val Loss = 11.737080645330497\n",
      "78500.iterasyon, Train Loss = 0.21603340856450715, Val Loss = 11.73711979490723\n",
      "78600.iterasyon, Train Loss = 0.21603252407491058, Val Loss = 11.737158941603596\n",
      "78700.iterasyon, Train Loss = 0.21603164007918155, Val Loss = 11.737198085347131\n",
      "78800.iterasyon, Train Loss = 0.21603075657683699, Val Loss = 11.73723722606597\n",
      "78900.iterasyon, Train Loss = 0.21602987356739597, Val Loss = 11.73727636368891\n",
      "79000.iterasyon, Train Loss = 0.21602899105037468, Val Loss = 11.737315498145163\n",
      "79100.iterasyon, Train Loss = 0.21602810902529457, Val Loss = 11.737354629364583\n",
      "79200.iterasyon, Train Loss = 0.21602722749167602, Val Loss = 11.737393757277566\n",
      "79300.iterasyon, Train Loss = 0.21602634644904192, Val Loss = 11.73743288181495\n",
      "79400.iterasyon, Train Loss = 0.21602546589691327, Val Loss = 11.737472002908367\n",
      "79500.iterasyon, Train Loss = 0.21602458583481687, Val Loss = 11.737511120489698\n",
      "79600.iterasyon, Train Loss = 0.21602370626227915, Val Loss = 11.73755023449157\n",
      "79700.iterasyon, Train Loss = 0.21602282717882562, Val Loss = 11.737589344847063\n",
      "79800.iterasyon, Train Loss = 0.21602194858398308, Val Loss = 11.737628451489709\n",
      "79900.iterasyon, Train Loss = 0.2160210704772805, Val Loss = 11.737667554353758\n",
      "80000.iterasyon, Train Loss = 0.21602019285824978, Val Loss = 11.737706653373797\n",
      "80100.iterasyon, Train Loss = 0.21601931572641966, Val Loss = 11.737745748484983\n",
      "80200.iterasyon, Train Loss = 0.21601843908132423, Val Loss = 11.737784839623039\n",
      "80300.iterasyon, Train Loss = 0.2160175629224955, Val Loss = 11.737823926724104\n",
      "80400.iterasyon, Train Loss = 0.2160166872494669, Val Loss = 11.737863009724922\n",
      "80500.iterasyon, Train Loss = 0.21601581206177534, Val Loss = 11.737902088562704\n",
      "80600.iterasyon, Train Loss = 0.21601493735895602, Val Loss = 11.737941163175085\n",
      "80700.iterasyon, Train Loss = 0.21601406314054714, Val Loss = 11.737980233500307\n",
      "80800.iterasyon, Train Loss = 0.2160131894060853, Val Loss = 11.738019299477044\n",
      "80900.iterasyon, Train Loss = 0.21601231615511252, Val Loss = 11.738058361044413\n",
      "81000.iterasyon, Train Loss = 0.21601144338716752, Val Loss = 11.738097418142086\n",
      "81100.iterasyon, Train Loss = 0.21601057110179145, Val Loss = 11.738136470710215\n",
      "81200.iterasyon, Train Loss = 0.21600969929852729, Val Loss = 11.738175518689395\n",
      "81300.iterasyon, Train Loss = 0.21600882797691867, Val Loss = 11.738214562020664\n",
      "81400.iterasyon, Train Loss = 0.21600795713650991, Val Loss = 11.738253600645585\n",
      "81500.iterasyon, Train Loss = 0.21600708677684533, Val Loss = 11.738292634506262\n",
      "81600.iterasyon, Train Loss = 0.216006216897471, Val Loss = 11.73833166354501\n",
      "81700.iterasyon, Train Loss = 0.21600534749793618, Val Loss = 11.738370687704894\n",
      "81800.iterasyon, Train Loss = 0.21600447857778726, Val Loss = 11.738409706929222\n",
      "81900.iterasyon, Train Loss = 0.2160036101365741, Val Loss = 11.738448721161852\n",
      "82000.iterasyon, Train Loss = 0.2160027421738471, Val Loss = 11.738487730347059\n",
      "82100.iterasyon, Train Loss = 0.2160018746891562, Val Loss = 11.738526734429641\n",
      "82200.iterasyon, Train Loss = 0.21600100768205488, Val Loss = 11.738565733354658\n",
      "82300.iterasyon, Train Loss = 0.2160001411520941, Val Loss = 11.738604727067825\n",
      "82400.iterasyon, Train Loss = 0.21599927509882935, Val Loss = 11.738643715515087\n",
      "82500.iterasyon, Train Loss = 0.21599840952181484, Val Loss = 11.738682698643055\n",
      "82600.iterasyon, Train Loss = 0.2159975444206049, Val Loss = 11.738721676398509\n",
      "82700.iterasyon, Train Loss = 0.2159966797947583, Val Loss = 11.738760648728869\n",
      "82800.iterasyon, Train Loss = 0.21599581564383044, Val Loss = 11.73879961558189\n",
      "82900.iterasyon, Train Loss = 0.21599495196738008, Val Loss = 11.73883857690571\n",
      "83000.iterasyon, Train Loss = 0.21599408876496762, Val Loss = 11.738877532648939\n",
      "83100.iterasyon, Train Loss = 0.2159932260361516, Val Loss = 11.738916482760587\n",
      "83200.iterasyon, Train Loss = 0.21599236378049433, Val Loss = 11.738955427190087\n",
      "83300.iterasyon, Train Loss = 0.21599150199755576, Val Loss = 11.738994365887212\n",
      "83400.iterasyon, Train Loss = 0.21599064068690135, Val Loss = 11.739033298802228\n",
      "83500.iterasyon, Train Loss = 0.21598977984809084, Val Loss = 11.73907222588578\n",
      "83600.iterasyon, Train Loss = 0.21598891948069113, Val Loss = 11.739111147088861\n",
      "83700.iterasyon, Train Loss = 0.2159880595842667, Val Loss = 11.739150062362942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83800.iterasyon, Train Loss = 0.21598720015838424, Val Loss = 11.739188971659772\n",
      "83900.iterasyon, Train Loss = 0.21598634120260884, Val Loss = 11.739227874931647\n",
      "84000.iterasyon, Train Loss = 0.21598548271650997, Val Loss = 11.739266772131085\n",
      "84100.iterasyon, Train Loss = 0.21598462469965682, Val Loss = 11.739305663211113\n",
      "84200.iterasyon, Train Loss = 0.2159837671516156, Val Loss = 11.739344548125016\n",
      "84300.iterasyon, Train Loss = 0.2159829100719586, Val Loss = 11.7393834268266\n",
      "84400.iterasyon, Train Loss = 0.21598205346025656, Val Loss = 11.73942229926998\n",
      "84500.iterasyon, Train Loss = 0.21598119731608045, Val Loss = 11.739461165409587\n",
      "84600.iterasyon, Train Loss = 0.21598034163900368, Val Loss = 11.739500025200345\n",
      "84700.iterasyon, Train Loss = 0.2159794864285989, Val Loss = 11.739538878597452\n",
      "84800.iterasyon, Train Loss = 0.21597863168444012, Val Loss = 11.739577725556437\n",
      "84900.iterasyon, Train Loss = 0.21597777740610405, Val Loss = 11.739616566033295\n",
      "85000.iterasyon, Train Loss = 0.2159769235931637, Val Loss = 11.739655399984336\n",
      "85100.iterasyon, Train Loss = 0.21597607024519594, Val Loss = 11.739694227366257\n",
      "85200.iterasyon, Train Loss = 0.21597521736178024, Val Loss = 11.739733048136005\n",
      "85300.iterasyon, Train Loss = 0.21597436494249322, Val Loss = 11.739771862250963\n",
      "85400.iterasyon, Train Loss = 0.2159735129869117, Val Loss = 11.739810669668877\n",
      "85500.iterasyon, Train Loss = 0.2159726614946197, Val Loss = 11.739849470347803\n",
      "85600.iterasyon, Train Loss = 0.2159718104651923, Val Loss = 11.739888264246161\n",
      "85700.iterasyon, Train Loss = 0.21597095989821327, Val Loss = 11.739927051322686\n",
      "85800.iterasyon, Train Loss = 0.2159701097932659, Val Loss = 11.739965831536445\n",
      "85900.iterasyon, Train Loss = 0.21596926014992687, Val Loss = 11.740004604846874\n",
      "86000.iterasyon, Train Loss = 0.21596841096778494, Val Loss = 11.740043371213789\n",
      "86100.iterasyon, Train Loss = 0.21596756224642252, Val Loss = 11.740082130597127\n",
      "86200.iterasyon, Train Loss = 0.2159667139854229, Val Loss = 11.740120882957466\n",
      "86300.iterasyon, Train Loss = 0.215965866184372, Val Loss = 11.740159628255501\n",
      "86400.iterasyon, Train Loss = 0.21596501884285838, Val Loss = 11.740198366452235\n",
      "86500.iterasyon, Train Loss = 0.21596417196046683, Val Loss = 11.740237097509105\n",
      "86600.iterasyon, Train Loss = 0.2159633255367834, Val Loss = 11.740275821387824\n",
      "86700.iterasyon, Train Loss = 0.2159624795713978, Val Loss = 11.74031453805038\n",
      "86800.iterasyon, Train Loss = 0.21596163406389868, Val Loss = 11.740353247459112\n",
      "86900.iterasyon, Train Loss = 0.21596078901387572, Val Loss = 11.740391949576747\n",
      "87000.iterasyon, Train Loss = 0.2159599444209205, Val Loss = 11.740430644366118\n",
      "87100.iterasyon, Train Loss = 0.21595910028462062, Val Loss = 11.740469331790612\n",
      "87200.iterasyon, Train Loss = 0.2159582566045707, Val Loss = 11.740508011813759\n",
      "87300.iterasyon, Train Loss = 0.21595741338036303, Val Loss = 11.740546684399376\n",
      "87400.iterasyon, Train Loss = 0.21595657061158832, Val Loss = 11.740585349511656\n",
      "87500.iterasyon, Train Loss = 0.21595572829784307, Val Loss = 11.740624007115136\n",
      "87600.iterasyon, Train Loss = 0.2159548864387192, Val Loss = 11.740662657174534\n",
      "87700.iterasyon, Train Loss = 0.21595404503381344, Val Loss = 11.740701299654914\n",
      "87800.iterasyon, Train Loss = 0.21595320408272078, Val Loss = 11.740739934521653\n",
      "87900.iterasyon, Train Loss = 0.21595236358503678, Val Loss = 11.74077856174039\n",
      "88000.iterasyon, Train Loss = 0.21595152354035962, Val Loss = 11.74081718127703\n",
      "88100.iterasyon, Train Loss = 0.21595068394828698, Val Loss = 11.740855793097836\n",
      "88200.iterasyon, Train Loss = 0.21594984480841659, Val Loss = 11.740894397169264\n",
      "88300.iterasyon, Train Loss = 0.21594900612034781, Val Loss = 11.740932993458133\n",
      "88400.iterasyon, Train Loss = 0.21594816788367932, Val Loss = 11.740971581931479\n",
      "88500.iterasyon, Train Loss = 0.215947330098013, Val Loss = 11.741010162556657\n",
      "88600.iterasyon, Train Loss = 0.21594649276294753, Val Loss = 11.74104873530125\n",
      "88700.iterasyon, Train Loss = 0.21594565587808645, Val Loss = 11.741087300133152\n",
      "88800.iterasyon, Train Loss = 0.2159448194430311, Val Loss = 11.741125857020556\n",
      "88900.iterasyon, Train Loss = 0.2159439834573842, Val Loss = 11.74116440593186\n",
      "89000.iterasyon, Train Loss = 0.21594314792074734, Val Loss = 11.741202946835735\n",
      "89100.iterasyon, Train Loss = 0.21594231283272672, Val Loss = 11.741241479701202\n",
      "89200.iterasyon, Train Loss = 0.21594147819292772, Val Loss = 11.741280004497398\n",
      "89300.iterasyon, Train Loss = 0.215940644000954, Val Loss = 11.74131852119388\n",
      "89400.iterasyon, Train Loss = 0.21593981025641065, Val Loss = 11.741357029760364\n",
      "89500.iterasyon, Train Loss = 0.2159389769589055, Val Loss = 11.741395530166836\n",
      "89600.iterasyon, Train Loss = 0.215938144108045, Val Loss = 11.741434022383539\n",
      "89700.iterasyon, Train Loss = 0.215937311703438, Val Loss = 11.741472506381038\n",
      "89800.iterasyon, Train Loss = 0.21593647974469157, Val Loss = 11.741510982129984\n",
      "89900.iterasyon, Train Loss = 0.21593564823141384, Val Loss = 11.741549449601498\n",
      "90000.iterasyon, Train Loss = 0.2159348171632133, Val Loss = 11.741587908766785\n",
      "90100.iterasyon, Train Loss = 0.21593398653970375, Val Loss = 11.741626359597355\n",
      "90200.iterasyon, Train Loss = 0.2159331563604937, Val Loss = 11.741664802064921\n",
      "90300.iterasyon, Train Loss = 0.2159323266251938, Val Loss = 11.741703236141566\n",
      "90400.iterasyon, Train Loss = 0.21593149733341524, Val Loss = 11.74174166179944\n",
      "90500.iterasyon, Train Loss = 0.21593066848477208, Val Loss = 11.741780079010981\n",
      "90600.iterasyon, Train Loss = 0.2159298400788783, Val Loss = 11.741818487748972\n",
      "90700.iterasyon, Train Loss = 0.21592901211534368, Val Loss = 11.741856887986314\n",
      "90800.iterasyon, Train Loss = 0.21592818459378427, Val Loss = 11.741895279696204\n",
      "90900.iterasyon, Train Loss = 0.21592735751381528, Val Loss = 11.74193366285199\n",
      "91000.iterasyon, Train Loss = 0.21592653087505054, Val Loss = 11.741972037427365\n",
      "91100.iterasyon, Train Loss = 0.21592570467710556, Val Loss = 11.742010403396206\n",
      "91200.iterasyon, Train Loss = 0.21592487891959858, Val Loss = 11.742048760732565\n",
      "91300.iterasyon, Train Loss = 0.2159240536021445, Val Loss = 11.74208710941074\n",
      "91400.iterasyon, Train Loss = 0.21592322872436373, Val Loss = 11.742125449405325\n",
      "91500.iterasyon, Train Loss = 0.2159224042858697, Val Loss = 11.742163780691039\n",
      "91600.iterasyon, Train Loss = 0.21592158028628364, Val Loss = 11.742202103242878\n",
      "91700.iterasyon, Train Loss = 0.21592075672522534, Val Loss = 11.742240417036086\n",
      "91800.iterasyon, Train Loss = 0.2159199336023119, Val Loss = 11.742278722046036\n",
      "91900.iterasyon, Train Loss = 0.215919110917166, Val Loss = 11.742317018248345\n",
      "92000.iterasyon, Train Loss = 0.2159182886694058, Val Loss = 11.742355305618862\n",
      "92100.iterasyon, Train Loss = 0.2159174668586552, Val Loss = 11.742393584133653\n",
      "92200.iterasyon, Train Loss = 0.2159166454845339, Val Loss = 11.742431853769013\n",
      "92300.iterasyon, Train Loss = 0.21591582454666378, Val Loss = 11.742470114501371\n",
      "92400.iterasyon, Train Loss = 0.21591500404467032, Val Loss = 11.742508366307446\n",
      "92500.iterasyon, Train Loss = 0.21591418397817433, Val Loss = 11.742546609164155\n",
      "92600.iterasyon, Train Loss = 0.21591336434680017, Val Loss = 11.742584843048487\n",
      "92700.iterasyon, Train Loss = 0.21591254515017314, Val Loss = 11.74262306793785\n",
      "92800.iterasyon, Train Loss = 0.21591172638791797, Val Loss = 11.742661283809653\n",
      "92900.iterasyon, Train Loss = 0.21591090805965918, Val Loss = 11.74269949064166\n",
      "93000.iterasyon, Train Loss = 0.2159100901650222, Val Loss = 11.742737688411763\n",
      "93100.iterasyon, Train Loss = 0.21590927270363447, Val Loss = 11.742775877098016\n",
      "93200.iterasyon, Train Loss = 0.21590845567512393, Val Loss = 11.742814056678743\n",
      "93300.iterasyon, Train Loss = 0.21590763907911603, Val Loss = 11.742852227132445\n",
      "93400.iterasyon, Train Loss = 0.2159068229152412, Val Loss = 11.742890388437695\n",
      "93500.iterasyon, Train Loss = 0.21590600718312467, Val Loss = 11.742928540573514\n",
      "93600.iterasyon, Train Loss = 0.21590519188239732, Val Loss = 11.742966683518803\n",
      "93700.iterasyon, Train Loss = 0.21590437701268797, Val Loss = 11.743004817252949\n",
      "93800.iterasyon, Train Loss = 0.21590356257362742, Val Loss = 11.74304294175522\n",
      "93900.iterasyon, Train Loss = 0.2159027485648457, Val Loss = 11.74308105700536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94000.iterasyon, Train Loss = 0.2159019349859748, Val Loss = 11.743119162983177\n",
      "94100.iterasyon, Train Loss = 0.21590112183664364, Val Loss = 11.74315725966858\n",
      "94200.iterasyon, Train Loss = 0.21590030911648575, Val Loss = 11.74319534704178\n",
      "94300.iterasyon, Train Loss = 0.2158994968251332, Val Loss = 11.74323342508309\n",
      "94400.iterasyon, Train Loss = 0.21589868496221865, Val Loss = 11.743271493773054\n",
      "94500.iterasyon, Train Loss = 0.21589787352737655, Val Loss = 11.74330955309238\n",
      "94600.iterasyon, Train Loss = 0.2158970625202411, Val Loss = 11.743347603021883\n",
      "94700.iterasyon, Train Loss = 0.21589625194044496, Val Loss = 11.743385643542736\n",
      "94800.iterasyon, Train Loss = 0.21589544178762318, Val Loss = 11.743423674636064\n",
      "94900.iterasyon, Train Loss = 0.2158946320614111, Val Loss = 11.743461696283298\n",
      "95000.iterasyon, Train Loss = 0.2158938227614455, Val Loss = 11.743499708465988\n",
      "95100.iterasyon, Train Loss = 0.21589301388736157, Val Loss = 11.743537711165889\n",
      "95200.iterasyon, Train Loss = 0.21589220543879709, Val Loss = 11.743575704364913\n",
      "95300.iterasyon, Train Loss = 0.21589139741538857, Val Loss = 11.743613688045135\n",
      "95400.iterasyon, Train Loss = 0.21589058981677353, Val Loss = 11.743651662188773\n",
      "95500.iterasyon, Train Loss = 0.21588978264258987, Val Loss = 11.743689626778226\n",
      "95600.iterasyon, Train Loss = 0.21588897589247638, Val Loss = 11.74372758179608\n",
      "95700.iterasyon, Train Loss = 0.21588816956607404, Val Loss = 11.74376552722505\n",
      "95800.iterasyon, Train Loss = 0.2158873636630196, Val Loss = 11.743803463048028\n",
      "95900.iterasyon, Train Loss = 0.21588655818295385, Val Loss = 11.74384138924809\n",
      "96000.iterasyon, Train Loss = 0.21588575312551764, Val Loss = 11.74387930580843\n",
      "96100.iterasyon, Train Loss = 0.2158849484903518, Val Loss = 11.743917212712377\n",
      "96200.iterasyon, Train Loss = 0.21588414427709657, Val Loss = 11.743955109943498\n",
      "96300.iterasyon, Train Loss = 0.21588334048539537, Val Loss = 11.743992997485504\n",
      "96400.iterasyon, Train Loss = 0.2158825371148886, Val Loss = 11.74403087532216\n",
      "96500.iterasyon, Train Loss = 0.21588173416522063, Val Loss = 11.744068743437458\n",
      "96600.iterasyon, Train Loss = 0.21588093163603453, Val Loss = 11.744106601815556\n",
      "96700.iterasyon, Train Loss = 0.21588012952697264, Val Loss = 11.744144450440722\n",
      "96800.iterasyon, Train Loss = 0.2158793278376788, Val Loss = 11.744182289297418\n",
      "96900.iterasyon, Train Loss = 0.2158785265677988, Val Loss = 11.7442201183702\n",
      "97000.iterasyon, Train Loss = 0.21587772571697708, Val Loss = 11.744257937643845\n",
      "97100.iterasyon, Train Loss = 0.21587692528485916, Val Loss = 11.744295747103166\n",
      "97200.iterasyon, Train Loss = 0.21587612527109015, Val Loss = 11.74433354673323\n",
      "97300.iterasyon, Train Loss = 0.21587532567531573, Val Loss = 11.744371336519212\n",
      "97400.iterasyon, Train Loss = 0.21587452649718297, Val Loss = 11.744409116446374\n",
      "97500.iterasyon, Train Loss = 0.21587372773634125, Val Loss = 11.744446886500198\n",
      "97600.iterasyon, Train Loss = 0.2158729293924343, Val Loss = 11.744484646666287\n",
      "97700.iterasyon, Train Loss = 0.21587213146511172, Val Loss = 11.74452239693036\n",
      "97800.iterasyon, Train Loss = 0.2158713339540228, Val Loss = 11.744560137278285\n",
      "97900.iterasyon, Train Loss = 0.21587053685881472, Val Loss = 11.744597867696058\n",
      "98000.iterasyon, Train Loss = 0.2158697401791382, Val Loss = 11.744635588169837\n",
      "98100.iterasyon, Train Loss = 0.2158689439146411, Val Loss = 11.744673298685937\n",
      "98200.iterasyon, Train Loss = 0.21586814806497298, Val Loss = 11.744710999230742\n",
      "98300.iterasyon, Train Loss = 0.21586735262978762, Val Loss = 11.744748689790788\n",
      "98400.iterasyon, Train Loss = 0.21586655760873472, Val Loss = 11.744786370352704\n",
      "98500.iterasyon, Train Loss = 0.2158657630014621, Val Loss = 11.74482404090343\n",
      "98600.iterasyon, Train Loss = 0.21586496880762548, Val Loss = 11.744861701429839\n",
      "98700.iterasyon, Train Loss = 0.2158641750268747, Val Loss = 11.744899351919008\n",
      "98800.iterasyon, Train Loss = 0.21586338165886573, Val Loss = 11.744936992358129\n",
      "98900.iterasyon, Train Loss = 0.21586258870324607, Val Loss = 11.744974622734551\n",
      "99000.iterasyon, Train Loss = 0.21586179615967394, Val Loss = 11.745012243035749\n",
      "99100.iterasyon, Train Loss = 0.21586100402779854, Val Loss = 11.745049853249283\n",
      "99200.iterasyon, Train Loss = 0.2158602123072789, Val Loss = 11.745087453362842\n",
      "99300.iterasyon, Train Loss = 0.21585942099776526, Val Loss = 11.745125043364238\n",
      "99400.iterasyon, Train Loss = 0.21585863009891684, Val Loss = 11.745162623241486\n",
      "99500.iterasyon, Train Loss = 0.21585783961038504, Val Loss = 11.745200192982646\n",
      "99600.iterasyon, Train Loss = 0.2158570495318282, Val Loss = 11.745237752575918\n",
      "99700.iterasyon, Train Loss = 0.21585625986290266, Val Loss = 11.745275302009592\n",
      "99800.iterasyon, Train Loss = 0.21585547060326135, Val Loss = 11.745312841272131\n",
      "99900.iterasyon, Train Loss = 0.2158546817525657, Val Loss = 11.745350370352071\n"
     ]
    }
   ],
   "source": [
    "iterasyon = 100000\n",
    "for i in range(iterasyon):\n",
    "    for j in range(x_train.shape[0]):\n",
    "        thetas = gradient_descent_reg(thetas,x_train[j:j+1],y_train[j:j+1],lr=0.01,\n",
    "                                      reg_katsayi=reg_values[son_losslar_train.index(min(son_losslar_train))])\n",
    "    train_losses.append(loss_hesapla_reg(thetas,x_train,y_train))\n",
    "    val_losses.append(loss_hesapla(thetas,x_val,y_val))\n",
    "    if(i%100 == 0):\n",
    "        train_loss = loss_hesapla_reg(thetas,x_train,y_train,\n",
    "                                      reg_katsayi=reg_values[son_losslar_train.index(min(son_losslar_train))])\n",
    "        val_loss = loss_hesapla(thetas,x_val,y_val)\n",
    "        print(\"{0}.iterasyon, Train Loss = {1}, Val Loss = {2}\".format(i,train_loss,val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range = np.linspace(x_train[:,1].min(),x_train[:,1].max(),1000).reshape(-1,1)\n",
    "temp = np.concatenate([np.ones((x_range.shape[0],1)),polinom_feature(x_range,derece)],axis=1)\n",
    "plt.scatter(x_train[:,1],y_train)\n",
    "plt.plot(x_range,thetas.dot(temp.T).reshape(-1,1),\"r\")\n",
    "plt.xlabel(\"Change in water level (x)\")\n",
    "plt.ylabel(\"Water flowing out of the dam (y)\")\n",
    "plt.title(\"Train Veriseti İçin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range = np.linspace(x_val[:,1].min(),x_val[:,1].max(),100).reshape(-1,1)\n",
    "temp = np.concatenate([np.ones((x_range.shape[0],1)),polinom_feature(x_range,derece)],axis=1)\n",
    "plt.scatter(x_val[:,1],y_val)\n",
    "plt.plot(x_range,thetas.dot(temp.T).reshape(-1,1),\"r\")\n",
    "plt.xlabel(\"Change in water level (x)\")\n",
    "plt.ylabel(\"Water flowing out of the dam (y)\")\n",
    "plt.title(\"Val Veriseti İçin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Örnek sayısının başarıma etkisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterasyon = 10000\n",
    "ornek_sayilari = range(1,x_train.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "son_losslar_train = list()\n",
    "son_losslar_val = list()\n",
    "for ornek_sayisi in ornek_sayilari:\n",
    "    thetas = np.ones((1,x_train.shape[1]))\n",
    "    for i in range(iterasyon):\n",
    "        for j in range(ornek_sayisi):\n",
    "            thetas = gradient_descent_reg(thetas,x_train[j:j+1],y_train[j:j+1],lr=0.31,reg_katsayi=0)\n",
    "    son_losslar_train.append(loss_hesapla_reg(thetas,x_train[:ornek_sayisi],y_train[:ornek_sayisi]))\n",
    "    son_losslar_val.append(loss_hesapla(thetas,x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,len(son_losslar_train)+1),son_losslar_train)\n",
    "plt.plot(range(1,len(son_losslar_val)+1),son_losslar_val)\n",
    "plt.xlabel(\"Ornek Sayisi\")\n",
    "plt.ylabel(\"Loss(MSE)\")\n",
    "plt.legend([\"Train Loss\",\"Val Loss\"])\n",
    "plt.title(\"Ornek Sayisina Göre Loss Değerleri\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation İle Polinomsal Lineer Regresyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_values = [0,0.001,0.01,0.1,1,10,100,1000]\n",
    "#reg_values = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "son_losslar_train = list()\n",
    "son_losslar_val = list()\n",
    "for reg_value in reg_values:\n",
    "    reg_matrix = np.eye(x_train.shape[1])\n",
    "    reg_matrix[0,0] = 0.\n",
    "    thetas_norm_eq = np.linalg.pinv(x_train.T.dot(x_train)+reg_matrix*reg_value).dot(x_train.T).dot(y_train)\n",
    "    son_losslar_train.append(loss_hesapla_reg(thetas_norm_eq.reshape(1,-1),x_train,y_train,reg_katsayi=reg_value))\n",
    "    son_losslar_val.append(loss_hesapla(thetas_norm_eq.reshape(1,-1),x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.286591476433012,\n",
       " 9.931832504928575,\n",
       " 10.385056323194915,\n",
       " 15.644360189770945,\n",
       " 36.66665652399256,\n",
       " 70.3623833765269,\n",
       " 80.8497841916982,\n",
       " 82.1094233157895]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "son_losslar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(reg_values,son_losslar_train)\n",
    "plt.plot(reg_values,son_losslar_val)\n",
    "plt.xlabel(\"Reguralizasyon Degeri\")\n",
    "plt.ylabel(\"Loss(MSE)\")\n",
    "plt.legend([\"Train Loss\",\"Val Loss\"])\n",
    "plt.title(\"Reguralizasyon Değerine Göre Loss Değerleri\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reg_value_norm = reg_values[son_losslar_val.index(min(son_losslar_val))]\n",
    "reg_matrix = np.eye(x_train.shape[1])\n",
    "reg_matrix[0,0] = 0.\n",
    "thetas_norm_eq = np.linalg.pinv(x_train.T.dot(x_train)+reg_matrix*best_reg_value_norm).dot(x_train.T).dot(y_train).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range = np.linspace(x_train[:,1].min(),x_train[:,1].max(),1000).reshape(-1,1)\n",
    "temp = np.concatenate([np.ones((x_range.shape[0],1)),polinom_feature(x_range,derece)],axis=1)\n",
    "plt.scatter(x_train[:,1],y_train)\n",
    "plt.plot(x_range,thetas_norm_eq.dot(temp.T).reshape(-1,1),\"r\")\n",
    "plt.xlabel(\"Change in water level (x)\")\n",
    "plt.ylabel(\"Water flowing out of the dam (y)\")\n",
    "plt.title(\"Train Veriseti İçin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range = np.linspace(x_val[:,1].min(),x_val[:,1].max(),100).reshape(-1,1)\n",
    "temp = np.concatenate([np.ones((x_range.shape[0],1)),polinom_feature(x_range,derece)],axis=1)\n",
    "plt.scatter(x_val[:,1],y_val)\n",
    "plt.plot(x_range,thetas_norm_eq.dot(temp.T).reshape(-1,1),\"r\")\n",
    "plt.xlabel(\"Change in water level (x)\")\n",
    "plt.ylabel(\"Water flowing out of the dam (y)\")\n",
    "plt.title(\"Val Veriseti İçin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range = np.linspace(x_test[:,1].min(),x_test[:,1].max(),100).reshape(-1,1)\n",
    "temp = np.concatenate([np.ones((x_range.shape[0],1)),polinom_feature(x_range,derece)],axis=1)\n",
    "plt.scatter(x_test[:,1],y_test)\n",
    "plt.plot(x_range,thetas_norm_eq.dot(temp.T).reshape(-1,1),\"r\")\n",
    "plt.xlabel(\"Change in water level (x)\")\n",
    "plt.ylabel(\"Water flowing out of the dam (y)\")\n",
    "plt.title(\"Test Veriseti İçin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
